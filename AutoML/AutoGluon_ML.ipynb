{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3ed797a",
   "metadata": {},
   "source": [
    "# AutoGluon AutoML - Vergleich mit Neural Network\n",
    "## Exakt gleiche Datenaufteilung wie das NN_aktuell.ipynb\n",
    "\n",
    "Dieses Notebook verwendet die **exakt gleichen Daten** und **identische Datenaufteilung** wie das Neural Network:\n",
    "- Gleiche Datenquelle: `data2_scaled_custom_new.json`\n",
    "- Gleiche Train/Validation/Test Split (60%/20%/20%)\n",
    "- Gleicher Random State (42) für Reproduzierbarkeit\n",
    "- Gleiches Target: `Creep rupture life`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "07539b83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📚 All imports successful!\n"
     ]
    }
   ],
   "source": [
    "# ===== IMPORTS =====\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from autogluon.tabular import TabularDataset, TabularPredictor\n",
    "import os\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"📚 All imports successful!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "398810bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Lade exakt die gleichen Daten wie das Neural Network...\n",
      "✅ Daten geladen: (156, 21)\n",
      "📋 Features: ['Ni', 'Cr', 'Co', 'Fe', 'Al', 'Ti', 'Nb', 'Mo', 'W', 'C', 'B', 'Zr', 'Test temperature (℃)', 'Test stress (Mpa)', 'solution treatment temperature', 'solution treatment time', 'Stable aging temperature (℃)', 'Stable aging time (h)', 'Aging temperature (℃)', 'Aging time (h)', 'Creep rupture life']\n",
      "🎯 Target: 'Creep rupture life'\n",
      "\n",
      "📊 Erste 5 Zeilen der Daten:\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Ni",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Cr",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Co",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Fe",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Al",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Ti",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Nb",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Mo",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "W",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "C",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "B",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Zr",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Test temperature (℃)",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Test stress (Mpa)",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "solution treatment temperature",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "solution treatment time",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Stable aging temperature (℃)",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Stable aging time (h)",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Aging temperature (℃)",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Aging time (h)",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Creep rupture life",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "ede98749-efa1-4fa7-9fa3-6689e4eced10",
       "rows": [
        [
         "3",
         "0.3556714768",
         "0.7121820616",
         "0.8346405229",
         "0.0524363097",
         "0.3215434084",
         "0.4494117647",
         "0.0",
         "0.4326241135",
         "0.0",
         "0.3384615385",
         "0.2666666667",
         "0.0",
         "0.328",
         "0.4401260504",
         "0.8755364807",
         "0.25",
         "0.78",
         "0.0638297872",
         "1.0",
         "0.0533333333",
         "0.5402435604"
        ],
        [
         "4",
         "0.3556714768",
         "0.7121820616",
         "0.8346405229",
         "0.0524363097",
         "0.3215434084",
         "0.4494117647",
         "0.0",
         "0.4326241135",
         "0.0",
         "0.3384615385",
         "0.2666666667",
         "0.0",
         "0.66",
         "0.243697479",
         "0.8755364807",
         "0.25",
         "0.78",
         "0.0638297872",
         "1.0",
         "0.0533333333",
         "0.3624176593"
        ],
        [
         "6",
         "0.3556714768",
         "0.7121820616",
         "0.8346405229",
         "0.0524363097",
         "0.3215434084",
         "0.4494117647",
         "0.0",
         "0.4326241135",
         "0.0",
         "0.3384615385",
         "0.2666666667",
         "0.0",
         "0.66",
         "0.243697479",
         "0.9270386266",
         "0.25",
         "0.78",
         "0.4893617021",
         "1.0",
         "0.0533333333",
         "0.6815773"
        ],
        [
         "8",
         "0.2806139167",
         "0.8326639893",
         "0.8823529412",
         "0.0257234727",
         "0.3344051447",
         "0.5458823529",
         "0.0",
         "0.4508611955",
         "0.0",
         "0.3076923077",
         "0.2666666667",
         "0.0",
         "0.32",
         "0.474789916",
         "0.8755364807",
         "0.25",
         "0.78",
         "0.0638297872",
         "1.0",
         "0.0533333333",
         "0.6457326152"
        ],
        [
         "9",
         "0.2806139167",
         "0.8326639893",
         "0.8823529412",
         "0.0257234727",
         "0.3344051447",
         "0.5458823529",
         "0.0",
         "0.4508611955",
         "0.0",
         "0.3076923077",
         "0.2666666667",
         "0.0",
         "0.32",
         "0.474789916",
         "0.8755364807",
         "0.25",
         "0.78",
         "0.4893617021",
         "1.0",
         "0.0533333333",
         "0.4805776716"
        ]
       ],
       "shape": {
        "columns": 21,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Ni</th>\n",
       "      <th>Cr</th>\n",
       "      <th>Co</th>\n",
       "      <th>Fe</th>\n",
       "      <th>Al</th>\n",
       "      <th>Ti</th>\n",
       "      <th>Nb</th>\n",
       "      <th>Mo</th>\n",
       "      <th>W</th>\n",
       "      <th>C</th>\n",
       "      <th>...</th>\n",
       "      <th>Zr</th>\n",
       "      <th>Test temperature (℃)</th>\n",
       "      <th>Test stress (Mpa)</th>\n",
       "      <th>solution treatment temperature</th>\n",
       "      <th>solution treatment time</th>\n",
       "      <th>Stable aging temperature (℃)</th>\n",
       "      <th>Stable aging time (h)</th>\n",
       "      <th>Aging temperature (℃)</th>\n",
       "      <th>Aging time (h)</th>\n",
       "      <th>Creep rupture life</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.355671</td>\n",
       "      <td>0.712182</td>\n",
       "      <td>0.834641</td>\n",
       "      <td>0.052436</td>\n",
       "      <td>0.321543</td>\n",
       "      <td>0.449412</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.432624</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.338462</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.328</td>\n",
       "      <td>0.440126</td>\n",
       "      <td>0.875536</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.063830</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.053333</td>\n",
       "      <td>0.540244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.355671</td>\n",
       "      <td>0.712182</td>\n",
       "      <td>0.834641</td>\n",
       "      <td>0.052436</td>\n",
       "      <td>0.321543</td>\n",
       "      <td>0.449412</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.432624</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.338462</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.660</td>\n",
       "      <td>0.243697</td>\n",
       "      <td>0.875536</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.063830</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.053333</td>\n",
       "      <td>0.362418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.355671</td>\n",
       "      <td>0.712182</td>\n",
       "      <td>0.834641</td>\n",
       "      <td>0.052436</td>\n",
       "      <td>0.321543</td>\n",
       "      <td>0.449412</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.432624</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.338462</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.660</td>\n",
       "      <td>0.243697</td>\n",
       "      <td>0.927039</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.489362</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.053333</td>\n",
       "      <td>0.681577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.280614</td>\n",
       "      <td>0.832664</td>\n",
       "      <td>0.882353</td>\n",
       "      <td>0.025723</td>\n",
       "      <td>0.334405</td>\n",
       "      <td>0.545882</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.450861</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.320</td>\n",
       "      <td>0.474790</td>\n",
       "      <td>0.875536</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.063830</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.053333</td>\n",
       "      <td>0.645733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.280614</td>\n",
       "      <td>0.832664</td>\n",
       "      <td>0.882353</td>\n",
       "      <td>0.025723</td>\n",
       "      <td>0.334405</td>\n",
       "      <td>0.545882</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.450861</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.320</td>\n",
       "      <td>0.474790</td>\n",
       "      <td>0.875536</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.489362</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.053333</td>\n",
       "      <td>0.480578</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Ni        Cr        Co        Fe        Al        Ti   Nb        Mo  \\\n",
       "3  0.355671  0.712182  0.834641  0.052436  0.321543  0.449412  0.0  0.432624   \n",
       "4  0.355671  0.712182  0.834641  0.052436  0.321543  0.449412  0.0  0.432624   \n",
       "6  0.355671  0.712182  0.834641  0.052436  0.321543  0.449412  0.0  0.432624   \n",
       "8  0.280614  0.832664  0.882353  0.025723  0.334405  0.545882  0.0  0.450861   \n",
       "9  0.280614  0.832664  0.882353  0.025723  0.334405  0.545882  0.0  0.450861   \n",
       "\n",
       "     W         C  ...   Zr  Test temperature (℃)  Test stress (Mpa)  \\\n",
       "3  0.0  0.338462  ...  0.0                 0.328           0.440126   \n",
       "4  0.0  0.338462  ...  0.0                 0.660           0.243697   \n",
       "6  0.0  0.338462  ...  0.0                 0.660           0.243697   \n",
       "8  0.0  0.307692  ...  0.0                 0.320           0.474790   \n",
       "9  0.0  0.307692  ...  0.0                 0.320           0.474790   \n",
       "\n",
       "   solution treatment temperature  solution treatment time  \\\n",
       "3                        0.875536                     0.25   \n",
       "4                        0.875536                     0.25   \n",
       "6                        0.927039                     0.25   \n",
       "8                        0.875536                     0.25   \n",
       "9                        0.875536                     0.25   \n",
       "\n",
       "   Stable aging temperature (℃)  Stable aging time (h)  Aging temperature (℃)  \\\n",
       "3                          0.78               0.063830                    1.0   \n",
       "4                          0.78               0.063830                    1.0   \n",
       "6                          0.78               0.489362                    1.0   \n",
       "8                          0.78               0.063830                    1.0   \n",
       "9                          0.78               0.489362                    1.0   \n",
       "\n",
       "   Aging time (h)  Creep rupture life  \n",
       "3        0.053333            0.540244  \n",
       "4        0.053333            0.362418  \n",
       "6        0.053333            0.681577  \n",
       "8        0.053333            0.645733  \n",
       "9        0.053333            0.480578  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📈 Datenstatistiken:\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Ni",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Cr",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Co",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Fe",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Al",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Ti",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Nb",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Mo",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "W",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "C",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "B",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Zr",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Test temperature (℃)",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Test stress (Mpa)",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "solution treatment temperature",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "solution treatment time",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Stable aging temperature (℃)",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Stable aging time (h)",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Aging temperature (℃)",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Aging time (h)",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Creep rupture life",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "cc69b6cf-cdf0-4b4b-a531-f76dcb8625ef",
       "rows": [
        [
         "count",
         "156.0",
         "156.0",
         "156.0",
         "156.0",
         "156.0",
         "156.0",
         "156.0",
         "156.0",
         "156.0",
         "156.0",
         "156.0",
         "156.0",
         "156.0",
         "156.0",
         "156.0",
         "156.0",
         "156.0",
         "156.0",
         "156.0",
         "156.0",
         "156.0"
        ],
        [
         "mean",
         "0.19876827498653846",
         "0.6814608862878204",
         "0.45983743924935905",
         "0.37997063617628213",
         "0.3261810536775641",
         "0.3460482654480769",
         "0.418280928326282",
         "0.4351375574852564",
         "0.1692307692352564",
         "0.3939842209166666",
         "0.26337606837435895",
         "0.3318681318711539",
         "0.3084871794871795",
         "0.5141941392019229",
         "0.7741003631634616",
         "0.14483173076923078",
         "0.4764102564102565",
         "0.17144026185576924",
         "0.5865384615442307",
         "0.045042735047435896",
         "0.6132588220903846"
        ],
        [
         "std",
         "0.1790558946509846",
         "0.2262577205064043",
         "0.39423730457207107",
         "0.40609158168947107",
         "0.25467642897640974",
         "0.2931605360271718",
         "0.43829500350871875",
         "0.2206124847305529",
         "0.28973027777577776",
         "0.2673936539850285",
         "0.1895676980464552",
         "0.39460431312188227",
         "0.3268193836234219",
         "0.23311299341607486",
         "0.3194728084506276",
         "0.1346144499369359",
         "0.2583469752794928",
         "0.13668058919637294",
         "0.4279805640443697",
         "0.13649030696765743",
         "0.19933923833422684"
        ],
        [
         "min",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0"
        ],
        [
         "25%",
         "0.07722016155",
         "0.42436412317500005",
         "0.0",
         "0.0074202325",
         "0.063504823175",
         "0.0164705882",
         "0.0",
         "0.3100303951",
         "0.0",
         "0.2307692308",
         "0.1333333333",
         "0.0",
         "0.08",
         "0.4737394958",
         "0.8240343348",
         "0.0625",
         "0.28",
         "0.0638297872",
         "0.0",
         "0.0",
         "0.5002926331750001"
        ],
        [
         "50%",
         "0.2176144977",
         "0.7951807229",
         "0.5555555556",
         "0.1978728667",
         "0.32958199355",
         "0.48",
         "0.200729927",
         "0.3444782168",
         "0.0",
         "0.3538461538",
         "0.2",
         "0.0",
         "0.2",
         "0.6218487395",
         "0.8841201717",
         "0.0625",
         "0.44",
         "0.1489361702",
         "0.8157894737",
         "0.0266666667",
         "0.6036703668"
        ],
        [
         "75%",
         "0.2894827669",
         "0.8058902276",
         "0.8496732026",
         "0.8706406134",
         "0.553858520925",
         "0.5458823529",
         "0.954379562",
         "0.4478216819",
         "0.275000000025",
         "0.5076923077",
         "0.3666666667",
         "0.7142857143",
         "0.328",
         "0.6323529412",
         "0.9356223176",
         "0.25",
         "0.78",
         "0.1489361702",
         "0.8947368421",
         "0.0533333333",
         "0.741528300575"
        ],
        [
         "max",
         "1.0",
         "1.0",
         "1.0",
         "1.0",
         "1.0",
         "1.0",
         "1.0",
         "1.0",
         "1.0",
         "1.0",
         "1.0",
         "1.0",
         "1.0",
         "1.0",
         "1.0",
         "1.0",
         "1.0",
         "1.0",
         "1.0",
         "1.0",
         "1.0"
        ]
       ],
       "shape": {
        "columns": 21,
        "rows": 8
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Ni</th>\n",
       "      <th>Cr</th>\n",
       "      <th>Co</th>\n",
       "      <th>Fe</th>\n",
       "      <th>Al</th>\n",
       "      <th>Ti</th>\n",
       "      <th>Nb</th>\n",
       "      <th>Mo</th>\n",
       "      <th>W</th>\n",
       "      <th>C</th>\n",
       "      <th>...</th>\n",
       "      <th>Zr</th>\n",
       "      <th>Test temperature (℃)</th>\n",
       "      <th>Test stress (Mpa)</th>\n",
       "      <th>solution treatment temperature</th>\n",
       "      <th>solution treatment time</th>\n",
       "      <th>Stable aging temperature (℃)</th>\n",
       "      <th>Stable aging time (h)</th>\n",
       "      <th>Aging temperature (℃)</th>\n",
       "      <th>Aging time (h)</th>\n",
       "      <th>Creep rupture life</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>156.000000</td>\n",
       "      <td>156.000000</td>\n",
       "      <td>156.000000</td>\n",
       "      <td>156.000000</td>\n",
       "      <td>156.000000</td>\n",
       "      <td>156.000000</td>\n",
       "      <td>156.000000</td>\n",
       "      <td>156.000000</td>\n",
       "      <td>156.000000</td>\n",
       "      <td>156.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>156.000000</td>\n",
       "      <td>156.000000</td>\n",
       "      <td>156.000000</td>\n",
       "      <td>156.000000</td>\n",
       "      <td>156.000000</td>\n",
       "      <td>156.000000</td>\n",
       "      <td>156.000000</td>\n",
       "      <td>156.000000</td>\n",
       "      <td>156.000000</td>\n",
       "      <td>156.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.198768</td>\n",
       "      <td>0.681461</td>\n",
       "      <td>0.459837</td>\n",
       "      <td>0.379971</td>\n",
       "      <td>0.326181</td>\n",
       "      <td>0.346048</td>\n",
       "      <td>0.418281</td>\n",
       "      <td>0.435138</td>\n",
       "      <td>0.169231</td>\n",
       "      <td>0.393984</td>\n",
       "      <td>...</td>\n",
       "      <td>0.331868</td>\n",
       "      <td>0.308487</td>\n",
       "      <td>0.514194</td>\n",
       "      <td>0.774100</td>\n",
       "      <td>0.144832</td>\n",
       "      <td>0.476410</td>\n",
       "      <td>0.171440</td>\n",
       "      <td>0.586538</td>\n",
       "      <td>0.045043</td>\n",
       "      <td>0.613259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.179056</td>\n",
       "      <td>0.226258</td>\n",
       "      <td>0.394237</td>\n",
       "      <td>0.406092</td>\n",
       "      <td>0.254676</td>\n",
       "      <td>0.293161</td>\n",
       "      <td>0.438295</td>\n",
       "      <td>0.220612</td>\n",
       "      <td>0.289730</td>\n",
       "      <td>0.267394</td>\n",
       "      <td>...</td>\n",
       "      <td>0.394604</td>\n",
       "      <td>0.326819</td>\n",
       "      <td>0.233113</td>\n",
       "      <td>0.319473</td>\n",
       "      <td>0.134614</td>\n",
       "      <td>0.258347</td>\n",
       "      <td>0.136681</td>\n",
       "      <td>0.427981</td>\n",
       "      <td>0.136490</td>\n",
       "      <td>0.199339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.077220</td>\n",
       "      <td>0.424364</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007420</td>\n",
       "      <td>0.063505</td>\n",
       "      <td>0.016471</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.310030</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.080000</td>\n",
       "      <td>0.473739</td>\n",
       "      <td>0.824034</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.280000</td>\n",
       "      <td>0.063830</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.217614</td>\n",
       "      <td>0.795181</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.197873</td>\n",
       "      <td>0.329582</td>\n",
       "      <td>0.480000</td>\n",
       "      <td>0.200730</td>\n",
       "      <td>0.344478</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.353846</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.621849</td>\n",
       "      <td>0.884120</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.440000</td>\n",
       "      <td>0.148936</td>\n",
       "      <td>0.815789</td>\n",
       "      <td>0.026667</td>\n",
       "      <td>0.603670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.289483</td>\n",
       "      <td>0.805890</td>\n",
       "      <td>0.849673</td>\n",
       "      <td>0.870641</td>\n",
       "      <td>0.553859</td>\n",
       "      <td>0.545882</td>\n",
       "      <td>0.954380</td>\n",
       "      <td>0.447822</td>\n",
       "      <td>0.275000</td>\n",
       "      <td>0.507692</td>\n",
       "      <td>...</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.328000</td>\n",
       "      <td>0.632353</td>\n",
       "      <td>0.935622</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.780000</td>\n",
       "      <td>0.148936</td>\n",
       "      <td>0.894737</td>\n",
       "      <td>0.053333</td>\n",
       "      <td>0.741528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               Ni          Cr          Co          Fe          Al          Ti  \\\n",
       "count  156.000000  156.000000  156.000000  156.000000  156.000000  156.000000   \n",
       "mean     0.198768    0.681461    0.459837    0.379971    0.326181    0.346048   \n",
       "std      0.179056    0.226258    0.394237    0.406092    0.254676    0.293161   \n",
       "min      0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
       "25%      0.077220    0.424364    0.000000    0.007420    0.063505    0.016471   \n",
       "50%      0.217614    0.795181    0.555556    0.197873    0.329582    0.480000   \n",
       "75%      0.289483    0.805890    0.849673    0.870641    0.553859    0.545882   \n",
       "max      1.000000    1.000000    1.000000    1.000000    1.000000    1.000000   \n",
       "\n",
       "               Nb          Mo           W           C  ...          Zr  \\\n",
       "count  156.000000  156.000000  156.000000  156.000000  ...  156.000000   \n",
       "mean     0.418281    0.435138    0.169231    0.393984  ...    0.331868   \n",
       "std      0.438295    0.220612    0.289730    0.267394  ...    0.394604   \n",
       "min      0.000000    0.000000    0.000000    0.000000  ...    0.000000   \n",
       "25%      0.000000    0.310030    0.000000    0.230769  ...    0.000000   \n",
       "50%      0.200730    0.344478    0.000000    0.353846  ...    0.000000   \n",
       "75%      0.954380    0.447822    0.275000    0.507692  ...    0.714286   \n",
       "max      1.000000    1.000000    1.000000    1.000000  ...    1.000000   \n",
       "\n",
       "       Test temperature (℃)  Test stress (Mpa)  \\\n",
       "count            156.000000         156.000000   \n",
       "mean               0.308487           0.514194   \n",
       "std                0.326819           0.233113   \n",
       "min                0.000000           0.000000   \n",
       "25%                0.080000           0.473739   \n",
       "50%                0.200000           0.621849   \n",
       "75%                0.328000           0.632353   \n",
       "max                1.000000           1.000000   \n",
       "\n",
       "       solution treatment temperature  solution treatment time  \\\n",
       "count                      156.000000               156.000000   \n",
       "mean                         0.774100                 0.144832   \n",
       "std                          0.319473                 0.134614   \n",
       "min                          0.000000                 0.000000   \n",
       "25%                          0.824034                 0.062500   \n",
       "50%                          0.884120                 0.062500   \n",
       "75%                          0.935622                 0.250000   \n",
       "max                          1.000000                 1.000000   \n",
       "\n",
       "       Stable aging temperature (℃)  Stable aging time (h)  \\\n",
       "count                    156.000000             156.000000   \n",
       "mean                       0.476410               0.171440   \n",
       "std                        0.258347               0.136681   \n",
       "min                        0.000000               0.000000   \n",
       "25%                        0.280000               0.063830   \n",
       "50%                        0.440000               0.148936   \n",
       "75%                        0.780000               0.148936   \n",
       "max                        1.000000               1.000000   \n",
       "\n",
       "       Aging temperature (℃)  Aging time (h)  Creep rupture life  \n",
       "count             156.000000      156.000000          156.000000  \n",
       "mean                0.586538        0.045043            0.613259  \n",
       "std                 0.427981        0.136490            0.199339  \n",
       "min                 0.000000        0.000000            0.000000  \n",
       "25%                 0.000000        0.000000            0.500293  \n",
       "50%                 0.815789        0.026667            0.603670  \n",
       "75%                 0.894737        0.053333            0.741528  \n",
       "max                 1.000000        1.000000            1.000000  \n",
       "\n",
       "[8 rows x 21 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ===== EXAKT GLEICHE DATENLADUNG WIE NN =====\n",
    "print(\"🔄 Lade exakt die gleichen Daten wie das Neural Network...\")\n",
    "\n",
    "# Exakt gleicher Pfad wie im NN\n",
    "json_path = \"./Data/data2_scaled_custom_new.json\"\n",
    "\n",
    "with open(json_path, \"r\") as f:\n",
    "    raw_data = json.load(f)\n",
    "\n",
    "# Erstelle DataFrame (exakt wie im NN)\n",
    "df = pd.DataFrame(raw_data)\n",
    "print(f\"✅ Daten geladen: {df.shape}\")\n",
    "print(f\"📋 Features: {list(df.columns)}\")\n",
    "print(f\"🎯 Target: 'Creep rupture life'\")\n",
    "\n",
    "# Zeige erste paar Zeilen\n",
    "print(\"\\n📊 Erste 5 Zeilen der Daten:\")\n",
    "display(df.head())\n",
    "\n",
    "print(f\"\\n📈 Datenstatistiken:\")\n",
    "display(df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2c925209",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Erstelle exakt die gleiche Datenaufteilung wie das Neural Network...\n",
      "✅ IDENTISCHE Datenaufteilung erstellt:\n",
      "  Training:     93 Samples (59.6%)\n",
      "  Validation:   31 Samples (19.9%)\n",
      "  Test:         32 Samples (20.5%)\n",
      "  Total:       156 Samples\n",
      "🔒 Gleiche Splits wie Neural Network garantiert!\n",
      "\n",
      "📋 Gespeicherte Indizes für Verifikation:\n",
      "  Train indices: 93 (z.B. [113, 49, 86, 103, 117]...)\n",
      "  Val indices:   31 (z.B. [67, 25, 64, 140, 73]...)\n",
      "  Test indices:  32 (z.B. [96, 69, 82, 76, 114]...)\n"
     ]
    }
   ],
   "source": [
    "# ===== EXAKT GLEICHE DATENAUFTEILUNG WIE NN =====\n",
    "print(\"🔄 Erstelle exakt die gleiche Datenaufteilung wie das Neural Network...\")\n",
    "\n",
    "def create_identical_data_splits(df, test_size=0.2, val_size=0.2, random_state=42):\n",
    "    \"\"\"\n",
    "    Erstellt EXAKT die gleiche Datenaufteilung wie im Neural Network\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame mit allen Daten\n",
    "        test_size: Anteil Test-Set (default: 20%)\n",
    "        val_size: Anteil Validation-Set (default: 20%) \n",
    "        random_state: Für Reproduzierbarkeit (42 wie im NN)\n",
    "        \n",
    "    Returns:\n",
    "        train_df, val_df, test_df\n",
    "    \"\"\"\n",
    "    \n",
    "    # Gesamte Indizes\n",
    "    all_indices = list(range(len(df)))\n",
    "    \n",
    "    # Erster Split: Train+Val vs Test (EXAKT wie im NN)\n",
    "    train_val_indices, test_indices = train_test_split(\n",
    "        all_indices, \n",
    "        test_size=test_size, \n",
    "        random_state=random_state,\n",
    "        shuffle=True\n",
    "    )\n",
    "    \n",
    "    # Zweiter Split: Train vs Validation (EXAKT wie im NN)\n",
    "    val_size_adjusted = val_size / (1 - test_size)\n",
    "    \n",
    "    train_indices, val_indices = train_test_split(\n",
    "        train_val_indices,\n",
    "        test_size=val_size_adjusted,\n",
    "        random_state=random_state,\n",
    "        shuffle=True\n",
    "    )\n",
    "    \n",
    "    # Erstelle DataFrames basierend auf Indizes\n",
    "    train_df = df.iloc[train_indices].copy()\n",
    "    val_df = df.iloc[val_indices].copy()\n",
    "    test_df = df.iloc[test_indices].copy()\n",
    "    \n",
    "    print(f\"✅ IDENTISCHE Datenaufteilung erstellt:\")\n",
    "    print(f\"  Training:   {len(train_df):4d} Samples ({len(train_df)/len(df)*100:.1f}%)\")\n",
    "    print(f\"  Validation: {len(val_df):4d} Samples ({len(val_df)/len(df)*100:.1f}%)\")\n",
    "    print(f\"  Test:       {len(test_df):4d} Samples ({len(test_df)/len(df)*100:.1f}%)\")\n",
    "    print(f\"  Total:      {len(df):4d} Samples\")\n",
    "    print(f\"🔒 Gleiche Splits wie Neural Network garantiert!\")\n",
    "    \n",
    "    return train_df, val_df, test_df, train_indices, val_indices, test_indices\n",
    "\n",
    "# Erstelle identische Aufteilung\n",
    "train_df, val_df, test_df, train_indices, val_indices, test_indices = create_identical_data_splits(df)\n",
    "\n",
    "# Speichere Indizes für Verifikation\n",
    "print(f\"\\n📋 Gespeicherte Indizes für Verifikation:\")\n",
    "print(f\"  Train indices: {len(train_indices)} (z.B. {train_indices[:5]}...)\")\n",
    "print(f\"  Val indices:   {len(val_indices)} (z.B. {val_indices[:5]}...)\")\n",
    "print(f\"  Test indices:  {len(test_indices)} (z.B. {test_indices[:5]}...)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a5d39f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🤖 Setup AutoGluon AutoML mit den identischen Daten...\n",
      "📁 Output Directory: /home/bt708583/ml_in_ms_wt24/AdvancedModule/AutoGluon_Models/automl_20250930_130252\n",
      "\n",
      "📊 Daten für AutoGluon:\n",
      "  Training (inkl. internal validation): 124 Samples\n",
      "  Test (für finale Evaluation):        32 Samples\n",
      "  Target: Creep rupture life\n",
      "  Features: 20\n",
      "✅ AutoGluon Datasets erstellt!\n"
     ]
    }
   ],
   "source": [
    "# ===== AUTOGLUON SETUP =====\n",
    "print(\"🤖 Setup AutoGluon AutoML mit den identischen Daten...\")\n",
    "\n",
    "# Definiere Output-Directory\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "output_dir = f\"./AutoML/automl_{timestamp}\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "print(f\"📁 Output Directory: {output_dir}\")\n",
    "\n",
    "# Target-Spalte\n",
    "target_column = 'Creep rupture life'\n",
    "\n",
    "# Kombiniere Train + Validation für AutoGluon Training\n",
    "# (AutoGluon macht intern weitere Splits für Validation)\n",
    "train_combined_df = pd.concat([train_df, val_df], ignore_index=True)\n",
    "\n",
    "print(f\"\\n📊 Daten für AutoGluon:\")\n",
    "print(f\"  Training (inkl. internal validation): {len(train_combined_df)} Samples\")\n",
    "print(f\"  Test (für finale Evaluation):        {len(test_df)} Samples\")\n",
    "print(f\"  Target: {target_column}\")\n",
    "print(f\"  Features: {len(train_combined_df.columns)-1}\")\n",
    "\n",
    "# Konvertiere zu TabularDataset\n",
    "train_data = TabularDataset(train_combined_df)\n",
    "test_data = TabularDataset(test_df)\n",
    "\n",
    "print(\"✅ AutoGluon Datasets erstellt!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38021ad7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No path specified. Models will be saved in: \"AutogluonModels/ag-20250930_130252\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.2\n",
      "Python Version:     3.12.3\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #140-Ubuntu SMP Wed Dec 18 17:59:53 UTC 2024\n",
      "CPU Count:          192\n",
      "Memory Avail:       211.17 GB / 1007.45 GB (21.0%)\n",
      "Disk Space Avail:   885.81 GB / 7096.34 GB (12.5%)\n",
      "===================================================\n",
      "Presets specified: ['best_quality']\n",
      "Setting dynamic_stacking from 'auto' to True. Reason: Enable dynamic_stacking when use_bag_holdout is disabled. (use_bag_holdout=False)\n",
      "Stack configuration (auto_stack=True): num_stack_levels=1, num_bag_folds=8, num_bag_sets=1\n",
      "DyStack is enabled (dynamic_stacking=True). AutoGluon will try to determine whether the input data is affected by stacked overfitting and enable or disable stacking as a consequence.\n",
      "\tThis is used to identify the optimal `num_stack_levels` value. Copies of AutoGluon will be fit on subsets of the data. Then holdout validation data is used to detect stacked overfitting.\n",
      "\tRunning DyStack for up to 1800s of the 7200s of remaining time (25%).\n",
      "\t\tContext path: \"/home/bt708583/ml_in_ms_wt24/AdvancedModule/AutogluonModels/ag-20250930_130252/ds_sub_fit/sub_fit_ho\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.2\n",
      "Python Version:     3.12.3\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #140-Ubuntu SMP Wed Dec 18 17:59:53 UTC 2024\n",
      "CPU Count:          192\n",
      "Memory Avail:       211.17 GB / 1007.45 GB (21.0%)\n",
      "Disk Space Avail:   885.81 GB / 7096.34 GB (12.5%)\n",
      "===================================================\n",
      "Presets specified: ['best_quality']\n",
      "Setting dynamic_stacking from 'auto' to True. Reason: Enable dynamic_stacking when use_bag_holdout is disabled. (use_bag_holdout=False)\n",
      "Stack configuration (auto_stack=True): num_stack_levels=1, num_bag_folds=8, num_bag_sets=1\n",
      "DyStack is enabled (dynamic_stacking=True). AutoGluon will try to determine whether the input data is affected by stacked overfitting and enable or disable stacking as a consequence.\n",
      "\tThis is used to identify the optimal `num_stack_levels` value. Copies of AutoGluon will be fit on subsets of the data. Then holdout validation data is used to detect stacked overfitting.\n",
      "\tRunning DyStack for up to 1800s of the 7200s of remaining time (25%).\n",
      "\t\tContext path: \"/home/bt708583/ml_in_ms_wt24/AdvancedModule/AutogluonModels/ag-20250930_130252/ds_sub_fit/sub_fit_ho\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Starte AutoGluon AutoML Training...\n",
      "⏰ Starte Training mit 15-Minuten Zeit-Budget...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Leaderboard on holdout data (DyStack):\n",
      "                     model  score_holdout  score_val              eval_metric  pred_time_test  pred_time_val    fit_time  pred_time_test_marginal  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
      "0    KNeighborsDist_BAG_L1      -0.136777  -0.149833  root_mean_squared_error        0.007456       0.008213    0.001925                 0.007456                0.008213           0.001925            1       True          2\n",
      "1          CatBoost_BAG_L1      -0.153390  -0.126817  root_mean_squared_error        0.361397       0.015545    5.059904                 0.361397                0.015545           5.059904            1       True          6\n",
      "2    KNeighborsUnif_BAG_L1      -0.156984  -0.157894  root_mean_squared_error        0.004444       0.038215    0.002399                 0.004444                0.038215           0.002399            1       True          1\n",
      "3           XGBoost_BAG_L2      -0.169104  -0.133990  root_mean_squared_error        1.172206       2.788886  486.214711                 0.205532                0.388389          31.249151            2       True         17\n",
      "4          CatBoost_BAG_L2      -0.169487  -0.120966  root_mean_squared_error        0.991883       2.423167  459.710189                 0.025208                0.022670           4.744629            2       True         14\n",
      "5          LightGBM_BAG_L1      -0.169753  -0.119686  root_mean_squared_error        0.020597       0.180683  370.706151                 0.020597                0.180683         370.706151            1       True          4\n",
      "6        LightGBMXT_BAG_L1      -0.169883  -0.120058  root_mean_squared_error        0.454413       0.195262  734.470147                 0.454413                0.195262         734.470147            1       True          3\n",
      "7   NeuralNetFastAI_BAG_L2      -0.172877  -0.115523  root_mean_squared_error        1.039064       3.621025  606.428168                 0.072390                1.220527         151.462608            2       True         16\n",
      "8      WeightedEnsemble_L3      -0.174177  -0.111998  root_mean_squared_error        1.065796       3.643960  611.180214                 0.001523                0.000265           0.007417            3       True         18\n",
      "9      WeightedEnsemble_L2      -0.177077  -0.116145  root_mean_squared_error        0.056855       0.243960  371.136102                 0.001109                0.000241           0.007388            2       True         10\n",
      "10    ExtraTreesMSE_BAG_L2      -0.177361  -0.127357  root_mean_squared_error        1.001173       2.493523  455.350503                 0.034499                0.093025           0.384943            2       True         15\n",
      "11  NeuralNetFastAI_BAG_L1      -0.178936  -0.149636  root_mean_squared_error        0.298151       1.691441   65.184871                 0.298151                1.691441          65.184871            1       True          8\n",
      "12  RandomForestMSE_BAG_L2      -0.179785  -0.133472  root_mean_squared_error        1.005534       2.475814  455.371042                 0.038860                0.075316           0.405482            2       True         13\n",
      "13         LightGBM_BAG_L2      -0.180216  -0.133875  root_mean_squared_error        0.974988       2.515488  659.255675                 0.008313                0.114990         204.290115            2       True         12\n",
      "14    ExtraTreesMSE_BAG_L1      -0.182975  -0.131652  root_mean_squared_error        0.039325       0.080398    0.338680                 0.039325                0.080398           0.338680            1       True          7\n",
      "15       LightGBMXT_BAG_L2      -0.184995  -0.123423  root_mean_squared_error        0.980771       2.583577  648.483659                 0.014097                0.183079         193.518099            2       True         11\n",
      "16  RandomForestMSE_BAG_L1      -0.190766  -0.123683  root_mean_squared_error        0.035150       0.063036    0.422563                 0.035150                0.063036           0.422563            1       True          5\n",
      "17          XGBoost_BAG_L1      -0.192917  -0.158040  root_mean_squared_error        0.204600       0.361182   13.251466                 0.204600                0.361182          13.251466            1       True          9\n",
      "\t1\t = Optimal   num_stack_levels (Stacked Overfitting Occurred: False)\n",
      "\t1802s\t = DyStack   runtime |\t5398s\t = Remaining runtime\n",
      "Starting main fit with num_stack_levels=1.\n",
      "\tFor future fit calls on this dataset, you can skip DyStack to save time: `predictor.fit(..., dynamic_stacking=False, num_stack_levels=1)`\n",
      "Beginning AutoGluon training ... Time limit = 5398s\n",
      "AutoGluon will save models to \"/home/bt708583/ml_in_ms_wt24/AdvancedModule/AutogluonModels/ag-20250930_130252\"\n",
      "Train Data Rows:    124\n",
      "Train Data Columns: 20\n",
      "Label Column:       Creep rupture life\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "                     model  score_holdout  score_val              eval_metric  pred_time_test  pred_time_val    fit_time  pred_time_test_marginal  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
      "0    KNeighborsDist_BAG_L1      -0.136777  -0.149833  root_mean_squared_error        0.007456       0.008213    0.001925                 0.007456                0.008213           0.001925            1       True          2\n",
      "1          CatBoost_BAG_L1      -0.153390  -0.126817  root_mean_squared_error        0.361397       0.015545    5.059904                 0.361397                0.015545           5.059904            1       True          6\n",
      "2    KNeighborsUnif_BAG_L1      -0.156984  -0.157894  root_mean_squared_error        0.004444       0.038215    0.002399                 0.004444                0.038215           0.002399            1       True          1\n",
      "3           XGBoost_BAG_L2      -0.169104  -0.133990  root_mean_squared_error        1.172206       2.788886  486.214711                 0.205532                0.388389          31.249151            2       True         17\n",
      "4          CatBoost_BAG_L2      -0.169487  -0.120966  root_mean_squared_error        0.991883       2.423167  459.710189                 0.025208                0.022670           4.744629            2       True         14\n",
      "5          LightGBM_BAG_L1      -0.169753  -0.119686  root_mean_squared_error        0.020597       0.180683  370.706151                 0.020597                0.180683         370.706151            1       True          4\n",
      "6        LightGBMXT_BAG_L1      -0.169883  -0.120058  root_mean_squared_error        0.454413       0.195262  734.470147                 0.454413                0.195262         734.470147            1       True          3\n",
      "7   NeuralNetFastAI_BAG_L2      -0.172877  -0.115523  root_mean_squared_error        1.039064       3.621025  606.428168                 0.072390                1.220527         151.462608            2       True         16\n",
      "8      WeightedEnsemble_L3      -0.174177  -0.111998  root_mean_squared_error        1.065796       3.643960  611.180214                 0.001523                0.000265           0.007417            3       True         18\n",
      "9      WeightedEnsemble_L2      -0.177077  -0.116145  root_mean_squared_error        0.056855       0.243960  371.136102                 0.001109                0.000241           0.007388            2       True         10\n",
      "10    ExtraTreesMSE_BAG_L2      -0.177361  -0.127357  root_mean_squared_error        1.001173       2.493523  455.350503                 0.034499                0.093025           0.384943            2       True         15\n",
      "11  NeuralNetFastAI_BAG_L1      -0.178936  -0.149636  root_mean_squared_error        0.298151       1.691441   65.184871                 0.298151                1.691441          65.184871            1       True          8\n",
      "12  RandomForestMSE_BAG_L2      -0.179785  -0.133472  root_mean_squared_error        1.005534       2.475814  455.371042                 0.038860                0.075316           0.405482            2       True         13\n",
      "13         LightGBM_BAG_L2      -0.180216  -0.133875  root_mean_squared_error        0.974988       2.515488  659.255675                 0.008313                0.114990         204.290115            2       True         12\n",
      "14    ExtraTreesMSE_BAG_L1      -0.182975  -0.131652  root_mean_squared_error        0.039325       0.080398    0.338680                 0.039325                0.080398           0.338680            1       True          7\n",
      "15       LightGBMXT_BAG_L2      -0.184995  -0.123423  root_mean_squared_error        0.980771       2.583577  648.483659                 0.014097                0.183079         193.518099            2       True         11\n",
      "16  RandomForestMSE_BAG_L1      -0.190766  -0.123683  root_mean_squared_error        0.035150       0.063036    0.422563                 0.035150                0.063036           0.422563            1       True          5\n",
      "17          XGBoost_BAG_L1      -0.192917  -0.158040  root_mean_squared_error        0.204600       0.361182   13.251466                 0.204600                0.361182          13.251466            1       True          9\n",
      "\t1\t = Optimal   num_stack_levels (Stacked Overfitting Occurred: False)\n",
      "\t1802s\t = DyStack   runtime |\t5398s\t = Remaining runtime\n",
      "Starting main fit with num_stack_levels=1.\n",
      "\tFor future fit calls on this dataset, you can skip DyStack to save time: `predictor.fit(..., dynamic_stacking=False, num_stack_levels=1)`\n",
      "Beginning AutoGluon training ... Time limit = 5398s\n",
      "AutoGluon will save models to \"/home/bt708583/ml_in_ms_wt24/AdvancedModule/AutogluonModels/ag-20250930_130252\"\n",
      "Train Data Rows:    124\n",
      "Train Data Columns: 20\n",
      "Label Column:       Creep rupture life\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    157901.67 MB\n",
      "\tTrain Data (Original)  Memory Usage: 0.02 MB (0.0% of available memory)\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    157901.67 MB\n",
      "\tTrain Data (Original)  Memory Usage: 0.02 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 20 | ['Ni', 'Cr', 'Co', 'Fe', 'Al', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 20 | ['Ni', 'Cr', 'Co', 'Fe', 'Al', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 20 | ['Ni', 'Cr', 'Co', 'Fe', 'Al', ...]\n",
      "\t0.0s = Fit runtime\n",
      "\t20 features in original data used to generate 20 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 0.02 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.06s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Large model count detected (112 configs) ... Only displaying the first 3 models of each family. To see all, set `verbosity=3`.\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}, {'activation': 'elu', 'dropout_prob': 0.10077639529843717, 'hidden_size': 108, 'learning_rate': 0.002735937344002146, 'num_layers': 4, 'use_batchnorm': True, 'weight_decay': 1.356433327634438e-12, 'ag_args': {'name_suffix': '_r79', 'priority': -2}}, {'activation': 'elu', 'dropout_prob': 0.11897478034205347, 'hidden_size': 213, 'learning_rate': 0.0010474382260641949, 'num_layers': 4, 'use_batchnorm': False, 'weight_decay': 5.594471067786272e-10, 'ag_args': {'name_suffix': '_r22', 'priority': -7}}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}, {'depth': 6, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 2.1542798306067823, 'learning_rate': 0.06864209415792857, 'max_ctr_complexity': 4, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r177', 'priority': -1}}, {'depth': 8, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 2.7997999596449104, 'learning_rate': 0.031375015734637225, 'max_ctr_complexity': 2, 'one_hot_max_size': 3, 'ag_args': {'name_suffix': '_r9', 'priority': -5}}],\n",
      "\t'XGB': [{}, {'colsample_bytree': 0.6917311125174739, 'enable_categorical': False, 'learning_rate': 0.018063876087523967, 'max_depth': 10, 'min_child_weight': 0.6028633586934382, 'ag_args': {'name_suffix': '_r33', 'priority': -8}}, {'colsample_bytree': 0.6628423832084077, 'enable_categorical': False, 'learning_rate': 0.08775715546881824, 'max_depth': 5, 'min_child_weight': 0.6294123374222513, 'ag_args': {'name_suffix': '_r89', 'priority': -16}}],\n",
      "\t'FASTAI': [{}, {'bs': 256, 'emb_drop': 0.5411770367537934, 'epochs': 43, 'layers': [800, 400], 'lr': 0.01519848858318159, 'ps': 0.23782946566604385, 'ag_args': {'name_suffix': '_r191', 'priority': -4}}, {'bs': 2048, 'emb_drop': 0.05070411322605811, 'epochs': 29, 'layers': [200, 100], 'lr': 0.08974235041576624, 'ps': 0.10393466140748028, 'ag_args': {'name_suffix': '_r102', 'priority': -11}}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "\t\t('float', []) : 20 | ['Ni', 'Cr', 'Co', 'Fe', 'Al', ...]\n",
      "\t0.0s = Fit runtime\n",
      "\t20 features in original data used to generate 20 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 0.02 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.06s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Large model count detected (112 configs) ... Only displaying the first 3 models of each family. To see all, set `verbosity=3`.\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}, {'activation': 'elu', 'dropout_prob': 0.10077639529843717, 'hidden_size': 108, 'learning_rate': 0.002735937344002146, 'num_layers': 4, 'use_batchnorm': True, 'weight_decay': 1.356433327634438e-12, 'ag_args': {'name_suffix': '_r79', 'priority': -2}}, {'activation': 'elu', 'dropout_prob': 0.11897478034205347, 'hidden_size': 213, 'learning_rate': 0.0010474382260641949, 'num_layers': 4, 'use_batchnorm': False, 'weight_decay': 5.594471067786272e-10, 'ag_args': {'name_suffix': '_r22', 'priority': -7}}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}, {'depth': 6, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 2.1542798306067823, 'learning_rate': 0.06864209415792857, 'max_ctr_complexity': 4, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r177', 'priority': -1}}, {'depth': 8, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 2.7997999596449104, 'learning_rate': 0.031375015734637225, 'max_ctr_complexity': 2, 'one_hot_max_size': 3, 'ag_args': {'name_suffix': '_r9', 'priority': -5}}],\n",
      "\t'XGB': [{}, {'colsample_bytree': 0.6917311125174739, 'enable_categorical': False, 'learning_rate': 0.018063876087523967, 'max_depth': 10, 'min_child_weight': 0.6028633586934382, 'ag_args': {'name_suffix': '_r33', 'priority': -8}}, {'colsample_bytree': 0.6628423832084077, 'enable_categorical': False, 'learning_rate': 0.08775715546881824, 'max_depth': 5, 'min_child_weight': 0.6294123374222513, 'ag_args': {'name_suffix': '_r89', 'priority': -16}}],\n",
      "\t'FASTAI': [{}, {'bs': 256, 'emb_drop': 0.5411770367537934, 'epochs': 43, 'layers': [800, 400], 'lr': 0.01519848858318159, 'ps': 0.23782946566604385, 'ag_args': {'name_suffix': '_r191', 'priority': -4}}, {'bs': 2048, 'emb_drop': 0.05070411322605811, 'epochs': 29, 'layers': [200, 100], 'lr': 0.08974235041576624, 'ps': 0.10393466140748028, 'ag_args': {'name_suffix': '_r102', 'priority': -11}}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
      "Fitting 108 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: KNeighborsUnif_BAG_L1 ... Training model for up to 3597.87s of the 5398.14s of remaining time.\n",
      "AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
      "Fitting 108 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: KNeighborsUnif_BAG_L1 ... Training model for up to 3597.87s of the 5398.14s of remaining time.\n",
      "\t-0.1574\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.0s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist_BAG_L1 ... Training model for up to 3597.75s of the 5398.02s of remaining time.\n",
      "\t-0.1574\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.0s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist_BAG_L1 ... Training model for up to 3597.75s of the 5398.02s of remaining time.\n",
      "\t-0.1395\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.0s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 3597.62s of the 5397.89s of remaining time.\n",
      "\t-0.1395\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.0s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 3597.62s of the 5397.89s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.00%)\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.00%)\n",
      "\t-0.124\t = Validation score   (-root_mean_squared_error)\n",
      "\t722.33s\t = Training   runtime\n",
      "\t0.22s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L1 ... Training model for up to 2873.41s of the 4673.69s of remaining time.\n",
      "\t-0.124\t = Validation score   (-root_mean_squared_error)\n",
      "\t722.33s\t = Training   runtime\n",
      "\t0.22s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L1 ... Training model for up to 2873.41s of the 4673.69s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.00%)\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.00%)\n",
      "\t-0.1217\t = Validation score   (-root_mean_squared_error)\n",
      "\t418.75s\t = Training   runtime\n",
      "\t0.19s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE_BAG_L1 ... Training model for up to 2452.81s of the 4253.08s of remaining time.\n",
      "\t-0.1217\t = Validation score   (-root_mean_squared_error)\n",
      "\t418.75s\t = Training   runtime\n",
      "\t0.19s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE_BAG_L1 ... Training model for up to 2452.81s of the 4253.08s of remaining time.\n",
      "\t-0.1218\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.34s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L1 ... Training model for up to 2452.32s of the 4252.59s of remaining time.\n",
      "\t-0.1218\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.34s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L1 ... Training model for up to 2452.32s of the 4252.59s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.01%)\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.01%)\n",
      "\t-0.1208\t = Validation score   (-root_mean_squared_error)\n",
      "\t40.16s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE_BAG_L1 ... Training model for up to 2410.43s of the 4210.70s of remaining time.\n",
      "\t-0.1208\t = Validation score   (-root_mean_squared_error)\n",
      "\t40.16s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE_BAG_L1 ... Training model for up to 2410.43s of the 4210.70s of remaining time.\n",
      "\t-0.1263\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.3s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_BAG_L1 ... Training model for up to 2409.98s of the 4210.26s of remaining time.\n",
      "\t-0.1263\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.3s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_BAG_L1 ... Training model for up to 2409.98s of the 4210.26s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.00%)\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.00%)\n",
      "\t-0.1364\t = Validation score   (-root_mean_squared_error)\n",
      "\t170.1s\t = Training   runtime\n",
      "\t1.06s\t = Validation runtime\n",
      "Fitting model: XGBoost_BAG_L1 ... Training model for up to 2238.02s of the 4038.29s of remaining time.\n",
      "\t-0.1364\t = Validation score   (-root_mean_squared_error)\n",
      "\t170.1s\t = Training   runtime\n",
      "\t1.06s\t = Validation runtime\n",
      "Fitting model: XGBoost_BAG_L1 ... Training model for up to 2238.02s of the 4038.29s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.01%)\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.01%)\n",
      "\t-0.1219\t = Validation score   (-root_mean_squared_error)\n",
      "\t421.9s\t = Training   runtime\n",
      "\t0.25s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_BAG_L1 ... Training model for up to 1814.24s of the 3614.51s of remaining time.\n",
      "\t-0.1219\t = Validation score   (-root_mean_squared_error)\n",
      "\t421.9s\t = Training   runtime\n",
      "\t0.25s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_BAG_L1 ... Training model for up to 1814.24s of the 3614.51s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.00%)\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.00%)\n",
      "\tWarning: Exception caused NeuralNetTorch_BAG_L1 to fail during training... Skipping this model.\n",
      "\t\t\u001b[36mray::_ray_fit()\u001b[39m (pid=617224, ip=172.17.0.5)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 413, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 201, in _fit\n",
      "    train_dataset = self._generate_dataset(X, y, train_params=processor_kwargs, is_train=True)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 669, in _generate_dataset\n",
      "    dataset = self._process_train_data(\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 734, in _process_train_data\n",
      "    self.processor = create_preprocessor(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/utils/data_preprocessor.py\", line 40, in create_preprocessor\n",
      "    return ColumnTransformer(\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "TypeError: ColumnTransformer.__init__() got an unexpected keyword argument 'force_int_remainder_cols'\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 2753, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 904, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(TypeError): \u001b[36mray::_ray_fit()\u001b[39m (pid=617224, ip=172.17.0.5)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 413, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 201, in _fit\n",
      "    train_dataset = self._generate_dataset(X, y, train_params=processor_kwargs, is_train=True)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 669, in _generate_dataset\n",
      "    dataset = self._process_train_data(\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 734, in _process_train_data\n",
      "    self.processor = create_preprocessor(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/utils/data_preprocessor.py\", line 40, in create_preprocessor\n",
      "    return ColumnTransformer(\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "TypeError: ColumnTransformer.__init__() got an unexpected keyword argument 'force_int_remainder_cols'\n",
      "\tWarning: Exception caused NeuralNetTorch_BAG_L1 to fail during training... Skipping this model.\n",
      "\t\t\u001b[36mray::_ray_fit()\u001b[39m (pid=617224, ip=172.17.0.5)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 413, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 201, in _fit\n",
      "    train_dataset = self._generate_dataset(X, y, train_params=processor_kwargs, is_train=True)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 669, in _generate_dataset\n",
      "    dataset = self._process_train_data(\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 734, in _process_train_data\n",
      "    self.processor = create_preprocessor(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/utils/data_preprocessor.py\", line 40, in create_preprocessor\n",
      "    return ColumnTransformer(\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "TypeError: ColumnTransformer.__init__() got an unexpected keyword argument 'force_int_remainder_cols'\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 2753, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 904, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(TypeError): \u001b[36mray::_ray_fit()\u001b[39m (pid=617224, ip=172.17.0.5)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 413, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 201, in _fit\n",
      "    train_dataset = self._generate_dataset(X, y, train_params=processor_kwargs, is_train=True)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 669, in _generate_dataset\n",
      "    dataset = self._process_train_data(\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 734, in _process_train_data\n",
      "    self.processor = create_preprocessor(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/utils/data_preprocessor.py\", line 40, in create_preprocessor\n",
      "    return ColumnTransformer(\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "TypeError: ColumnTransformer.__init__() got an unexpected keyword argument 'force_int_remainder_cols'\n",
      "Fitting model: LightGBMLarge_BAG_L1 ... Training model for up to 1810.14s of the 3610.41s of remaining time.\n",
      "Fitting model: LightGBMLarge_BAG_L1 ... Training model for up to 1810.14s of the 3610.41s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.01%)\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.01%)\n",
      "2025-09-30 14:02:46,977\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-09-30 14:02:46,982\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-09-30 14:02:46,984\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-09-30 14:02:46,985\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-09-30 14:02:46,985\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-09-30 14:02:46,986\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-09-30 14:02:46,986\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-09-30 14:02:46,977\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-09-30 14:02:46,982\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-09-30 14:02:46,984\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-09-30 14:02:46,985\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-09-30 14:02:46,985\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-09-30 14:02:46,986\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-09-30 14:02:46,986\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "\t-0.1296\t = Validation score   (-root_mean_squared_error)\n",
      "\t1450.1s\t = Training   runtime\n",
      "\t0.21s\t = Validation runtime\n",
      "Fitting model: CatBoost_r177_BAG_L1 ... Training model for up to 358.27s of the 2158.54s of remaining time.\n",
      "\t-0.1296\t = Validation score   (-root_mean_squared_error)\n",
      "\t1450.1s\t = Training   runtime\n",
      "\t0.21s\t = Validation runtime\n",
      "Fitting model: CatBoost_r177_BAG_L1 ... Training model for up to 358.27s of the 2158.54s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.01%)\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.01%)\n",
      "\t-0.121\t = Validation score   (-root_mean_squared_error)\n",
      "\t40.41s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_r79_BAG_L1 ... Training model for up to 315.49s of the 2115.76s of remaining time.\n",
      "\t-0.121\t = Validation score   (-root_mean_squared_error)\n",
      "\t40.41s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_r79_BAG_L1 ... Training model for up to 315.49s of the 2115.76s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.00%)\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.00%)\n",
      "\tWarning: Exception caused NeuralNetTorch_r79_BAG_L1 to fail during training... Skipping this model.\n",
      "\t\t\u001b[36mray::_ray_fit()\u001b[39m (pid=630316, ip=172.17.0.5)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 413, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 201, in _fit\n",
      "    train_dataset = self._generate_dataset(X, y, train_params=processor_kwargs, is_train=True)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 669, in _generate_dataset\n",
      "    dataset = self._process_train_data(\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 734, in _process_train_data\n",
      "    self.processor = create_preprocessor(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/utils/data_preprocessor.py\", line 40, in create_preprocessor\n",
      "    return ColumnTransformer(\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "TypeError: ColumnTransformer.__init__() got an unexpected keyword argument 'force_int_remainder_cols'\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 2753, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 904, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(TypeError): \u001b[36mray::_ray_fit()\u001b[39m (pid=630316, ip=172.17.0.5)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 413, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 201, in _fit\n",
      "    train_dataset = self._generate_dataset(X, y, train_params=processor_kwargs, is_train=True)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 669, in _generate_dataset\n",
      "    dataset = self._process_train_data(\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 734, in _process_train_data\n",
      "    self.processor = create_preprocessor(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/utils/data_preprocessor.py\", line 40, in create_preprocessor\n",
      "    return ColumnTransformer(\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "TypeError: ColumnTransformer.__init__() got an unexpected keyword argument 'force_int_remainder_cols'\n",
      "\tWarning: Exception caused NeuralNetTorch_r79_BAG_L1 to fail during training... Skipping this model.\n",
      "\t\t\u001b[36mray::_ray_fit()\u001b[39m (pid=630316, ip=172.17.0.5)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 413, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 201, in _fit\n",
      "    train_dataset = self._generate_dataset(X, y, train_params=processor_kwargs, is_train=True)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 669, in _generate_dataset\n",
      "    dataset = self._process_train_data(\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 734, in _process_train_data\n",
      "    self.processor = create_preprocessor(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/utils/data_preprocessor.py\", line 40, in create_preprocessor\n",
      "    return ColumnTransformer(\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "TypeError: ColumnTransformer.__init__() got an unexpected keyword argument 'force_int_remainder_cols'\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 2753, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 904, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(TypeError): \u001b[36mray::_ray_fit()\u001b[39m (pid=630316, ip=172.17.0.5)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 413, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 201, in _fit\n",
      "    train_dataset = self._generate_dataset(X, y, train_params=processor_kwargs, is_train=True)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 669, in _generate_dataset\n",
      "    dataset = self._process_train_data(\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 734, in _process_train_data\n",
      "    self.processor = create_preprocessor(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/utils/data_preprocessor.py\", line 40, in create_preprocessor\n",
      "    return ColumnTransformer(\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "TypeError: ColumnTransformer.__init__() got an unexpected keyword argument 'force_int_remainder_cols'\n",
      "Fitting model: LightGBM_r131_BAG_L1 ... Training model for up to 311.70s of the 2111.97s of remaining time.\n",
      "Fitting model: LightGBM_r131_BAG_L1 ... Training model for up to 311.70s of the 2111.97s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.01%)\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.01%)\n",
      "2025-09-30 14:27:45,361\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-09-30 14:27:45,368\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-09-30 14:27:45,368\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-09-30 14:27:45,369\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-09-30 14:27:45,369\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-09-30 14:27:45,369\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-09-30 14:27:45,370\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-09-30 14:27:45,361\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-09-30 14:27:45,368\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-09-30 14:27:45,368\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-09-30 14:27:45,369\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-09-30 14:27:45,369\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-09-30 14:27:45,369\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-09-30 14:27:45,370\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "\tWarning: Exception caused LightGBM_r131_BAG_L1 to fail during training... Skipping this model.\n",
      "\t\tTask was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: 3c326f52a097e2fed347bd22e6e7d97105021a7601000000, name=_ray_fit, pid=631949, memory used=0.22GB) was running was 957.17GB / 1007.45GB (0.95009), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0632cbf7004b714071ef382f424e2f04a8085cb7318df1c8689897f6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-0632cbf7004b714071ef382f424e2f04a8085cb7318df1c8689897f6*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.78\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "521385\t0.75\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "631949\t0.22\tray::_ray_fit\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 2753, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: 3c326f52a097e2fed347bd22e6e7d97105021a7601000000, name=_ray_fit, pid=631949, memory used=0.22GB) was running was 957.17GB / 1007.45GB (0.95009), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0632cbf7004b714071ef382f424e2f04a8085cb7318df1c8689897f6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-0632cbf7004b714071ef382f424e2f04a8085cb7318df1c8689897f6*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.78\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "521385\t0.75\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "631949\t0.22\tray::_ray_fit\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\tWarning: Exception caused LightGBM_r131_BAG_L1 to fail during training... Skipping this model.\n",
      "\t\tTask was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: 3c326f52a097e2fed347bd22e6e7d97105021a7601000000, name=_ray_fit, pid=631949, memory used=0.22GB) was running was 957.17GB / 1007.45GB (0.95009), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0632cbf7004b714071ef382f424e2f04a8085cb7318df1c8689897f6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-0632cbf7004b714071ef382f424e2f04a8085cb7318df1c8689897f6*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.78\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "521385\t0.75\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "631949\t0.22\tray::_ray_fit\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 2753, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: 3c326f52a097e2fed347bd22e6e7d97105021a7601000000, name=_ray_fit, pid=631949, memory used=0.22GB) was running was 957.17GB / 1007.45GB (0.95009), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0632cbf7004b714071ef382f424e2f04a8085cb7318df1c8689897f6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-0632cbf7004b714071ef382f424e2f04a8085cb7318df1c8689897f6*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.78\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "521385\t0.75\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "631949\t0.22\tray::_ray_fit\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Fitting model: NeuralNetFastAI_r191_BAG_L1 ... Training model for up to 174.09s of the 1974.36s of remaining time.\n",
      "Fitting model: NeuralNetFastAI_r191_BAG_L1 ... Training model for up to 174.09s of the 1974.36s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.00%)\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.00%)\n",
      "\tWarning: Exception caused NeuralNetFastAI_r191_BAG_L1 to fail during training... Skipping this model.\n",
      "\t\tTask was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: fc78cd055944082af4a530d2b7478ce7f505223801000000, name=_ray_fit, pid=635394, memory used=0.11GB) was running was 957.22GB / 1007.45GB (0.95014), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 72a18728669a8bfeddea697f6c91f465423b08cb2efdb0a9671f267e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-72a18728669a8bfeddea697f6c91f465423b08cb2efdb0a9671f267e*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.78\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "521385\t0.76\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "635394\t0.11\tray::IDLE\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 2753, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: fc78cd055944082af4a530d2b7478ce7f505223801000000, name=_ray_fit, pid=635394, memory used=0.11GB) was running was 957.22GB / 1007.45GB (0.95014), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 72a18728669a8bfeddea697f6c91f465423b08cb2efdb0a9671f267e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-72a18728669a8bfeddea697f6c91f465423b08cb2efdb0a9671f267e*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.78\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "521385\t0.76\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "635394\t0.11\tray::IDLE\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\tWarning: Exception caused NeuralNetFastAI_r191_BAG_L1 to fail during training... Skipping this model.\n",
      "\t\tTask was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: fc78cd055944082af4a530d2b7478ce7f505223801000000, name=_ray_fit, pid=635394, memory used=0.11GB) was running was 957.22GB / 1007.45GB (0.95014), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 72a18728669a8bfeddea697f6c91f465423b08cb2efdb0a9671f267e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-72a18728669a8bfeddea697f6c91f465423b08cb2efdb0a9671f267e*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.78\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "521385\t0.76\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "635394\t0.11\tray::IDLE\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 2753, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: fc78cd055944082af4a530d2b7478ce7f505223801000000, name=_ray_fit, pid=635394, memory used=0.11GB) was running was 957.22GB / 1007.45GB (0.95014), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 72a18728669a8bfeddea697f6c91f465423b08cb2efdb0a9671f267e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-72a18728669a8bfeddea697f6c91f465423b08cb2efdb0a9671f267e*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.78\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "521385\t0.76\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "635394\t0.11\tray::IDLE\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Fitting model: CatBoost_r9_BAG_L1 ... Training model for up to 173.34s of the 1973.61s of remaining time.\n",
      "Fitting model: CatBoost_r9_BAG_L1 ... Training model for up to 173.34s of the 1973.61s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.06%)\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.06%)\n",
      "\tWarning: Exception caused CatBoost_r9_BAG_L1 to fail during training... Skipping this model.\n",
      "\t\tTask was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: c3207b385ed834e25b5e576c0bb15c86fc4329b301000000, name=_ray_fit, pid=635625, memory used=0.13GB) was running was 957.26GB / 1007.45GB (0.950181), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 03502a4337ef04cd7c061862e74e1a264917e92746945c8d363b6090) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-03502a4337ef04cd7c061862e74e1a264917e92746945c8d363b6090*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.78\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "521385\t0.76\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "635625\t0.13\tray::IDLE\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 2753, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: c3207b385ed834e25b5e576c0bb15c86fc4329b301000000, name=_ray_fit, pid=635625, memory used=0.13GB) was running was 957.26GB / 1007.45GB (0.950181), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 03502a4337ef04cd7c061862e74e1a264917e92746945c8d363b6090) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-03502a4337ef04cd7c061862e74e1a264917e92746945c8d363b6090*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.78\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "521385\t0.76\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "635625\t0.13\tray::IDLE\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\tWarning: Exception caused CatBoost_r9_BAG_L1 to fail during training... Skipping this model.\n",
      "\t\tTask was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: c3207b385ed834e25b5e576c0bb15c86fc4329b301000000, name=_ray_fit, pid=635625, memory used=0.13GB) was running was 957.26GB / 1007.45GB (0.950181), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 03502a4337ef04cd7c061862e74e1a264917e92746945c8d363b6090) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-03502a4337ef04cd7c061862e74e1a264917e92746945c8d363b6090*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.78\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "521385\t0.76\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "635625\t0.13\tray::IDLE\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 2753, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: c3207b385ed834e25b5e576c0bb15c86fc4329b301000000, name=_ray_fit, pid=635625, memory used=0.13GB) was running was 957.26GB / 1007.45GB (0.950181), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 03502a4337ef04cd7c061862e74e1a264917e92746945c8d363b6090) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-03502a4337ef04cd7c061862e74e1a264917e92746945c8d363b6090*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.78\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "521385\t0.76\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "635625\t0.13\tray::IDLE\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "2025-09-30 14:29:59,399\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-09-30 14:29:59,399\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-09-30 14:29:59,400\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-09-30 14:29:59,401\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-09-30 14:29:59,401\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-09-30 14:29:59,402\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-09-30 14:29:59,403\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "Fitting model: LightGBM_r96_BAG_L1 ... Training model for up to 172.60s of the 1972.87s of remaining time.\n",
      "2025-09-30 14:29:59,399\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-09-30 14:29:59,399\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-09-30 14:29:59,400\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-09-30 14:29:59,401\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-09-30 14:29:59,401\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-09-30 14:29:59,402\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-09-30 14:29:59,403\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "Fitting model: LightGBM_r96_BAG_L1 ... Training model for up to 172.60s of the 1972.87s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.00%)\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.00%)\n",
      "\tWarning: Exception caused LightGBM_r96_BAG_L1 to fail during training... Skipping this model.\n",
      "\t\tTask was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: e79f1d116b746c910a6d9b8acca8b0615f220d2b01000000, name=_ray_fit, pid=636560, memory used=0.13GB) was running was 957.13GB / 1007.45GB (0.950055), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 79c9ab972f7f54901cc5c9b390d5806a6a39968c5dfc19b7b87d8d94) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-79c9ab972f7f54901cc5c9b390d5806a6a39968c5dfc19b7b87d8d94*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.78\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "521385\t0.76\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "636560\t0.13\tray::IDLE\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 2753, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: e79f1d116b746c910a6d9b8acca8b0615f220d2b01000000, name=_ray_fit, pid=636560, memory used=0.13GB) was running was 957.13GB / 1007.45GB (0.950055), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 79c9ab972f7f54901cc5c9b390d5806a6a39968c5dfc19b7b87d8d94) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-79c9ab972f7f54901cc5c9b390d5806a6a39968c5dfc19b7b87d8d94*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.78\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "521385\t0.76\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "636560\t0.13\tray::IDLE\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\tWarning: Exception caused LightGBM_r96_BAG_L1 to fail during training... Skipping this model.\n",
      "\t\tTask was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: e79f1d116b746c910a6d9b8acca8b0615f220d2b01000000, name=_ray_fit, pid=636560, memory used=0.13GB) was running was 957.13GB / 1007.45GB (0.950055), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 79c9ab972f7f54901cc5c9b390d5806a6a39968c5dfc19b7b87d8d94) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-79c9ab972f7f54901cc5c9b390d5806a6a39968c5dfc19b7b87d8d94*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.78\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "521385\t0.76\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "636560\t0.13\tray::IDLE\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 2753, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: e79f1d116b746c910a6d9b8acca8b0615f220d2b01000000, name=_ray_fit, pid=636560, memory used=0.13GB) was running was 957.13GB / 1007.45GB (0.950055), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 79c9ab972f7f54901cc5c9b390d5806a6a39968c5dfc19b7b87d8d94) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-79c9ab972f7f54901cc5c9b390d5806a6a39968c5dfc19b7b87d8d94*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.78\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "521385\t0.76\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "636560\t0.13\tray::IDLE\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "2025-09-30 14:30:00,147\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-09-30 14:30:00,148\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-09-30 14:30:00,149\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-09-30 14:30:00,150\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-09-30 14:30:00,151\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "Fitting model: NeuralNetTorch_r22_BAG_L1 ... Training model for up to 171.85s of the 1972.12s of remaining time.\n",
      "2025-09-30 14:30:00,152\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-09-30 14:30:00,155\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-09-30 14:30:00,147\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-09-30 14:30:00,148\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-09-30 14:30:00,149\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-09-30 14:30:00,150\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-09-30 14:30:00,151\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "Fitting model: NeuralNetTorch_r22_BAG_L1 ... Training model for up to 171.85s of the 1972.12s of remaining time.\n",
      "2025-09-30 14:30:00,152\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-09-30 14:30:00,155\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.00%)\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.00%)\n",
      "\tWarning: Exception caused NeuralNetTorch_r22_BAG_L1 to fail during training... Skipping this model.\n",
      "\t\tTask was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: 16e377ad7dfc4b1b15f71d5c1c078487328305e001000000, name=_ray_fit, pid=637574, memory used=0.13GB) was running was 957.20GB / 1007.45GB (0.950124), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 9334a81399921e540fc3feec15ebe42ac89fe01595dda69c5d0a0c8b) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-9334a81399921e540fc3feec15ebe42ac89fe01595dda69c5d0a0c8b*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.78\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "521385\t0.76\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "637574\t0.13\tray::IDLE\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 2753, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: 16e377ad7dfc4b1b15f71d5c1c078487328305e001000000, name=_ray_fit, pid=637574, memory used=0.13GB) was running was 957.20GB / 1007.45GB (0.950124), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 9334a81399921e540fc3feec15ebe42ac89fe01595dda69c5d0a0c8b) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-9334a81399921e540fc3feec15ebe42ac89fe01595dda69c5d0a0c8b*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.78\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "521385\t0.76\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "637574\t0.13\tray::IDLE\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\tWarning: Exception caused NeuralNetTorch_r22_BAG_L1 to fail during training... Skipping this model.\n",
      "\t\tTask was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: 16e377ad7dfc4b1b15f71d5c1c078487328305e001000000, name=_ray_fit, pid=637574, memory used=0.13GB) was running was 957.20GB / 1007.45GB (0.950124), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 9334a81399921e540fc3feec15ebe42ac89fe01595dda69c5d0a0c8b) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-9334a81399921e540fc3feec15ebe42ac89fe01595dda69c5d0a0c8b*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.78\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "521385\t0.76\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "637574\t0.13\tray::IDLE\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 2753, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: 16e377ad7dfc4b1b15f71d5c1c078487328305e001000000, name=_ray_fit, pid=637574, memory used=0.13GB) was running was 957.20GB / 1007.45GB (0.950124), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 9334a81399921e540fc3feec15ebe42ac89fe01595dda69c5d0a0c8b) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-9334a81399921e540fc3feec15ebe42ac89fe01595dda69c5d0a0c8b*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.78\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "521385\t0.76\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "637574\t0.13\tray::IDLE\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "2025-09-30 14:30:00,900\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-09-30 14:30:00,901\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-09-30 14:30:00,902\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-09-30 14:30:00,903\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-09-30 14:30:00,903\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-09-30 14:30:00,904\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "Fitting model: XGBoost_r33_BAG_L1 ... Training model for up to 171.10s of the 1971.37s of remaining time.\n",
      "2025-09-30 14:30:00,905\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-09-30 14:30:00,900\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-09-30 14:30:00,901\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-09-30 14:30:00,902\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-09-30 14:30:00,903\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-09-30 14:30:00,903\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-09-30 14:30:00,904\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "Fitting model: XGBoost_r33_BAG_L1 ... Training model for up to 171.10s of the 1971.37s of remaining time.\n",
      "2025-09-30 14:30:00,905\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.12%)\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.12%)\n",
      "2025-09-30 14:30:06,389\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-09-30 14:30:06,389\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "\tWarning: Exception caused XGBoost_r33_BAG_L1 to fail during training... Skipping this model.\n",
      "\t\tTask was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: 6e1b3b324240f9f95c52dc5932c1bbdc8fe5eecf01000000, name=_ray_fit, pid=638399, memory used=0.20GB) was running was 957.49GB / 1007.45GB (0.950412), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 072db8e8ca3d7e5706e39a4195d5d90e8c2fddc5fe0508a4ec1031a2) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-072db8e8ca3d7e5706e39a4195d5d90e8c2fddc5fe0508a4ec1031a2*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.78\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "521385\t0.76\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "638399\t0.20\tray::_ray_fit\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 2753, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: 6e1b3b324240f9f95c52dc5932c1bbdc8fe5eecf01000000, name=_ray_fit, pid=638399, memory used=0.20GB) was running was 957.49GB / 1007.45GB (0.950412), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 072db8e8ca3d7e5706e39a4195d5d90e8c2fddc5fe0508a4ec1031a2) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-072db8e8ca3d7e5706e39a4195d5d90e8c2fddc5fe0508a4ec1031a2*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.78\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "521385\t0.76\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "638399\t0.20\tray::_ray_fit\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\tWarning: Exception caused XGBoost_r33_BAG_L1 to fail during training... Skipping this model.\n",
      "\t\tTask was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: 6e1b3b324240f9f95c52dc5932c1bbdc8fe5eecf01000000, name=_ray_fit, pid=638399, memory used=0.20GB) was running was 957.49GB / 1007.45GB (0.950412), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 072db8e8ca3d7e5706e39a4195d5d90e8c2fddc5fe0508a4ec1031a2) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-072db8e8ca3d7e5706e39a4195d5d90e8c2fddc5fe0508a4ec1031a2*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.78\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "521385\t0.76\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "638399\t0.20\tray::_ray_fit\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 2753, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: 6e1b3b324240f9f95c52dc5932c1bbdc8fe5eecf01000000, name=_ray_fit, pid=638399, memory used=0.20GB) was running was 957.49GB / 1007.45GB (0.950412), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 072db8e8ca3d7e5706e39a4195d5d90e8c2fddc5fe0508a4ec1031a2) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-072db8e8ca3d7e5706e39a4195d5d90e8c2fddc5fe0508a4ec1031a2*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.78\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "521385\t0.76\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "638399\t0.20\tray::_ray_fit\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Fitting model: ExtraTrees_r42_BAG_L1 ... Training model for up to 163.76s of the 1964.04s of remaining time.\n",
      "Fitting model: ExtraTrees_r42_BAG_L1 ... Training model for up to 163.76s of the 1964.04s of remaining time.\n",
      "\t-0.125\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.37s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: CatBoost_r137_BAG_L1 ... Training model for up to 163.30s of the 1963.57s of remaining time.\n",
      "\t-0.125\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.37s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: CatBoost_r137_BAG_L1 ... Training model for up to 163.30s of the 1963.57s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.00%)\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.00%)\n",
      "2025-09-30 14:30:13,391\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-09-30 14:30:13,392\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-09-30 14:30:13,392\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-09-30 14:30:13,391\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-09-30 14:30:13,392\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-09-30 14:30:13,392\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "\tWarning: Exception caused CatBoost_r137_BAG_L1 to fail during training... Skipping this model.\n",
      "\t\tTask was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: 84785c54b7f7993cfd8d64e91ec03c15c663283401000000, name=_ray_fit, pid=640889, memory used=0.25GB) was running was 957.56GB / 1007.45GB (0.950475), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c735af1d056d7b7260d735366fb0c8e0de4588fe75422f7a33f2010d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-c735af1d056d7b7260d735366fb0c8e0de4588fe75422f7a33f2010d*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.78\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "521385\t0.77\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "640889\t0.25\tray::_ray_fit\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 2753, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: 84785c54b7f7993cfd8d64e91ec03c15c663283401000000, name=_ray_fit, pid=640889, memory used=0.25GB) was running was 957.56GB / 1007.45GB (0.950475), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c735af1d056d7b7260d735366fb0c8e0de4588fe75422f7a33f2010d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-c735af1d056d7b7260d735366fb0c8e0de4588fe75422f7a33f2010d*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.78\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "521385\t0.77\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "640889\t0.25\tray::_ray_fit\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\tWarning: Exception caused CatBoost_r137_BAG_L1 to fail during training... Skipping this model.\n",
      "\t\tTask was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: 84785c54b7f7993cfd8d64e91ec03c15c663283401000000, name=_ray_fit, pid=640889, memory used=0.25GB) was running was 957.56GB / 1007.45GB (0.950475), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c735af1d056d7b7260d735366fb0c8e0de4588fe75422f7a33f2010d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-c735af1d056d7b7260d735366fb0c8e0de4588fe75422f7a33f2010d*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.78\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "521385\t0.77\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "640889\t0.25\tray::_ray_fit\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 2753, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: 84785c54b7f7993cfd8d64e91ec03c15c663283401000000, name=_ray_fit, pid=640889, memory used=0.25GB) was running was 957.56GB / 1007.45GB (0.950475), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c735af1d056d7b7260d735366fb0c8e0de4588fe75422f7a33f2010d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-c735af1d056d7b7260d735366fb0c8e0de4588fe75422f7a33f2010d*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.78\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "521385\t0.77\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "640889\t0.25\tray::_ray_fit\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Fitting model: NeuralNetFastAI_r102_BAG_L1 ... Training model for up to 157.44s of the 1957.71s of remaining time.\n",
      "Fitting model: NeuralNetFastAI_r102_BAG_L1 ... Training model for up to 157.44s of the 1957.71s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.00%)\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.00%)\n",
      "\tWarning: Exception caused NeuralNetFastAI_r102_BAG_L1 to fail during training... Skipping this model.\n",
      "\t\tTask was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: 6ec53ae3ed4acb913c4bbc7870d34a5c31aaa39101000000, name=_ray_fit, pid=644498, memory used=0.27GB) was running was 957.70GB / 1007.45GB (0.950618), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: f987925ca3622833849915bd34a333731cc7842bc2d89d3f2d0edbf6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-f987925ca3622833849915bd34a333731cc7842bc2d89d3f2d0edbf6*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.78\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "521385\t0.67\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "644498\t0.27\tray::_ray_fit\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 2753, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: 6ec53ae3ed4acb913c4bbc7870d34a5c31aaa39101000000, name=_ray_fit, pid=644498, memory used=0.27GB) was running was 957.70GB / 1007.45GB (0.950618), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: f987925ca3622833849915bd34a333731cc7842bc2d89d3f2d0edbf6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-f987925ca3622833849915bd34a333731cc7842bc2d89d3f2d0edbf6*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.78\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "521385\t0.67\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "644498\t0.27\tray::_ray_fit\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\tWarning: Exception caused NeuralNetFastAI_r102_BAG_L1 to fail during training... Skipping this model.\n",
      "\t\tTask was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: 6ec53ae3ed4acb913c4bbc7870d34a5c31aaa39101000000, name=_ray_fit, pid=644498, memory used=0.27GB) was running was 957.70GB / 1007.45GB (0.950618), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: f987925ca3622833849915bd34a333731cc7842bc2d89d3f2d0edbf6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-f987925ca3622833849915bd34a333731cc7842bc2d89d3f2d0edbf6*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.78\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "521385\t0.67\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "644498\t0.27\tray::_ray_fit\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 2753, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: 6ec53ae3ed4acb913c4bbc7870d34a5c31aaa39101000000, name=_ray_fit, pid=644498, memory used=0.27GB) was running was 957.70GB / 1007.45GB (0.950618), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: f987925ca3622833849915bd34a333731cc7842bc2d89d3f2d0edbf6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-f987925ca3622833849915bd34a333731cc7842bc2d89d3f2d0edbf6*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.78\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "521385\t0.67\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "644498\t0.27\tray::_ray_fit\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Fitting model: CatBoost_r13_BAG_L1 ... Training model for up to 149.34s of the 1949.61s of remaining time.\n",
      "Fitting model: CatBoost_r13_BAG_L1 ... Training model for up to 149.34s of the 1949.61s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.06%)\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.06%)\n",
      "\tWarning: Exception caused CatBoost_r13_BAG_L1 to fail during training... Skipping this model.\n",
      "\t\tTask was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: df441696b6747c4f4a18827925ecd936856284ad01000000, name=_ray_fit, pid=644837, memory used=0.26GB) was running was 958.08GB / 1007.45GB (0.950993), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4267ce07c8cfe580b51031ce0d53ca6376497401619e642cbf9343ec) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-4267ce07c8cfe580b51031ce0d53ca6376497401619e642cbf9343ec*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.78\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "521385\t0.77\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "644837\t0.26\tray::_ray_fit\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 2753, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: df441696b6747c4f4a18827925ecd936856284ad01000000, name=_ray_fit, pid=644837, memory used=0.26GB) was running was 958.08GB / 1007.45GB (0.950993), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4267ce07c8cfe580b51031ce0d53ca6376497401619e642cbf9343ec) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-4267ce07c8cfe580b51031ce0d53ca6376497401619e642cbf9343ec*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.78\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "521385\t0.77\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "644837\t0.26\tray::_ray_fit\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\tWarning: Exception caused CatBoost_r13_BAG_L1 to fail during training... Skipping this model.\n",
      "\t\tTask was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: df441696b6747c4f4a18827925ecd936856284ad01000000, name=_ray_fit, pid=644837, memory used=0.26GB) was running was 958.08GB / 1007.45GB (0.950993), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4267ce07c8cfe580b51031ce0d53ca6376497401619e642cbf9343ec) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-4267ce07c8cfe580b51031ce0d53ca6376497401619e642cbf9343ec*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.78\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "521385\t0.77\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "644837\t0.26\tray::_ray_fit\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 2753, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: df441696b6747c4f4a18827925ecd936856284ad01000000, name=_ray_fit, pid=644837, memory used=0.26GB) was running was 958.08GB / 1007.45GB (0.950993), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4267ce07c8cfe580b51031ce0d53ca6376497401619e642cbf9343ec) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-4267ce07c8cfe580b51031ce0d53ca6376497401619e642cbf9343ec*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.78\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "521385\t0.77\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "644837\t0.26\tray::_ray_fit\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Fitting model: RandomForest_r195_BAG_L1 ... Training model for up to 142.39s of the 1942.66s of remaining time.\n",
      "Fitting model: RandomForest_r195_BAG_L1 ... Training model for up to 142.39s of the 1942.66s of remaining time.\n",
      "\t-0.1224\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.52s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: LightGBM_r188_BAG_L1 ... Training model for up to 141.76s of the 1942.03s of remaining time.\n",
      "\t-0.1224\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.52s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: LightGBM_r188_BAG_L1 ... Training model for up to 141.76s of the 1942.03s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.03%)\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.03%)\n",
      "\tWarning: Exception caused LightGBM_r188_BAG_L1 to fail during training... Skipping this model.\n",
      "\t\tTask was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: 02ed3103c5dbd99729577c021d4b621d6c82ce0101000000, name=_ray_fit, pid=646661, memory used=0.23GB) was running was 958.60GB / 1007.45GB (0.951514), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: aad08081aacb083a8b48a468a5bd2ccd5dedcc7c248c97b139ded7ea) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-aad08081aacb083a8b48a468a5bd2ccd5dedcc7c248c97b139ded7ea*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.00\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.78\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "646661\t0.23\tray::IDLE\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 2753, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: 02ed3103c5dbd99729577c021d4b621d6c82ce0101000000, name=_ray_fit, pid=646661, memory used=0.23GB) was running was 958.60GB / 1007.45GB (0.951514), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: aad08081aacb083a8b48a468a5bd2ccd5dedcc7c248c97b139ded7ea) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-aad08081aacb083a8b48a468a5bd2ccd5dedcc7c248c97b139ded7ea*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.00\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.78\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "646661\t0.23\tray::IDLE\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\tWarning: Exception caused LightGBM_r188_BAG_L1 to fail during training... Skipping this model.\n",
      "\t\tTask was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: 02ed3103c5dbd99729577c021d4b621d6c82ce0101000000, name=_ray_fit, pid=646661, memory used=0.23GB) was running was 958.60GB / 1007.45GB (0.951514), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: aad08081aacb083a8b48a468a5bd2ccd5dedcc7c248c97b139ded7ea) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-aad08081aacb083a8b48a468a5bd2ccd5dedcc7c248c97b139ded7ea*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.00\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.78\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "646661\t0.23\tray::IDLE\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 2753, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: 02ed3103c5dbd99729577c021d4b621d6c82ce0101000000, name=_ray_fit, pid=646661, memory used=0.23GB) was running was 958.60GB / 1007.45GB (0.951514), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: aad08081aacb083a8b48a468a5bd2ccd5dedcc7c248c97b139ded7ea) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-aad08081aacb083a8b48a468a5bd2ccd5dedcc7c248c97b139ded7ea*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.00\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.78\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "646661\t0.23\tray::IDLE\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Fitting model: NeuralNetFastAI_r145_BAG_L1 ... Training model for up to 134.97s of the 1935.24s of remaining time.\n",
      "Fitting model: NeuralNetFastAI_r145_BAG_L1 ... Training model for up to 134.97s of the 1935.24s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.00%)\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.00%)\n",
      "2025-09-30 14:30:42,400\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-09-30 14:30:42,404\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-09-30 14:30:42,404\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-09-30 14:30:42,405\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-09-30 14:30:42,400\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-09-30 14:30:42,404\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-09-30 14:30:42,404\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-09-30 14:30:42,405\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "\tWarning: Exception caused NeuralNetFastAI_r145_BAG_L1 to fail during training... Skipping this model.\n",
      "\t\tTask was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: 4999076d742ddda2f5b6caa76cddb65629b98cab01000000, name=_ray_fit, pid=650594, memory used=0.25GB) was running was 958.04GB / 1007.45GB (0.950951), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6387834d64e5e8356b72574d98086b39aa52019e7ae0d6cfc37db1da) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-6387834d64e5e8356b72574d98086b39aa52019e7ae0d6cfc37db1da*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t0.95\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.78\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "650594\t0.25\tray::_ray_fit\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 2753, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: 4999076d742ddda2f5b6caa76cddb65629b98cab01000000, name=_ray_fit, pid=650594, memory used=0.25GB) was running was 958.04GB / 1007.45GB (0.950951), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6387834d64e5e8356b72574d98086b39aa52019e7ae0d6cfc37db1da) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-6387834d64e5e8356b72574d98086b39aa52019e7ae0d6cfc37db1da*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t0.95\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.78\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "650594\t0.25\tray::_ray_fit\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\tWarning: Exception caused NeuralNetFastAI_r145_BAG_L1 to fail during training... Skipping this model.\n",
      "\t\tTask was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: 4999076d742ddda2f5b6caa76cddb65629b98cab01000000, name=_ray_fit, pid=650594, memory used=0.25GB) was running was 958.04GB / 1007.45GB (0.950951), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6387834d64e5e8356b72574d98086b39aa52019e7ae0d6cfc37db1da) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-6387834d64e5e8356b72574d98086b39aa52019e7ae0d6cfc37db1da*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t0.95\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.78\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "650594\t0.25\tray::_ray_fit\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 2753, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: 4999076d742ddda2f5b6caa76cddb65629b98cab01000000, name=_ray_fit, pid=650594, memory used=0.25GB) was running was 958.04GB / 1007.45GB (0.950951), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6387834d64e5e8356b72574d98086b39aa52019e7ae0d6cfc37db1da) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-6387834d64e5e8356b72574d98086b39aa52019e7ae0d6cfc37db1da*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t0.95\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.78\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "650594\t0.25\tray::_ray_fit\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Fitting model: XGBoost_r89_BAG_L1 ... Training model for up to 124.48s of the 1924.75s of remaining time.\n",
      "Fitting model: XGBoost_r89_BAG_L1 ... Training model for up to 124.48s of the 1924.75s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.02%)\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.02%)\n",
      "\tWarning: Exception caused XGBoost_r89_BAG_L1 to fail during training... Skipping this model.\n",
      "\t\tTask was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: a7c5fc7df94bbc0e34db2455deab67455a1178b201000000, name=_ray_fit, pid=652347, memory used=0.12GB) was running was 958.58GB / 1007.45GB (0.951494), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 8a4622d96a9e13eb567f7649bf6a0166a3186a4e82616f6ccca92b12) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-8a4622d96a9e13eb567f7649bf6a0166a3186a4e82616f6ccca92b12*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.01\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.78\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "652347\t0.12\tray::IDLE\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 2753, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: a7c5fc7df94bbc0e34db2455deab67455a1178b201000000, name=_ray_fit, pid=652347, memory used=0.12GB) was running was 958.58GB / 1007.45GB (0.951494), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 8a4622d96a9e13eb567f7649bf6a0166a3186a4e82616f6ccca92b12) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-8a4622d96a9e13eb567f7649bf6a0166a3186a4e82616f6ccca92b12*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.01\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.78\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "652347\t0.12\tray::IDLE\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\tWarning: Exception caused XGBoost_r89_BAG_L1 to fail during training... Skipping this model.\n",
      "\t\tTask was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: a7c5fc7df94bbc0e34db2455deab67455a1178b201000000, name=_ray_fit, pid=652347, memory used=0.12GB) was running was 958.58GB / 1007.45GB (0.951494), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 8a4622d96a9e13eb567f7649bf6a0166a3186a4e82616f6ccca92b12) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-8a4622d96a9e13eb567f7649bf6a0166a3186a4e82616f6ccca92b12*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.01\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.78\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "652347\t0.12\tray::IDLE\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 2753, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: a7c5fc7df94bbc0e34db2455deab67455a1178b201000000, name=_ray_fit, pid=652347, memory used=0.12GB) was running was 958.58GB / 1007.45GB (0.951494), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 8a4622d96a9e13eb567f7649bf6a0166a3186a4e82616f6ccca92b12) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-8a4622d96a9e13eb567f7649bf6a0166a3186a4e82616f6ccca92b12*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.01\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.78\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "652347\t0.12\tray::IDLE\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Fitting model: NeuralNetTorch_r30_BAG_L1 ... Training model for up to 117.47s of the 1917.74s of remaining time.\n",
      "Fitting model: NeuralNetTorch_r30_BAG_L1 ... Training model for up to 117.47s of the 1917.74s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.00%)\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.00%)\n",
      "\tWarning: Exception caused NeuralNetTorch_r30_BAG_L1 to fail during training... Skipping this model.\n",
      "\t\t\u001b[36mray::_ray_fit()\u001b[39m (pid=652652, ip=172.17.0.5)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 413, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 201, in _fit\n",
      "    train_dataset = self._generate_dataset(X, y, train_params=processor_kwargs, is_train=True)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 669, in _generate_dataset\n",
      "    dataset = self._process_train_data(\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 734, in _process_train_data\n",
      "    self.processor = create_preprocessor(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/utils/data_preprocessor.py\", line 40, in create_preprocessor\n",
      "    return ColumnTransformer(\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "TypeError: ColumnTransformer.__init__() got an unexpected keyword argument 'force_int_remainder_cols'\n",
      "\tWarning: Exception caused NeuralNetTorch_r30_BAG_L1 to fail during training... Skipping this model.\n",
      "\t\t\u001b[36mray::_ray_fit()\u001b[39m (pid=652652, ip=172.17.0.5)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 413, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 201, in _fit\n",
      "    train_dataset = self._generate_dataset(X, y, train_params=processor_kwargs, is_train=True)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 669, in _generate_dataset\n",
      "    dataset = self._process_train_data(\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 734, in _process_train_data\n",
      "    self.processor = create_preprocessor(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/utils/data_preprocessor.py\", line 40, in create_preprocessor\n",
      "    return ColumnTransformer(\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "TypeError: ColumnTransformer.__init__() got an unexpected keyword argument 'force_int_remainder_cols'\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 2753, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 904, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(TypeError): \u001b[36mray::_ray_fit()\u001b[39m (pid=652652, ip=172.17.0.5)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 413, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 201, in _fit\n",
      "    train_dataset = self._generate_dataset(X, y, train_params=processor_kwargs, is_train=True)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 669, in _generate_dataset\n",
      "    dataset = self._process_train_data(\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 734, in _process_train_data\n",
      "    self.processor = create_preprocessor(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/utils/data_preprocessor.py\", line 40, in create_preprocessor\n",
      "    return ColumnTransformer(\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "TypeError: ColumnTransformer.__init__() got an unexpected keyword argument 'force_int_remainder_cols'\n",
      "Fitting model: LightGBM_r130_BAG_L1 ... Training model for up to 113.65s of the 1913.92s of remaining time.\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 2753, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 904, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(TypeError): \u001b[36mray::_ray_fit()\u001b[39m (pid=652652, ip=172.17.0.5)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 413, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 201, in _fit\n",
      "    train_dataset = self._generate_dataset(X, y, train_params=processor_kwargs, is_train=True)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 669, in _generate_dataset\n",
      "    dataset = self._process_train_data(\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 734, in _process_train_data\n",
      "    self.processor = create_preprocessor(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/utils/data_preprocessor.py\", line 40, in create_preprocessor\n",
      "    return ColumnTransformer(\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "TypeError: ColumnTransformer.__init__() got an unexpected keyword argument 'force_int_remainder_cols'\n",
      "Fitting model: LightGBM_r130_BAG_L1 ... Training model for up to 113.65s of the 1913.92s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.02%)\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.02%)\n",
      "\tWarning: Exception caused LightGBM_r130_BAG_L1 to fail during training... Skipping this model.\n",
      "\t\tTask was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: dae299081f4675c562e5dc78a9edb785787e4cf601000000, name=_ray_fit, pid=653523, memory used=0.07GB) was running was 959.41GB / 1007.45GB (0.952314), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 28e5dae53cb244f559daaa32f9b4c16f740ab5de37a43909d5fe20c6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-28e5dae53cb244f559daaa32f9b4c16f740ab5de37a43909d5fe20c6*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "521385\t0.87\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "522871\t0.78\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "652652\t0.32\tray::IDLE\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 2753, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: dae299081f4675c562e5dc78a9edb785787e4cf601000000, name=_ray_fit, pid=653523, memory used=0.07GB) was running was 959.41GB / 1007.45GB (0.952314), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 28e5dae53cb244f559daaa32f9b4c16f740ab5de37a43909d5fe20c6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-28e5dae53cb244f559daaa32f9b4c16f740ab5de37a43909d5fe20c6*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "521385\t0.87\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "522871\t0.78\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "652652\t0.32\tray::IDLE\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\tWarning: Exception caused LightGBM_r130_BAG_L1 to fail during training... Skipping this model.\n",
      "\t\tTask was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: dae299081f4675c562e5dc78a9edb785787e4cf601000000, name=_ray_fit, pid=653523, memory used=0.07GB) was running was 959.41GB / 1007.45GB (0.952314), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 28e5dae53cb244f559daaa32f9b4c16f740ab5de37a43909d5fe20c6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-28e5dae53cb244f559daaa32f9b4c16f740ab5de37a43909d5fe20c6*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "521385\t0.87\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "522871\t0.78\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "652652\t0.32\tray::IDLE\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 2753, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: dae299081f4675c562e5dc78a9edb785787e4cf601000000, name=_ray_fit, pid=653523, memory used=0.07GB) was running was 959.41GB / 1007.45GB (0.952314), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 28e5dae53cb244f559daaa32f9b4c16f740ab5de37a43909d5fe20c6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-28e5dae53cb244f559daaa32f9b4c16f740ab5de37a43909d5fe20c6*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "521385\t0.87\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "522871\t0.78\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "652652\t0.32\tray::IDLE\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "2025-09-30 14:30:58,966\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-09-30 14:30:58,967\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-09-30 14:30:58,968\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "Fitting model: NeuralNetTorch_r86_BAG_L1 ... Training model for up to 113.03s of the 1913.31s of remaining time.\n",
      "2025-09-30 14:30:58,966\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-09-30 14:30:58,967\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-09-30 14:30:58,968\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "Fitting model: NeuralNetTorch_r86_BAG_L1 ... Training model for up to 113.03s of the 1913.31s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.00%)\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.00%)\n",
      "\tWarning: Exception caused NeuralNetTorch_r86_BAG_L1 to fail during training... Skipping this model.\n",
      "\t\t\u001b[36mray::_ray_fit()\u001b[39m (pid=654060, ip=172.17.0.5)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 413, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 201, in _fit\n",
      "    train_dataset = self._generate_dataset(X, y, train_params=processor_kwargs, is_train=True)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 669, in _generate_dataset\n",
      "    dataset = self._process_train_data(\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 734, in _process_train_data\n",
      "    self.processor = create_preprocessor(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/utils/data_preprocessor.py\", line 40, in create_preprocessor\n",
      "    return ColumnTransformer(\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "TypeError: ColumnTransformer.__init__() got an unexpected keyword argument 'force_int_remainder_cols'\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 2753, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 904, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(TypeError): \u001b[36mray::_ray_fit()\u001b[39m (pid=654060, ip=172.17.0.5)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 413, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 201, in _fit\n",
      "    train_dataset = self._generate_dataset(X, y, train_params=processor_kwargs, is_train=True)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 669, in _generate_dataset\n",
      "    dataset = self._process_train_data(\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 734, in _process_train_data\n",
      "    self.processor = create_preprocessor(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/utils/data_preprocessor.py\", line 40, in create_preprocessor\n",
      "    return ColumnTransformer(\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "TypeError: ColumnTransformer.__init__() got an unexpected keyword argument 'force_int_remainder_cols'\n",
      "\tWarning: Exception caused NeuralNetTorch_r86_BAG_L1 to fail during training... Skipping this model.\n",
      "\t\t\u001b[36mray::_ray_fit()\u001b[39m (pid=654060, ip=172.17.0.5)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 413, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 201, in _fit\n",
      "    train_dataset = self._generate_dataset(X, y, train_params=processor_kwargs, is_train=True)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 669, in _generate_dataset\n",
      "    dataset = self._process_train_data(\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 734, in _process_train_data\n",
      "    self.processor = create_preprocessor(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/utils/data_preprocessor.py\", line 40, in create_preprocessor\n",
      "    return ColumnTransformer(\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "TypeError: ColumnTransformer.__init__() got an unexpected keyword argument 'force_int_remainder_cols'\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 2753, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 904, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(TypeError): \u001b[36mray::_ray_fit()\u001b[39m (pid=654060, ip=172.17.0.5)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 413, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 201, in _fit\n",
      "    train_dataset = self._generate_dataset(X, y, train_params=processor_kwargs, is_train=True)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 669, in _generate_dataset\n",
      "    dataset = self._process_train_data(\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 734, in _process_train_data\n",
      "    self.processor = create_preprocessor(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/utils/data_preprocessor.py\", line 40, in create_preprocessor\n",
      "    return ColumnTransformer(\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "TypeError: ColumnTransformer.__init__() got an unexpected keyword argument 'force_int_remainder_cols'\n",
      "2025-09-30 14:31:01,933\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-09-30 14:31:01,935\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-09-30 14:31:01,936\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "Fitting model: CatBoost_r50_BAG_L1 ... Training model for up to 110.06s of the 1910.34s of remaining time.\n",
      "2025-09-30 14:31:01,933\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-09-30 14:31:01,935\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-09-30 14:31:01,936\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "Fitting model: CatBoost_r50_BAG_L1 ... Training model for up to 110.06s of the 1910.34s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.00%)\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.00%)\n",
      "2025-09-30 14:31:07,407\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-09-30 14:31:07,408\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-09-30 14:31:07,407\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-09-30 14:31:07,408\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "\tWarning: Exception caused CatBoost_r50_BAG_L1 to fail during training... Skipping this model.\n",
      "\t\tTask was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: 15ce678cb57ba189347547169e61f34a05283b4701000000, name=_ray_fit, pid=655317, memory used=0.25GB) was running was 959.58GB / 1007.45GB (0.952486), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e89dd3f09ff3f9c5bc88ea743d103d6fa1f8446b801e4a5ab82522ca) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-e89dd3f09ff3f9c5bc88ea743d103d6fa1f8446b801e4a5ab82522ca*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.00\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.78\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "655317\t0.25\tray::_ray_fit\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 2753, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: 15ce678cb57ba189347547169e61f34a05283b4701000000, name=_ray_fit, pid=655317, memory used=0.25GB) was running was 959.58GB / 1007.45GB (0.952486), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e89dd3f09ff3f9c5bc88ea743d103d6fa1f8446b801e4a5ab82522ca) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-e89dd3f09ff3f9c5bc88ea743d103d6fa1f8446b801e4a5ab82522ca*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.00\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.78\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "655317\t0.25\tray::_ray_fit\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\tWarning: Exception caused CatBoost_r50_BAG_L1 to fail during training... Skipping this model.\n",
      "\t\tTask was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: 15ce678cb57ba189347547169e61f34a05283b4701000000, name=_ray_fit, pid=655317, memory used=0.25GB) was running was 959.58GB / 1007.45GB (0.952486), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e89dd3f09ff3f9c5bc88ea743d103d6fa1f8446b801e4a5ab82522ca) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-e89dd3f09ff3f9c5bc88ea743d103d6fa1f8446b801e4a5ab82522ca*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.00\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.78\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "655317\t0.25\tray::_ray_fit\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 2753, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: 15ce678cb57ba189347547169e61f34a05283b4701000000, name=_ray_fit, pid=655317, memory used=0.25GB) was running was 959.58GB / 1007.45GB (0.952486), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e89dd3f09ff3f9c5bc88ea743d103d6fa1f8446b801e4a5ab82522ca) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-e89dd3f09ff3f9c5bc88ea743d103d6fa1f8446b801e4a5ab82522ca*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.00\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.78\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "655317\t0.25\tray::_ray_fit\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Fitting model: NeuralNetFastAI_r11_BAG_L1 ... Training model for up to 103.60s of the 1903.87s of remaining time.\n",
      "Fitting model: NeuralNetFastAI_r11_BAG_L1 ... Training model for up to 103.60s of the 1903.87s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.00%)\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.00%)\n",
      "\tWarning: Exception caused NeuralNetFastAI_r11_BAG_L1 to fail during training... Skipping this model.\n",
      "\t\tTask was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: a0cd24e17a1b8095ab4a96c2acf1be9c502d92df01000000, name=_ray_fit, pid=660002, memory used=0.35GB) was running was 959.89GB / 1007.45GB (0.952791), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e8a03b5f7a24177499e01f0f48714432b1198a497f14219e9fda0a0b) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-e8a03b5f7a24177499e01f0f48714432b1198a497f14219e9fda0a0b*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "521385\t0.87\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "522871\t0.78\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "660002\t0.35\tray::_ray_fit\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 2753, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: a0cd24e17a1b8095ab4a96c2acf1be9c502d92df01000000, name=_ray_fit, pid=660002, memory used=0.35GB) was running was 959.89GB / 1007.45GB (0.952791), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e8a03b5f7a24177499e01f0f48714432b1198a497f14219e9fda0a0b) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-e8a03b5f7a24177499e01f0f48714432b1198a497f14219e9fda0a0b*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "521385\t0.87\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "522871\t0.78\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "660002\t0.35\tray::_ray_fit\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\tWarning: Exception caused NeuralNetFastAI_r11_BAG_L1 to fail during training... Skipping this model.\n",
      "\t\tTask was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: a0cd24e17a1b8095ab4a96c2acf1be9c502d92df01000000, name=_ray_fit, pid=660002, memory used=0.35GB) was running was 959.89GB / 1007.45GB (0.952791), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e8a03b5f7a24177499e01f0f48714432b1198a497f14219e9fda0a0b) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-e8a03b5f7a24177499e01f0f48714432b1198a497f14219e9fda0a0b*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "521385\t0.87\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "522871\t0.78\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "660002\t0.35\tray::_ray_fit\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 2753, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: a0cd24e17a1b8095ab4a96c2acf1be9c502d92df01000000, name=_ray_fit, pid=660002, memory used=0.35GB) was running was 959.89GB / 1007.45GB (0.952791), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e8a03b5f7a24177499e01f0f48714432b1198a497f14219e9fda0a0b) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-e8a03b5f7a24177499e01f0f48714432b1198a497f14219e9fda0a0b*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "521385\t0.87\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "522871\t0.78\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "660002\t0.35\tray::_ray_fit\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Fitting model: XGBoost_r194_BAG_L1 ... Training model for up to 88.92s of the 1889.19s of remaining time.\n",
      "Fitting model: XGBoost_r194_BAG_L1 ... Training model for up to 88.92s of the 1889.19s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.03%)\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.03%)\n",
      "\tWarning: Exception caused XGBoost_r194_BAG_L1 to fail during training... Skipping this model.\n",
      "\t\tTask was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: f271aa4984fa47567b58e51b4cc7ac5db04a6da101000000, name=_ray_fit, pid=662362, memory used=0.19GB) was running was 960.39GB / 1007.45GB (0.953291), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 38ae93e6c20c41316f08fcc53808098b50432919d789e65d41b6503c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-38ae93e6c20c41316f08fcc53808098b50432919d789e65d41b6503c*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.00\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.78\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "662362\t0.19\tray::_ray_fit\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 2753, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: f271aa4984fa47567b58e51b4cc7ac5db04a6da101000000, name=_ray_fit, pid=662362, memory used=0.19GB) was running was 960.39GB / 1007.45GB (0.953291), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 38ae93e6c20c41316f08fcc53808098b50432919d789e65d41b6503c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-38ae93e6c20c41316f08fcc53808098b50432919d789e65d41b6503c*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.00\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.78\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "662362\t0.19\tray::_ray_fit\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\tWarning: Exception caused XGBoost_r194_BAG_L1 to fail during training... Skipping this model.\n",
      "\t\tTask was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: f271aa4984fa47567b58e51b4cc7ac5db04a6da101000000, name=_ray_fit, pid=662362, memory used=0.19GB) was running was 960.39GB / 1007.45GB (0.953291), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 38ae93e6c20c41316f08fcc53808098b50432919d789e65d41b6503c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-38ae93e6c20c41316f08fcc53808098b50432919d789e65d41b6503c*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.00\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.78\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "662362\t0.19\tray::_ray_fit\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 2753, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: f271aa4984fa47567b58e51b4cc7ac5db04a6da101000000, name=_ray_fit, pid=662362, memory used=0.19GB) was running was 960.39GB / 1007.45GB (0.953291), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 38ae93e6c20c41316f08fcc53808098b50432919d789e65d41b6503c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-38ae93e6c20c41316f08fcc53808098b50432919d789e65d41b6503c*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.00\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.78\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "662362\t0.19\tray::_ray_fit\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Fitting model: ExtraTrees_r172_BAG_L1 ... Training model for up to 79.10s of the 1879.38s of remaining time.\n",
      "Fitting model: ExtraTrees_r172_BAG_L1 ... Training model for up to 79.10s of the 1879.38s of remaining time.\n",
      "\t-0.1509\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.28s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: CatBoost_r69_BAG_L1 ... Training model for up to 78.67s of the 1878.94s of remaining time.\n",
      "\t-0.1509\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.28s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: CatBoost_r69_BAG_L1 ... Training model for up to 78.67s of the 1878.94s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.01%)\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.01%)\n",
      "\tWarning: Exception caused CatBoost_r69_BAG_L1 to fail during training... Skipping this model.\n",
      "\t\tTask was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: 081cc4ed597ccf0a8ac939a75f7547ba8daeca8301000000, name=_ray_fit, pid=664318, memory used=0.11GB) was running was 960.31GB / 1007.45GB (0.953211), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6b165ea9cf73211e18e7d57c00782e980a6329f443a8481f195d95a8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-6b165ea9cf73211e18e7d57c00782e980a6329f443a8481f195d95a8*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.01\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.79\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "664318\t0.11\tray::IDLE\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 2753, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: 081cc4ed597ccf0a8ac939a75f7547ba8daeca8301000000, name=_ray_fit, pid=664318, memory used=0.11GB) was running was 960.31GB / 1007.45GB (0.953211), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6b165ea9cf73211e18e7d57c00782e980a6329f443a8481f195d95a8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-6b165ea9cf73211e18e7d57c00782e980a6329f443a8481f195d95a8*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.01\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.79\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "664318\t0.11\tray::IDLE\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\tWarning: Exception caused CatBoost_r69_BAG_L1 to fail during training... Skipping this model.\n",
      "\t\tTask was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: 081cc4ed597ccf0a8ac939a75f7547ba8daeca8301000000, name=_ray_fit, pid=664318, memory used=0.11GB) was running was 960.31GB / 1007.45GB (0.953211), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6b165ea9cf73211e18e7d57c00782e980a6329f443a8481f195d95a8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-6b165ea9cf73211e18e7d57c00782e980a6329f443a8481f195d95a8*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.01\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.79\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "664318\t0.11\tray::IDLE\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 2753, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: 081cc4ed597ccf0a8ac939a75f7547ba8daeca8301000000, name=_ray_fit, pid=664318, memory used=0.11GB) was running was 960.31GB / 1007.45GB (0.953211), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6b165ea9cf73211e18e7d57c00782e980a6329f443a8481f195d95a8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-6b165ea9cf73211e18e7d57c00782e980a6329f443a8481f195d95a8*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.01\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.79\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "664318\t0.11\tray::IDLE\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Fitting model: NeuralNetFastAI_r103_BAG_L1 ... Training model for up to 72.21s of the 1872.49s of remaining time.\n",
      "Fitting model: NeuralNetFastAI_r103_BAG_L1 ... Training model for up to 72.21s of the 1872.49s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.00%)\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.00%)\n",
      "2025-09-30 14:31:45,420\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-09-30 14:31:45,420\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "\tWarning: Exception caused NeuralNetFastAI_r103_BAG_L1 to fail during training... Skipping this model.\n",
      "\t\tTask was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: 1bc3464b080aa3d686650a9c3742212d49ea633e01000000, name=_ray_fit, pid=667182, memory used=0.39GB) was running was 960.85GB / 1007.45GB (0.953746), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 444289e0826399f22aa5bdbb11e58c154e88c3672ef0f344aac668d0) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-444289e0826399f22aa5bdbb11e58c154e88c3672ef0f344aac668d0*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "521385\t0.87\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "522871\t0.79\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "667182\t0.39\tray::_ray_fit\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 2753, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: 1bc3464b080aa3d686650a9c3742212d49ea633e01000000, name=_ray_fit, pid=667182, memory used=0.39GB) was running was 960.85GB / 1007.45GB (0.953746), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 444289e0826399f22aa5bdbb11e58c154e88c3672ef0f344aac668d0) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-444289e0826399f22aa5bdbb11e58c154e88c3672ef0f344aac668d0*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "521385\t0.87\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "522871\t0.79\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "667182\t0.39\tray::_ray_fit\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\tWarning: Exception caused NeuralNetFastAI_r103_BAG_L1 to fail during training... Skipping this model.\n",
      "\t\tTask was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: 1bc3464b080aa3d686650a9c3742212d49ea633e01000000, name=_ray_fit, pid=667182, memory used=0.39GB) was running was 960.85GB / 1007.45GB (0.953746), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 444289e0826399f22aa5bdbb11e58c154e88c3672ef0f344aac668d0) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-444289e0826399f22aa5bdbb11e58c154e88c3672ef0f344aac668d0*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "521385\t0.87\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "522871\t0.79\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "667182\t0.39\tray::_ray_fit\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 2753, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: 1bc3464b080aa3d686650a9c3742212d49ea633e01000000, name=_ray_fit, pid=667182, memory used=0.39GB) was running was 960.85GB / 1007.45GB (0.953746), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 444289e0826399f22aa5bdbb11e58c154e88c3672ef0f344aac668d0) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-444289e0826399f22aa5bdbb11e58c154e88c3672ef0f344aac668d0*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "521385\t0.87\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "522871\t0.79\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "667182\t0.39\tray::_ray_fit\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Fitting model: NeuralNetTorch_r14_BAG_L1 ... Training model for up to 57.19s of the 1857.46s of remaining time.\n",
      "Fitting model: NeuralNetTorch_r14_BAG_L1 ... Training model for up to 57.19s of the 1857.46s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.00%)\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.00%)\n",
      "\tWarning: Exception caused NeuralNetTorch_r14_BAG_L1 to fail during training... Skipping this model.\n",
      "\t\t\u001b[36mray::_ray_fit()\u001b[39m (pid=667958, ip=172.17.0.5)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 413, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 201, in _fit\n",
      "    train_dataset = self._generate_dataset(X, y, train_params=processor_kwargs, is_train=True)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 669, in _generate_dataset\n",
      "    dataset = self._process_train_data(\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 734, in _process_train_data\n",
      "    self.processor = create_preprocessor(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/utils/data_preprocessor.py\", line 40, in create_preprocessor\n",
      "    return ColumnTransformer(\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "TypeError: ColumnTransformer.__init__() got an unexpected keyword argument 'force_int_remainder_cols'\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 2753, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 904, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(TypeError): \u001b[36mray::_ray_fit()\u001b[39m (pid=667958, ip=172.17.0.5)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 413, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 201, in _fit\n",
      "    train_dataset = self._generate_dataset(X, y, train_params=processor_kwargs, is_train=True)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 669, in _generate_dataset\n",
      "    dataset = self._process_train_data(\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 734, in _process_train_data\n",
      "    self.processor = create_preprocessor(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/utils/data_preprocessor.py\", line 40, in create_preprocessor\n",
      "    return ColumnTransformer(\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "TypeError: ColumnTransformer.__init__() got an unexpected keyword argument 'force_int_remainder_cols'\n",
      "\tWarning: Exception caused NeuralNetTorch_r14_BAG_L1 to fail during training... Skipping this model.\n",
      "\t\t\u001b[36mray::_ray_fit()\u001b[39m (pid=667958, ip=172.17.0.5)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 413, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 201, in _fit\n",
      "    train_dataset = self._generate_dataset(X, y, train_params=processor_kwargs, is_train=True)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 669, in _generate_dataset\n",
      "    dataset = self._process_train_data(\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 734, in _process_train_data\n",
      "    self.processor = create_preprocessor(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/utils/data_preprocessor.py\", line 40, in create_preprocessor\n",
      "    return ColumnTransformer(\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "TypeError: ColumnTransformer.__init__() got an unexpected keyword argument 'force_int_remainder_cols'\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 2753, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 904, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(TypeError): \u001b[36mray::_ray_fit()\u001b[39m (pid=667958, ip=172.17.0.5)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 413, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 201, in _fit\n",
      "    train_dataset = self._generate_dataset(X, y, train_params=processor_kwargs, is_train=True)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 669, in _generate_dataset\n",
      "    dataset = self._process_train_data(\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 734, in _process_train_data\n",
      "    self.processor = create_preprocessor(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/utils/data_preprocessor.py\", line 40, in create_preprocessor\n",
      "    return ColumnTransformer(\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "TypeError: ColumnTransformer.__init__() got an unexpected keyword argument 'force_int_remainder_cols'\n",
      "Fitting model: LightGBM_r161_BAG_L1 ... Training model for up to 53.56s of the 1853.83s of remaining time.\n",
      "Fitting model: LightGBM_r161_BAG_L1 ... Training model for up to 53.56s of the 1853.83s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.06%)\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.06%)\n",
      "\tWarning: Exception caused LightGBM_r161_BAG_L1 to fail during training... Skipping this model.\n",
      "\t\tTask was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: 920f509fd72280c500bad7644a8cc05d9234a33601000000, name=_ray_fit, pid=668826, memory used=0.07GB) was running was 961.24GB / 1007.45GB (0.954128), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: f4706ce3bb99e8e61e14ac53cc5fd0d976c91ad491ad3540315a67ab) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-f4706ce3bb99e8e61e14ac53cc5fd0d976c91ad491ad3540315a67ab*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "521385\t0.87\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "522871\t0.79\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "443\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-lab --ip=* --port=8888 --no-browser --LabApp.token= --LabApp...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 2753, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: 920f509fd72280c500bad7644a8cc05d9234a33601000000, name=_ray_fit, pid=668826, memory used=0.07GB) was running was 961.24GB / 1007.45GB (0.954128), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: f4706ce3bb99e8e61e14ac53cc5fd0d976c91ad491ad3540315a67ab) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-f4706ce3bb99e8e61e14ac53cc5fd0d976c91ad491ad3540315a67ab*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "521385\t0.87\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "522871\t0.79\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "443\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-lab --ip=* --port=8888 --no-browser --LabApp.token= --LabApp...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\tWarning: Exception caused LightGBM_r161_BAG_L1 to fail during training... Skipping this model.\n",
      "\t\tTask was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: 920f509fd72280c500bad7644a8cc05d9234a33601000000, name=_ray_fit, pid=668826, memory used=0.07GB) was running was 961.24GB / 1007.45GB (0.954128), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: f4706ce3bb99e8e61e14ac53cc5fd0d976c91ad491ad3540315a67ab) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-f4706ce3bb99e8e61e14ac53cc5fd0d976c91ad491ad3540315a67ab*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "521385\t0.87\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "522871\t0.79\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "443\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-lab --ip=* --port=8888 --no-browser --LabApp.token= --LabApp...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 2753, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: 920f509fd72280c500bad7644a8cc05d9234a33601000000, name=_ray_fit, pid=668826, memory used=0.07GB) was running was 961.24GB / 1007.45GB (0.954128), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: f4706ce3bb99e8e61e14ac53cc5fd0d976c91ad491ad3540315a67ab) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-f4706ce3bb99e8e61e14ac53cc5fd0d976c91ad491ad3540315a67ab*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "521385\t0.87\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "522871\t0.79\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "443\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-lab --ip=* --port=8888 --no-browser --LabApp.token= --LabApp...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "2025-09-30 14:31:58,975\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-09-30 14:31:58,975\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-09-30 14:31:58,976\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "Fitting model: NeuralNetFastAI_r143_BAG_L1 ... Training model for up to 53.02s of the 1853.30s of remaining time.\n",
      "2025-09-30 14:31:58,975\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-09-30 14:31:58,975\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-09-30 14:31:58,976\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "Fitting model: NeuralNetFastAI_r143_BAG_L1 ... Training model for up to 53.02s of the 1853.30s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.00%)\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.00%)\n",
      "\tWarning: Exception caused NeuralNetFastAI_r143_BAG_L1 to fail during training... Skipping this model.\n",
      "\t\tTask was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: 828be81f2e75879b75f7352b8a9659be36e7e0d601000000, name=_ray_fit, pid=669565, memory used=0.33GB) was running was 961.89GB / 1007.45GB (0.954775), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4f1bde4859ca4d33226378ace13b7ea80d64a9f9ddc3caef4fcf6ff7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-4f1bde4859ca4d33226378ace13b7ea80d64a9f9ddc3caef4fcf6ff7*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "521385\t0.87\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "522871\t0.79\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "669566\t0.33\tray::IDLE\n",
      "669565\t0.33\tray::_ray_fit\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 2753, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: 828be81f2e75879b75f7352b8a9659be36e7e0d601000000, name=_ray_fit, pid=669565, memory used=0.33GB) was running was 961.89GB / 1007.45GB (0.954775), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4f1bde4859ca4d33226378ace13b7ea80d64a9f9ddc3caef4fcf6ff7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-4f1bde4859ca4d33226378ace13b7ea80d64a9f9ddc3caef4fcf6ff7*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "521385\t0.87\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "522871\t0.79\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "669566\t0.33\tray::IDLE\n",
      "669565\t0.33\tray::_ray_fit\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\tWarning: Exception caused NeuralNetFastAI_r143_BAG_L1 to fail during training... Skipping this model.\n",
      "\t\tTask was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: 828be81f2e75879b75f7352b8a9659be36e7e0d601000000, name=_ray_fit, pid=669565, memory used=0.33GB) was running was 961.89GB / 1007.45GB (0.954775), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4f1bde4859ca4d33226378ace13b7ea80d64a9f9ddc3caef4fcf6ff7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-4f1bde4859ca4d33226378ace13b7ea80d64a9f9ddc3caef4fcf6ff7*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "521385\t0.87\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "522871\t0.79\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "669566\t0.33\tray::IDLE\n",
      "669565\t0.33\tray::_ray_fit\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 2753, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: 828be81f2e75879b75f7352b8a9659be36e7e0d601000000, name=_ray_fit, pid=669565, memory used=0.33GB) was running was 961.89GB / 1007.45GB (0.954775), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4f1bde4859ca4d33226378ace13b7ea80d64a9f9ddc3caef4fcf6ff7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-4f1bde4859ca4d33226378ace13b7ea80d64a9f9ddc3caef4fcf6ff7*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "521385\t0.87\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "522871\t0.79\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "669566\t0.33\tray::IDLE\n",
      "669565\t0.33\tray::_ray_fit\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Fitting model: CatBoost_r70_BAG_L1 ... Training model for up to 44.35s of the 1844.62s of remaining time.\n",
      "Fitting model: CatBoost_r70_BAG_L1 ... Training model for up to 44.35s of the 1844.62s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.02%)\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.02%)\n",
      "\tWarning: Exception caused CatBoost_r70_BAG_L1 to fail during training... Skipping this model.\n",
      "\t\tTask was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: f21abc0777d9cc64b8ed5f83ccf400473977954b01000000, name=_ray_fit, pid=671851, memory used=0.24GB) was running was 961.43GB / 1007.45GB (0.954324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 044ef6487c4eef2fcdd3eda132f4542251c82bcdb37d9fadc8f0c0c9) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-044ef6487c4eef2fcdd3eda132f4542251c82bcdb37d9fadc8f0c0c9*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.01\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.79\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "671851\t0.24\tray::_ray_fit\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 2753, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: f21abc0777d9cc64b8ed5f83ccf400473977954b01000000, name=_ray_fit, pid=671851, memory used=0.24GB) was running was 961.43GB / 1007.45GB (0.954324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 044ef6487c4eef2fcdd3eda132f4542251c82bcdb37d9fadc8f0c0c9) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-044ef6487c4eef2fcdd3eda132f4542251c82bcdb37d9fadc8f0c0c9*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.01\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.79\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "671851\t0.24\tray::_ray_fit\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\tWarning: Exception caused CatBoost_r70_BAG_L1 to fail during training... Skipping this model.\n",
      "\t\tTask was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: f21abc0777d9cc64b8ed5f83ccf400473977954b01000000, name=_ray_fit, pid=671851, memory used=0.24GB) was running was 961.43GB / 1007.45GB (0.954324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 044ef6487c4eef2fcdd3eda132f4542251c82bcdb37d9fadc8f0c0c9) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-044ef6487c4eef2fcdd3eda132f4542251c82bcdb37d9fadc8f0c0c9*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.01\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.79\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "671851\t0.24\tray::_ray_fit\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 2753, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: f21abc0777d9cc64b8ed5f83ccf400473977954b01000000, name=_ray_fit, pid=671851, memory used=0.24GB) was running was 961.43GB / 1007.45GB (0.954324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 044ef6487c4eef2fcdd3eda132f4542251c82bcdb37d9fadc8f0c0c9) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-044ef6487c4eef2fcdd3eda132f4542251c82bcdb37d9fadc8f0c0c9*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.01\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.79\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "671851\t0.24\tray::_ray_fit\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Fitting model: NeuralNetFastAI_r156_BAG_L1 ... Training model for up to 38.96s of the 1839.23s of remaining time.\n",
      "Fitting model: NeuralNetFastAI_r156_BAG_L1 ... Training model for up to 38.96s of the 1839.23s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.00%)\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.00%)\n",
      "2025-09-30 14:32:18,429\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-09-30 14:32:18,429\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "\tWarning: Exception caused NeuralNetFastAI_r156_BAG_L1 to fail during training... Skipping this model.\n",
      "\t\tTask was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: c1855a14ef52160d20ea3b45fe7ab37b3fbbb33501000000, name=_ray_fit, pid=675546, memory used=0.12GB) was running was 961.73GB / 1007.45GB (0.954613), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 52b811f635e07f61c01267b08fe7c8c581bce6182a61ccb0e4ee2eca) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-52b811f635e07f61c01267b08fe7c8c581bce6182a61ccb0e4ee2eca*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.01\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.79\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "675546\t0.12\tray::IDLE\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 2753, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: c1855a14ef52160d20ea3b45fe7ab37b3fbbb33501000000, name=_ray_fit, pid=675546, memory used=0.12GB) was running was 961.73GB / 1007.45GB (0.954613), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 52b811f635e07f61c01267b08fe7c8c581bce6182a61ccb0e4ee2eca) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-52b811f635e07f61c01267b08fe7c8c581bce6182a61ccb0e4ee2eca*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.01\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.79\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "675546\t0.12\tray::IDLE\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\tWarning: Exception caused NeuralNetFastAI_r156_BAG_L1 to fail during training... Skipping this model.\n",
      "\t\tTask was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: c1855a14ef52160d20ea3b45fe7ab37b3fbbb33501000000, name=_ray_fit, pid=675546, memory used=0.12GB) was running was 961.73GB / 1007.45GB (0.954613), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 52b811f635e07f61c01267b08fe7c8c581bce6182a61ccb0e4ee2eca) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-52b811f635e07f61c01267b08fe7c8c581bce6182a61ccb0e4ee2eca*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.01\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.79\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "675546\t0.12\tray::IDLE\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 2753, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: c1855a14ef52160d20ea3b45fe7ab37b3fbbb33501000000, name=_ray_fit, pid=675546, memory used=0.12GB) was running was 961.73GB / 1007.45GB (0.954613), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 52b811f635e07f61c01267b08fe7c8c581bce6182a61ccb0e4ee2eca) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-52b811f635e07f61c01267b08fe7c8c581bce6182a61ccb0e4ee2eca*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.01\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.79\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "675546\t0.12\tray::IDLE\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Fitting model: LightGBM_r196_BAG_L1 ... Training model for up to 29.37s of the 1829.64s of remaining time.\n",
      "Fitting model: LightGBM_r196_BAG_L1 ... Training model for up to 29.37s of the 1829.64s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.03%)\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.03%)\n",
      "\tWarning: Exception caused LightGBM_r196_BAG_L1 to fail during training... Skipping this model.\n",
      "\t\tTask was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: 1669cf571bdad58e683e4ae9a1ecb68afe3a696501000000, name=_ray_fit, pid=677993, memory used=0.11GB) was running was 961.96GB / 1007.45GB (0.954846), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4d1c50ace26699cbadadbdbcb45b3083f7fdf9e65b73773015629cc) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-d4d1c50ace26699cbadadbdbcb45b3083f7fdf9e65b73773015629cc*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.01\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.79\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "677993\t0.11\tray::IDLE\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 2753, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: 1669cf571bdad58e683e4ae9a1ecb68afe3a696501000000, name=_ray_fit, pid=677993, memory used=0.11GB) was running was 961.96GB / 1007.45GB (0.954846), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4d1c50ace26699cbadadbdbcb45b3083f7fdf9e65b73773015629cc) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-d4d1c50ace26699cbadadbdbcb45b3083f7fdf9e65b73773015629cc*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.01\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.79\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "677993\t0.11\tray::IDLE\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\tWarning: Exception caused LightGBM_r196_BAG_L1 to fail during training... Skipping this model.\n",
      "\t\tTask was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: 1669cf571bdad58e683e4ae9a1ecb68afe3a696501000000, name=_ray_fit, pid=677993, memory used=0.11GB) was running was 961.96GB / 1007.45GB (0.954846), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4d1c50ace26699cbadadbdbcb45b3083f7fdf9e65b73773015629cc) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-d4d1c50ace26699cbadadbdbcb45b3083f7fdf9e65b73773015629cc*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.01\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.79\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "677993\t0.11\tray::IDLE\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 2753, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: 1669cf571bdad58e683e4ae9a1ecb68afe3a696501000000, name=_ray_fit, pid=677993, memory used=0.11GB) was running was 961.96GB / 1007.45GB (0.954846), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4d1c50ace26699cbadadbdbcb45b3083f7fdf9e65b73773015629cc) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-d4d1c50ace26699cbadadbdbcb45b3083f7fdf9e65b73773015629cc*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.01\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.79\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "677993\t0.11\tray::IDLE\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Fitting model: RandomForest_r39_BAG_L1 ... Training model for up to 17.50s of the 1817.77s of remaining time.\n",
      "Fitting model: RandomForest_r39_BAG_L1 ... Training model for up to 17.50s of the 1817.77s of remaining time.\n",
      "\t-0.1275\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.36s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: CatBoost_r167_BAG_L1 ... Training model for up to 17.00s of the 1817.27s of remaining time.\n",
      "\t-0.1275\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.36s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: CatBoost_r167_BAG_L1 ... Training model for up to 17.00s of the 1817.27s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.03%)\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.03%)\n",
      "\tWarning: Exception caused CatBoost_r167_BAG_L1 to fail during training... Skipping this model.\n",
      "\t\tTask was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: d3767b8aed239a97cfb3ba3cd93d822e45049d7501000000, name=_ray_fit, pid=678179, memory used=0.24GB) was running was 961.97GB / 1007.45GB (0.954857), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 2ca0659ee47a6787242a0caa8e6b4bcfe05dcf29aec646e7d92b0639) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-2ca0659ee47a6787242a0caa8e6b4bcfe05dcf29aec646e7d92b0639*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.02\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.79\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "678179\t0.24\tray::_ray_fit\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 2753, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: d3767b8aed239a97cfb3ba3cd93d822e45049d7501000000, name=_ray_fit, pid=678179, memory used=0.24GB) was running was 961.97GB / 1007.45GB (0.954857), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 2ca0659ee47a6787242a0caa8e6b4bcfe05dcf29aec646e7d92b0639) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-2ca0659ee47a6787242a0caa8e6b4bcfe05dcf29aec646e7d92b0639*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.02\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.79\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "678179\t0.24\tray::_ray_fit\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\tWarning: Exception caused CatBoost_r167_BAG_L1 to fail during training... Skipping this model.\n",
      "\t\tTask was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: d3767b8aed239a97cfb3ba3cd93d822e45049d7501000000, name=_ray_fit, pid=678179, memory used=0.24GB) was running was 961.97GB / 1007.45GB (0.954857), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 2ca0659ee47a6787242a0caa8e6b4bcfe05dcf29aec646e7d92b0639) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-2ca0659ee47a6787242a0caa8e6b4bcfe05dcf29aec646e7d92b0639*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.02\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.79\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "678179\t0.24\tray::_ray_fit\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 2753, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: d3767b8aed239a97cfb3ba3cd93d822e45049d7501000000, name=_ray_fit, pid=678179, memory used=0.24GB) was running was 961.97GB / 1007.45GB (0.954857), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 2ca0659ee47a6787242a0caa8e6b4bcfe05dcf29aec646e7d92b0639) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-2ca0659ee47a6787242a0caa8e6b4bcfe05dcf29aec646e7d92b0639*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.02\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.79\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "678179\t0.24\tray::_ray_fit\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Fitting model: NeuralNetFastAI_r95_BAG_L1 ... Training model for up to 12.00s of the 1812.27s of remaining time.\n",
      "Fitting model: NeuralNetFastAI_r95_BAG_L1 ... Training model for up to 12.00s of the 1812.27s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.00%)\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.00%)\n",
      "\tWarning: Exception caused NeuralNetFastAI_r95_BAG_L1 to fail during training... Skipping this model.\n",
      "\t\tTask was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: 99732077f94540eb30c035ae7ad3f2d4d73a8cc401000000, name=_ray_fit, pid=682285, memory used=0.12GB) was running was 961.74GB / 1007.45GB (0.954628), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6692eda775eb59d79222ead6bc4fc3b018b8317797f8736385b06faa) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-6692eda775eb59d79222ead6bc4fc3b018b8317797f8736385b06faa*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.03\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.79\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "682285\t0.12\tray::IDLE\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 2753, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: 99732077f94540eb30c035ae7ad3f2d4d73a8cc401000000, name=_ray_fit, pid=682285, memory used=0.12GB) was running was 961.74GB / 1007.45GB (0.954628), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6692eda775eb59d79222ead6bc4fc3b018b8317797f8736385b06faa) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-6692eda775eb59d79222ead6bc4fc3b018b8317797f8736385b06faa*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.03\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.79\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "682285\t0.12\tray::IDLE\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\tWarning: Exception caused NeuralNetFastAI_r95_BAG_L1 to fail during training... Skipping this model.\n",
      "\t\tTask was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: 99732077f94540eb30c035ae7ad3f2d4d73a8cc401000000, name=_ray_fit, pid=682285, memory used=0.12GB) was running was 961.74GB / 1007.45GB (0.954628), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6692eda775eb59d79222ead6bc4fc3b018b8317797f8736385b06faa) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-6692eda775eb59d79222ead6bc4fc3b018b8317797f8736385b06faa*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.03\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.79\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "682285\t0.12\tray::IDLE\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 2753, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: 99732077f94540eb30c035ae7ad3f2d4d73a8cc401000000, name=_ray_fit, pid=682285, memory used=0.12GB) was running was 961.74GB / 1007.45GB (0.954628), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6692eda775eb59d79222ead6bc4fc3b018b8317797f8736385b06faa) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-6692eda775eb59d79222ead6bc4fc3b018b8317797f8736385b06faa*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.03\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.79\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "682285\t0.12\tray::IDLE\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Fitting model: NeuralNetTorch_r41_BAG_L1 ... Training model for up to 2.45s of the 1802.72s of remaining time.\n",
      "Fitting model: NeuralNetTorch_r41_BAG_L1 ... Training model for up to 2.45s of the 1802.72s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.00%)\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.00%)\n",
      "\tWarning: Exception caused NeuralNetTorch_r41_BAG_L1 to fail during training... Skipping this model.\n",
      "\t\t\u001b[36mray::_ray_fit()\u001b[39m (pid=682526, ip=172.17.0.5)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 413, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 201, in _fit\n",
      "    train_dataset = self._generate_dataset(X, y, train_params=processor_kwargs, is_train=True)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 669, in _generate_dataset\n",
      "    dataset = self._process_train_data(\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 734, in _process_train_data\n",
      "    self.processor = create_preprocessor(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/utils/data_preprocessor.py\", line 40, in create_preprocessor\n",
      "    return ColumnTransformer(\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "TypeError: ColumnTransformer.__init__() got an unexpected keyword argument 'force_int_remainder_cols'\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 2753, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 904, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(TypeError): \u001b[36mray::_ray_fit()\u001b[39m (pid=682526, ip=172.17.0.5)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 413, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 201, in _fit\n",
      "    train_dataset = self._generate_dataset(X, y, train_params=processor_kwargs, is_train=True)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 669, in _generate_dataset\n",
      "    dataset = self._process_train_data(\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 734, in _process_train_data\n",
      "    self.processor = create_preprocessor(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/utils/data_preprocessor.py\", line 40, in create_preprocessor\n",
      "    return ColumnTransformer(\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "TypeError: ColumnTransformer.__init__() got an unexpected keyword argument 'force_int_remainder_cols'\n",
      "\tWarning: Exception caused NeuralNetTorch_r41_BAG_L1 to fail during training... Skipping this model.\n",
      "\t\t\u001b[36mray::_ray_fit()\u001b[39m (pid=682526, ip=172.17.0.5)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 413, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 201, in _fit\n",
      "    train_dataset = self._generate_dataset(X, y, train_params=processor_kwargs, is_train=True)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 669, in _generate_dataset\n",
      "    dataset = self._process_train_data(\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 734, in _process_train_data\n",
      "    self.processor = create_preprocessor(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/utils/data_preprocessor.py\", line 40, in create_preprocessor\n",
      "    return ColumnTransformer(\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "TypeError: ColumnTransformer.__init__() got an unexpected keyword argument 'force_int_remainder_cols'\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 2753, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 904, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(TypeError): \u001b[36mray::_ray_fit()\u001b[39m (pid=682526, ip=172.17.0.5)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 413, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 201, in _fit\n",
      "    train_dataset = self._generate_dataset(X, y, train_params=processor_kwargs, is_train=True)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 669, in _generate_dataset\n",
      "    dataset = self._process_train_data(\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 734, in _process_train_data\n",
      "    self.processor = create_preprocessor(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/utils/data_preprocessor.py\", line 40, in create_preprocessor\n",
      "    return ColumnTransformer(\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "TypeError: ColumnTransformer.__init__() got an unexpected keyword argument 'force_int_remainder_cols'\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 360.00s of the 1798.88s of remaining time.\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 360.00s of the 1798.88s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBM_BAG_L1': 0.391, 'XGBoost_BAG_L1': 0.304, 'RandomForestMSE_BAG_L1': 0.217, 'CatBoost_BAG_L1': 0.087}\n",
      "\t-0.1154\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "\tEnsemble Weights: {'LightGBM_BAG_L1': 0.391, 'XGBoost_BAG_L1': 0.304, 'RandomForestMSE_BAG_L1': 0.217, 'CatBoost_BAG_L1': 0.087}\n",
      "\t-0.1154\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting 106 L2 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBMXT_BAG_L2 ... Training model for up to 1798.83s of the 1798.81s of remaining time.\n",
      "Fitting 106 L2 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBMXT_BAG_L2 ... Training model for up to 1798.83s of the 1798.81s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.01%)\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.01%)\n",
      "\tWarning: Exception caused LightGBMXT_BAG_L2 to fail during training... Skipping this model.\n",
      "\t\tTask was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: 1b294a967b35583a8030e8fda832ecd465d5af2801000000, name=_ray_fit, pid=683439, memory used=0.07GB) was running was 963.14GB / 1007.45GB (0.95602), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: de668d36112af32c06a60d90e6dbb43a341797d2d5ae7b17924c845e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-de668d36112af32c06a60d90e6dbb43a341797d2d5ae7b17924c845e*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "521385\t0.89\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "522871\t0.79\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "682525\t0.31\tray::IDLE\n",
      "682526\t0.30\tray::IDLE\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 2753, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: 1b294a967b35583a8030e8fda832ecd465d5af2801000000, name=_ray_fit, pid=683439, memory used=0.07GB) was running was 963.14GB / 1007.45GB (0.95602), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: de668d36112af32c06a60d90e6dbb43a341797d2d5ae7b17924c845e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-de668d36112af32c06a60d90e6dbb43a341797d2d5ae7b17924c845e*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "521385\t0.89\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "522871\t0.79\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "682525\t0.31\tray::IDLE\n",
      "682526\t0.30\tray::IDLE\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\tWarning: Exception caused LightGBMXT_BAG_L2 to fail during training... Skipping this model.\n",
      "\t\tTask was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: 1b294a967b35583a8030e8fda832ecd465d5af2801000000, name=_ray_fit, pid=683439, memory used=0.07GB) was running was 963.14GB / 1007.45GB (0.95602), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: de668d36112af32c06a60d90e6dbb43a341797d2d5ae7b17924c845e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-de668d36112af32c06a60d90e6dbb43a341797d2d5ae7b17924c845e*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "521385\t0.89\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "522871\t0.79\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "682525\t0.31\tray::IDLE\n",
      "682526\t0.30\tray::IDLE\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 2753, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: 1b294a967b35583a8030e8fda832ecd465d5af2801000000, name=_ray_fit, pid=683439, memory used=0.07GB) was running was 963.14GB / 1007.45GB (0.95602), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: de668d36112af32c06a60d90e6dbb43a341797d2d5ae7b17924c845e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-de668d36112af32c06a60d90e6dbb43a341797d2d5ae7b17924c845e*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "521385\t0.89\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "522871\t0.79\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "682525\t0.31\tray::IDLE\n",
      "682526\t0.30\tray::IDLE\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "2025-09-30 14:32:54,087\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-09-30 14:32:54,088\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::_ray_fit()\u001b[39m (pid=682525, ip=172.17.0.5)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 413, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 201, in _fit\n",
      "    train_dataset = self._generate_dataset(X, y, train_params=processor_kwargs, is_train=True)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 669, in _generate_dataset\n",
      "    dataset = self._process_train_data(\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 734, in _process_train_data\n",
      "    self.processor = create_preprocessor(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/utils/data_preprocessor.py\", line 40, in create_preprocessor\n",
      "    return ColumnTransformer(\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "TypeError: ColumnTransformer.__init__() got an unexpected keyword argument 'force_int_remainder_cols'\n",
      "2025-09-30 14:32:54,089\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "Fitting model: LightGBM_BAG_L2 ... Training model for up to 1798.21s of the 1798.18s of remaining time.\n",
      "2025-09-30 14:32:54,087\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-09-30 14:32:54,088\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::_ray_fit()\u001b[39m (pid=682525, ip=172.17.0.5)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 413, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 201, in _fit\n",
      "    train_dataset = self._generate_dataset(X, y, train_params=processor_kwargs, is_train=True)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 669, in _generate_dataset\n",
      "    dataset = self._process_train_data(\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 734, in _process_train_data\n",
      "    self.processor = create_preprocessor(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/utils/data_preprocessor.py\", line 40, in create_preprocessor\n",
      "    return ColumnTransformer(\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "TypeError: ColumnTransformer.__init__() got an unexpected keyword argument 'force_int_remainder_cols'\n",
      "2025-09-30 14:32:54,089\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "Fitting model: LightGBM_BAG_L2 ... Training model for up to 1798.21s of the 1798.18s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.01%)\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.01%)\n",
      "\tWarning: Exception caused LightGBM_BAG_L2 to fail during training... Skipping this model.\n",
      "\t\tTask was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: 2f9e3b70e0474b2f88dbad88fd7acabc8947283001000000, name=_ray_fit, pid=684173, memory used=0.10GB) was running was 962.13GB / 1007.45GB (0.955011), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: bab0ae0bcb812417a127bfe1cee41afbc628a7750f035f68f2adec12) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-bab0ae0bcb812417a127bfe1cee41afbc628a7750f035f68f2adec12*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.03\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.79\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "684173\t0.10\tray::IDLE\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 2753, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: 2f9e3b70e0474b2f88dbad88fd7acabc8947283001000000, name=_ray_fit, pid=684173, memory used=0.10GB) was running was 962.13GB / 1007.45GB (0.955011), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: bab0ae0bcb812417a127bfe1cee41afbc628a7750f035f68f2adec12) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-bab0ae0bcb812417a127bfe1cee41afbc628a7750f035f68f2adec12*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.03\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.79\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "684173\t0.10\tray::IDLE\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\tWarning: Exception caused LightGBM_BAG_L2 to fail during training... Skipping this model.\n",
      "\t\tTask was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: 2f9e3b70e0474b2f88dbad88fd7acabc8947283001000000, name=_ray_fit, pid=684173, memory used=0.10GB) was running was 962.13GB / 1007.45GB (0.955011), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: bab0ae0bcb812417a127bfe1cee41afbc628a7750f035f68f2adec12) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-bab0ae0bcb812417a127bfe1cee41afbc628a7750f035f68f2adec12*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.03\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.79\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "684173\t0.10\tray::IDLE\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 2753, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: 2f9e3b70e0474b2f88dbad88fd7acabc8947283001000000, name=_ray_fit, pid=684173, memory used=0.10GB) was running was 962.13GB / 1007.45GB (0.955011), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: bab0ae0bcb812417a127bfe1cee41afbc628a7750f035f68f2adec12) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-bab0ae0bcb812417a127bfe1cee41afbc628a7750f035f68f2adec12*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.03\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.79\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "684173\t0.10\tray::IDLE\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "2025-09-30 14:32:54,603\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-09-30 14:32:54,605\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-09-30 14:32:54,605\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-09-30 14:32:54,607\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-09-30 14:32:54,608\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-09-30 14:32:54,609\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-09-30 14:32:54,610\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "Fitting model: RandomForestMSE_BAG_L2 ... Training model for up to 1797.69s of the 1797.66s of remaining time.\n",
      "2025-09-30 14:32:54,603\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-09-30 14:32:54,605\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-09-30 14:32:54,605\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-09-30 14:32:54,607\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-09-30 14:32:54,608\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-09-30 14:32:54,609\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-09-30 14:32:54,610\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "Fitting model: RandomForestMSE_BAG_L2 ... Training model for up to 1797.69s of the 1797.66s of remaining time.\n",
      "\t-0.1296\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.34s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L2 ... Training model for up to 1797.21s of the 1797.19s of remaining time.\n",
      "\t-0.1296\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.34s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L2 ... Training model for up to 1797.21s of the 1797.19s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.03%)\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.03%)\n",
      "\tWarning: Exception caused CatBoost_BAG_L2 to fail during training... Skipping this model.\n",
      "\t\tTask was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: d2936572c77478d93c8b3b76e65aaff8782881bd01000000, name=_ray_fit, pid=685117, memory used=0.16GB) was running was 962.64GB / 1007.45GB (0.955518), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: dc8ac076d5d78b06962ca7eb8a35ee19e6b81d0a0987ba4f84195c95) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-dc8ac076d5d78b06962ca7eb8a35ee19e6b81d0a0987ba4f84195c95*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.02\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.79\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "685105\t0.17\tray::IDLE\n",
      "685116\t0.17\t\n",
      "685117\t0.16\tray::_ray_fit\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 2753, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: d2936572c77478d93c8b3b76e65aaff8782881bd01000000, name=_ray_fit, pid=685117, memory used=0.16GB) was running was 962.64GB / 1007.45GB (0.955518), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: dc8ac076d5d78b06962ca7eb8a35ee19e6b81d0a0987ba4f84195c95) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-dc8ac076d5d78b06962ca7eb8a35ee19e6b81d0a0987ba4f84195c95*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.02\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.79\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "685105\t0.17\tray::IDLE\n",
      "685116\t0.17\t\n",
      "685117\t0.16\tray::_ray_fit\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\tWarning: Exception caused CatBoost_BAG_L2 to fail during training... Skipping this model.\n",
      "\t\tTask was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: d2936572c77478d93c8b3b76e65aaff8782881bd01000000, name=_ray_fit, pid=685117, memory used=0.16GB) was running was 962.64GB / 1007.45GB (0.955518), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: dc8ac076d5d78b06962ca7eb8a35ee19e6b81d0a0987ba4f84195c95) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-dc8ac076d5d78b06962ca7eb8a35ee19e6b81d0a0987ba4f84195c95*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.02\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.79\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "685105\t0.17\tray::IDLE\n",
      "685116\t0.17\t\n",
      "685117\t0.16\tray::_ray_fit\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 2753, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: d2936572c77478d93c8b3b76e65aaff8782881bd01000000, name=_ray_fit, pid=685117, memory used=0.16GB) was running was 962.64GB / 1007.45GB (0.955518), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: dc8ac076d5d78b06962ca7eb8a35ee19e6b81d0a0987ba4f84195c95) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-dc8ac076d5d78b06962ca7eb8a35ee19e6b81d0a0987ba4f84195c95*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.02\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.79\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "685105\t0.17\tray::IDLE\n",
      "685116\t0.17\t\n",
      "685117\t0.16\tray::_ray_fit\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Fitting model: ExtraTreesMSE_BAG_L2 ... Training model for up to 1794.17s of the 1794.15s of remaining time.\n",
      "Fitting model: ExtraTreesMSE_BAG_L2 ... Training model for up to 1794.17s of the 1794.15s of remaining time.\n",
      "\t-0.1161\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.33s\t = Training   runtime\n",
      "\t0.09s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_BAG_L2 ... Training model for up to 1793.70s of the 1793.67s of remaining time.\n",
      "\t-0.1161\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.33s\t = Training   runtime\n",
      "\t0.09s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_BAG_L2 ... Training model for up to 1793.70s of the 1793.67s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.00%)\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.00%)\n",
      "2025-09-30 14:33:03,440\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-09-30 14:33:03,441\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-09-30 14:33:03,440\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-09-30 14:33:03,441\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "\tWarning: Exception caused NeuralNetFastAI_BAG_L2 to fail during training... Skipping this model.\n",
      "\t\tTask was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: f18a4ddc3ca6032763deb0aebeb7658c42ab17f801000000, name=_ray_fit, pid=689269, memory used=0.36GB) was running was 962.66GB / 1007.45GB (0.955539), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 38ef59bdc61752baf5e3a63a2669316b5ba78702766a7dd30a2dd741) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-38ef59bdc61752baf5e3a63a2669316b5ba78702766a7dd30a2dd741*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "521385\t0.89\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "522871\t0.79\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "689269\t0.36\tray::_ray_fit\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 2753, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: f18a4ddc3ca6032763deb0aebeb7658c42ab17f801000000, name=_ray_fit, pid=689269, memory used=0.36GB) was running was 962.66GB / 1007.45GB (0.955539), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 38ef59bdc61752baf5e3a63a2669316b5ba78702766a7dd30a2dd741) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-38ef59bdc61752baf5e3a63a2669316b5ba78702766a7dd30a2dd741*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "521385\t0.89\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "522871\t0.79\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "689269\t0.36\tray::_ray_fit\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\tWarning: Exception caused NeuralNetFastAI_BAG_L2 to fail during training... Skipping this model.\n",
      "\t\tTask was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: f18a4ddc3ca6032763deb0aebeb7658c42ab17f801000000, name=_ray_fit, pid=689269, memory used=0.36GB) was running was 962.66GB / 1007.45GB (0.955539), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 38ef59bdc61752baf5e3a63a2669316b5ba78702766a7dd30a2dd741) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-38ef59bdc61752baf5e3a63a2669316b5ba78702766a7dd30a2dd741*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "521385\t0.89\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "522871\t0.79\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "689269\t0.36\tray::_ray_fit\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 2753, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: f18a4ddc3ca6032763deb0aebeb7658c42ab17f801000000, name=_ray_fit, pid=689269, memory used=0.36GB) was running was 962.66GB / 1007.45GB (0.955539), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 38ef59bdc61752baf5e3a63a2669316b5ba78702766a7dd30a2dd741) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-38ef59bdc61752baf5e3a63a2669316b5ba78702766a7dd30a2dd741*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "521385\t0.89\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "522871\t0.79\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "689269\t0.36\tray::_ray_fit\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Fitting model: XGBoost_BAG_L2 ... Training model for up to 1783.15s of the 1783.12s of remaining time.\n",
      "Fitting model: XGBoost_BAG_L2 ... Training model for up to 1783.15s of the 1783.12s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.03%)\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.03%)\n",
      "\tWarning: Exception caused XGBoost_BAG_L2 to fail during training... Skipping this model.\n",
      "\t\tTask was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: 73be12fea789697dfed0b6c60fc85400be9115f101000000, name=_ray_fit, pid=692299, memory used=0.10GB) was running was 962.96GB / 1007.45GB (0.955833), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: a80a811495b9ca37ccfc6a0e566e4a877b8d078fd5255517349d48b9) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-a80a811495b9ca37ccfc6a0e566e4a877b8d078fd5255517349d48b9*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.03\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.79\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "692299\t0.10\tray::IDLE\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 2753, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: 73be12fea789697dfed0b6c60fc85400be9115f101000000, name=_ray_fit, pid=692299, memory used=0.10GB) was running was 962.96GB / 1007.45GB (0.955833), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: a80a811495b9ca37ccfc6a0e566e4a877b8d078fd5255517349d48b9) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-a80a811495b9ca37ccfc6a0e566e4a877b8d078fd5255517349d48b9*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.03\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.79\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "692299\t0.10\tray::IDLE\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\tWarning: Exception caused XGBoost_BAG_L2 to fail during training... Skipping this model.\n",
      "\t\tTask was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: 73be12fea789697dfed0b6c60fc85400be9115f101000000, name=_ray_fit, pid=692299, memory used=0.10GB) was running was 962.96GB / 1007.45GB (0.955833), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: a80a811495b9ca37ccfc6a0e566e4a877b8d078fd5255517349d48b9) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-a80a811495b9ca37ccfc6a0e566e4a877b8d078fd5255517349d48b9*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.03\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.79\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "692299\t0.10\tray::IDLE\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 2753, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: 73be12fea789697dfed0b6c60fc85400be9115f101000000, name=_ray_fit, pid=692299, memory used=0.10GB) was running was 962.96GB / 1007.45GB (0.955833), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: a80a811495b9ca37ccfc6a0e566e4a877b8d078fd5255517349d48b9) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-a80a811495b9ca37ccfc6a0e566e4a877b8d078fd5255517349d48b9*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.03\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.79\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "692299\t0.10\tray::IDLE\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Fitting model: NeuralNetTorch_BAG_L2 ... Training model for up to 1772.88s of the 1772.85s of remaining time.\n",
      "Fitting model: NeuralNetTorch_BAG_L2 ... Training model for up to 1772.88s of the 1772.85s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.00%)\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.00%)\n",
      "\tWarning: Exception caused NeuralNetTorch_BAG_L2 to fail during training... Skipping this model.\n",
      "\t\t\u001b[36mray::_ray_fit()\u001b[39m (pid=692461, ip=172.17.0.5)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 413, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 201, in _fit\n",
      "    train_dataset = self._generate_dataset(X, y, train_params=processor_kwargs, is_train=True)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 669, in _generate_dataset\n",
      "    dataset = self._process_train_data(\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 734, in _process_train_data\n",
      "    self.processor = create_preprocessor(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/utils/data_preprocessor.py\", line 40, in create_preprocessor\n",
      "    return ColumnTransformer(\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "TypeError: ColumnTransformer.__init__() got an unexpected keyword argument 'force_int_remainder_cols'\n",
      "\tWarning: Exception caused NeuralNetTorch_BAG_L2 to fail during training... Skipping this model.\n",
      "\t\t\u001b[36mray::_ray_fit()\u001b[39m (pid=692461, ip=172.17.0.5)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 413, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 201, in _fit\n",
      "    train_dataset = self._generate_dataset(X, y, train_params=processor_kwargs, is_train=True)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 669, in _generate_dataset\n",
      "    dataset = self._process_train_data(\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 734, in _process_train_data\n",
      "    self.processor = create_preprocessor(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/utils/data_preprocessor.py\", line 40, in create_preprocessor\n",
      "    return ColumnTransformer(\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "TypeError: ColumnTransformer.__init__() got an unexpected keyword argument 'force_int_remainder_cols'\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 2753, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 904, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(TypeError): \u001b[36mray::_ray_fit()\u001b[39m (pid=692461, ip=172.17.0.5)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 413, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 201, in _fit\n",
      "    train_dataset = self._generate_dataset(X, y, train_params=processor_kwargs, is_train=True)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 669, in _generate_dataset\n",
      "    dataset = self._process_train_data(\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 734, in _process_train_data\n",
      "    self.processor = create_preprocessor(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/utils/data_preprocessor.py\", line 40, in create_preprocessor\n",
      "    return ColumnTransformer(\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "TypeError: ColumnTransformer.__init__() got an unexpected keyword argument 'force_int_remainder_cols'\n",
      "Fitting model: LightGBMLarge_BAG_L2 ... Training model for up to 1769.07s of the 1769.04s of remaining time.\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 2753, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 904, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(TypeError): \u001b[36mray::_ray_fit()\u001b[39m (pid=692461, ip=172.17.0.5)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 413, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 201, in _fit\n",
      "    train_dataset = self._generate_dataset(X, y, train_params=processor_kwargs, is_train=True)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 669, in _generate_dataset\n",
      "    dataset = self._process_train_data(\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 734, in _process_train_data\n",
      "    self.processor = create_preprocessor(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/utils/data_preprocessor.py\", line 40, in create_preprocessor\n",
      "    return ColumnTransformer(\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "TypeError: ColumnTransformer.__init__() got an unexpected keyword argument 'force_int_remainder_cols'\n",
      "Fitting model: LightGBMLarge_BAG_L2 ... Training model for up to 1769.07s of the 1769.04s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.06%)\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.06%)\n",
      "\tWarning: Exception caused LightGBMLarge_BAG_L2 to fail during training... Skipping this model.\n",
      "\t\tTask was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: 6c015c02d0df1a45e219c8d380e83386bf32650301000000, name=_ray_fit, pid=693328, memory used=0.08GB) was running was 963.78GB / 1007.45GB (0.956655), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 91180d5a7a9853f07f225610faf2c92861dff0c91082dc3ae323115d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-91180d5a7a9853f07f225610faf2c92861dff0c91082dc3ae323115d*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "521385\t0.89\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "522871\t0.79\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "692461\t0.33\tray::IDLE\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 2753, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: 6c015c02d0df1a45e219c8d380e83386bf32650301000000, name=_ray_fit, pid=693328, memory used=0.08GB) was running was 963.78GB / 1007.45GB (0.956655), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 91180d5a7a9853f07f225610faf2c92861dff0c91082dc3ae323115d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-91180d5a7a9853f07f225610faf2c92861dff0c91082dc3ae323115d*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "521385\t0.89\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "522871\t0.79\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "692461\t0.33\tray::IDLE\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\tWarning: Exception caused LightGBMLarge_BAG_L2 to fail during training... Skipping this model.\n",
      "\t\tTask was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: 6c015c02d0df1a45e219c8d380e83386bf32650301000000, name=_ray_fit, pid=693328, memory used=0.08GB) was running was 963.78GB / 1007.45GB (0.956655), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 91180d5a7a9853f07f225610faf2c92861dff0c91082dc3ae323115d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-91180d5a7a9853f07f225610faf2c92861dff0c91082dc3ae323115d*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "521385\t0.89\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "522871\t0.79\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "692461\t0.33\tray::IDLE\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 2753, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: 6c015c02d0df1a45e219c8d380e83386bf32650301000000, name=_ray_fit, pid=693328, memory used=0.08GB) was running was 963.78GB / 1007.45GB (0.956655), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 91180d5a7a9853f07f225610faf2c92861dff0c91082dc3ae323115d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-91180d5a7a9853f07f225610faf2c92861dff0c91082dc3ae323115d*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "521385\t0.89\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "522871\t0.79\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "692461\t0.33\tray::IDLE\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "2025-09-30 14:33:23,893\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-09-30 14:33:23,894\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-09-30 14:33:23,895\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "Fitting model: CatBoost_r177_BAG_L2 ... Training model for up to 1768.40s of the 1768.38s of remaining time.\n",
      "2025-09-30 14:33:23,893\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-09-30 14:33:23,894\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-09-30 14:33:23,895\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "Fitting model: CatBoost_r177_BAG_L2 ... Training model for up to 1768.40s of the 1768.38s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.03%)\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.03%)\n",
      "\tWarning: Exception caused CatBoost_r177_BAG_L2 to fail during training... Skipping this model.\n",
      "\t\tTask was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: f90cba7e5cda1d6713d81d081c22635bc7c6dd0201000000, name=_ray_fit, pid=694875, memory used=0.25GB) was running was 963.30GB / 1007.45GB (0.956176), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 66d48e49384db6560e595bb33947ae5c7651202c75b74edb58f4de7f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-66d48e49384db6560e595bb33947ae5c7651202c75b74edb58f4de7f*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.03\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.79\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "694875\t0.25\tray::_ray_fit\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 2753, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: f90cba7e5cda1d6713d81d081c22635bc7c6dd0201000000, name=_ray_fit, pid=694875, memory used=0.25GB) was running was 963.30GB / 1007.45GB (0.956176), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 66d48e49384db6560e595bb33947ae5c7651202c75b74edb58f4de7f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-66d48e49384db6560e595bb33947ae5c7651202c75b74edb58f4de7f*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.03\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.79\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "694875\t0.25\tray::_ray_fit\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\tWarning: Exception caused CatBoost_r177_BAG_L2 to fail during training... Skipping this model.\n",
      "\t\tTask was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: f90cba7e5cda1d6713d81d081c22635bc7c6dd0201000000, name=_ray_fit, pid=694875, memory used=0.25GB) was running was 963.30GB / 1007.45GB (0.956176), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 66d48e49384db6560e595bb33947ae5c7651202c75b74edb58f4de7f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-66d48e49384db6560e595bb33947ae5c7651202c75b74edb58f4de7f*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.03\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.79\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "694875\t0.25\tray::_ray_fit\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 2753, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: f90cba7e5cda1d6713d81d081c22635bc7c6dd0201000000, name=_ray_fit, pid=694875, memory used=0.25GB) was running was 963.30GB / 1007.45GB (0.956176), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 66d48e49384db6560e595bb33947ae5c7651202c75b74edb58f4de7f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-66d48e49384db6560e595bb33947ae5c7651202c75b74edb58f4de7f*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.03\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.79\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "694875\t0.25\tray::_ray_fit\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "2025-09-30 14:33:27,790\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-09-30 14:33:27,791\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-09-30 14:33:27,792\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-09-30 14:33:27,793\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-09-30 14:33:27,794\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-09-30 14:33:27,794\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-09-30 14:33:27,795\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "Fitting model: NeuralNetTorch_r79_BAG_L2 ... Training model for up to 1764.50s of the 1764.48s of remaining time.\n",
      "2025-09-30 14:33:27,790\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-09-30 14:33:27,791\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-09-30 14:33:27,792\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-09-30 14:33:27,793\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-09-30 14:33:27,794\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-09-30 14:33:27,794\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-09-30 14:33:27,795\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "Fitting model: NeuralNetTorch_r79_BAG_L2 ... Training model for up to 1764.50s of the 1764.48s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.00%)\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.00%)\n",
      "\tWarning: Exception caused NeuralNetTorch_r79_BAG_L2 to fail during training... Skipping this model.\n",
      "\t\t\u001b[36mray::_ray_fit()\u001b[39m (pid=696665, ip=172.17.0.5)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 413, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 201, in _fit\n",
      "    train_dataset = self._generate_dataset(X, y, train_params=processor_kwargs, is_train=True)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 669, in _generate_dataset\n",
      "    dataset = self._process_train_data(\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 734, in _process_train_data\n",
      "    self.processor = create_preprocessor(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/utils/data_preprocessor.py\", line 40, in create_preprocessor\n",
      "    return ColumnTransformer(\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "TypeError: ColumnTransformer.__init__() got an unexpected keyword argument 'force_int_remainder_cols'\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 2753, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 904, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(TypeError): \u001b[36mray::_ray_fit()\u001b[39m (pid=696665, ip=172.17.0.5)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 413, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 201, in _fit\n",
      "    train_dataset = self._generate_dataset(X, y, train_params=processor_kwargs, is_train=True)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 669, in _generate_dataset\n",
      "    dataset = self._process_train_data(\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 734, in _process_train_data\n",
      "    self.processor = create_preprocessor(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/utils/data_preprocessor.py\", line 40, in create_preprocessor\n",
      "    return ColumnTransformer(\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "TypeError: ColumnTransformer.__init__() got an unexpected keyword argument 'force_int_remainder_cols'\n",
      "\tWarning: Exception caused NeuralNetTorch_r79_BAG_L2 to fail during training... Skipping this model.\n",
      "\t\t\u001b[36mray::_ray_fit()\u001b[39m (pid=696665, ip=172.17.0.5)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 413, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 201, in _fit\n",
      "    train_dataset = self._generate_dataset(X, y, train_params=processor_kwargs, is_train=True)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 669, in _generate_dataset\n",
      "    dataset = self._process_train_data(\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 734, in _process_train_data\n",
      "    self.processor = create_preprocessor(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/utils/data_preprocessor.py\", line 40, in create_preprocessor\n",
      "    return ColumnTransformer(\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "TypeError: ColumnTransformer.__init__() got an unexpected keyword argument 'force_int_remainder_cols'\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 2753, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 904, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(TypeError): \u001b[36mray::_ray_fit()\u001b[39m (pid=696665, ip=172.17.0.5)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 413, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 201, in _fit\n",
      "    train_dataset = self._generate_dataset(X, y, train_params=processor_kwargs, is_train=True)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 669, in _generate_dataset\n",
      "    dataset = self._process_train_data(\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 734, in _process_train_data\n",
      "    self.processor = create_preprocessor(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/utils/data_preprocessor.py\", line 40, in create_preprocessor\n",
      "    return ColumnTransformer(\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "TypeError: ColumnTransformer.__init__() got an unexpected keyword argument 'force_int_remainder_cols'\n",
      "Fitting model: LightGBM_r131_BAG_L2 ... Training model for up to 1761.15s of the 1761.12s of remaining time.\n",
      "Fitting model: LightGBM_r131_BAG_L2 ... Training model for up to 1761.15s of the 1761.12s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.02%)\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.02%)\n",
      "\tWarning: Exception caused LightGBM_r131_BAG_L2 to fail during training... Skipping this model.\n",
      "\t\tTask was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: 5bf5903488bc538e30def2020e0b83cd5428c07601000000, name=_ray_fit, pid=696755, memory used=0.10GB) was running was 963.76GB / 1007.45GB (0.95663), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: a8d41f33f9ab26b4467dc85d7499fb97dabd82824ff1221f0bfc9099) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-a8d41f33f9ab26b4467dc85d7499fb97dabd82824ff1221f0bfc9099*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "521385\t0.89\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "522871\t0.79\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "696755\t0.10\tray::IDLE\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 2753, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: 5bf5903488bc538e30def2020e0b83cd5428c07601000000, name=_ray_fit, pid=696755, memory used=0.10GB) was running was 963.76GB / 1007.45GB (0.95663), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: a8d41f33f9ab26b4467dc85d7499fb97dabd82824ff1221f0bfc9099) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-a8d41f33f9ab26b4467dc85d7499fb97dabd82824ff1221f0bfc9099*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "521385\t0.89\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "522871\t0.79\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "696755\t0.10\tray::IDLE\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\tWarning: Exception caused LightGBM_r131_BAG_L2 to fail during training... Skipping this model.\n",
      "\t\tTask was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: 5bf5903488bc538e30def2020e0b83cd5428c07601000000, name=_ray_fit, pid=696755, memory used=0.10GB) was running was 963.76GB / 1007.45GB (0.95663), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: a8d41f33f9ab26b4467dc85d7499fb97dabd82824ff1221f0bfc9099) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-a8d41f33f9ab26b4467dc85d7499fb97dabd82824ff1221f0bfc9099*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "521385\t0.89\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "522871\t0.79\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "696755\t0.10\tray::IDLE\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 2753, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: 5bf5903488bc538e30def2020e0b83cd5428c07601000000, name=_ray_fit, pid=696755, memory used=0.10GB) was running was 963.76GB / 1007.45GB (0.95663), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: a8d41f33f9ab26b4467dc85d7499fb97dabd82824ff1221f0bfc9099) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-a8d41f33f9ab26b4467dc85d7499fb97dabd82824ff1221f0bfc9099*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "521385\t0.89\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "522871\t0.79\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "696755\t0.10\tray::IDLE\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "2025-09-30 14:33:31,763\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-09-30 14:33:31,764\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-09-30 14:33:31,765\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "Fitting model: NeuralNetFastAI_r191_BAG_L2 ... Training model for up to 1760.53s of the 1760.51s of remaining time.\n",
      "2025-09-30 14:33:31,763\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-09-30 14:33:31,764\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-09-30 14:33:31,765\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "Fitting model: NeuralNetFastAI_r191_BAG_L2 ... Training model for up to 1760.53s of the 1760.51s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.00%)\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.00%)\n",
      "\tWarning: Exception caused NeuralNetFastAI_r191_BAG_L2 to fail during training... Skipping this model.\n",
      "\t\tTask was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: 9971949b818631b0792439a3d1779901d603864701000000, name=_ray_fit, pid=698286, memory used=0.10GB) was running was 963.39GB / 1007.45GB (0.956261), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 54de7676c16fe8c7237bc74c96cbaf95df5b91fa4948e0006679e1d8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-54de7676c16fe8c7237bc74c96cbaf95df5b91fa4948e0006679e1d8*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.03\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.79\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "698286\t0.10\tray::IDLE\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 2753, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: 9971949b818631b0792439a3d1779901d603864701000000, name=_ray_fit, pid=698286, memory used=0.10GB) was running was 963.39GB / 1007.45GB (0.956261), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 54de7676c16fe8c7237bc74c96cbaf95df5b91fa4948e0006679e1d8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-54de7676c16fe8c7237bc74c96cbaf95df5b91fa4948e0006679e1d8*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.03\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.79\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "698286\t0.10\tray::IDLE\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\tWarning: Exception caused NeuralNetFastAI_r191_BAG_L2 to fail during training... Skipping this model.\n",
      "\t\tTask was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: 9971949b818631b0792439a3d1779901d603864701000000, name=_ray_fit, pid=698286, memory used=0.10GB) was running was 963.39GB / 1007.45GB (0.956261), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 54de7676c16fe8c7237bc74c96cbaf95df5b91fa4948e0006679e1d8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-54de7676c16fe8c7237bc74c96cbaf95df5b91fa4948e0006679e1d8*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.03\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.79\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "698286\t0.10\tray::IDLE\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 2753, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: 9971949b818631b0792439a3d1779901d603864701000000, name=_ray_fit, pid=698286, memory used=0.10GB) was running was 963.39GB / 1007.45GB (0.956261), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 54de7676c16fe8c7237bc74c96cbaf95df5b91fa4948e0006679e1d8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-54de7676c16fe8c7237bc74c96cbaf95df5b91fa4948e0006679e1d8*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.03\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.79\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "698286\t0.10\tray::IDLE\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "2025-09-30 14:33:32,265\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-09-30 14:33:32,266\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-09-30 14:33:32,266\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-09-30 14:33:32,267\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-09-30 14:33:32,268\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-09-30 14:33:32,269\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-09-30 14:33:32,270\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "Fitting model: CatBoost_r9_BAG_L2 ... Training model for up to 1760.03s of the 1760.00s of remaining time.\n",
      "2025-09-30 14:33:32,265\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-09-30 14:33:32,266\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-09-30 14:33:32,266\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-09-30 14:33:32,267\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-09-30 14:33:32,268\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-09-30 14:33:32,269\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-09-30 14:33:32,270\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "Fitting model: CatBoost_r9_BAG_L2 ... Training model for up to 1760.03s of the 1760.00s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.12%)\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.12%)\n",
      "\tWarning: Exception caused CatBoost_r9_BAG_L2 to fail during training... Skipping this model.\n",
      "\t\tTask was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: 5219de44682d29430dfdbae6a667cfa044d7d18e01000000, name=_ray_fit, pid=700276, memory used=0.24GB) was running was 964.01GB / 1007.45GB (0.956882), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 42314c90e9162e5daae87364cd33e8198a183ce11569f0c20612bc31) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-42314c90e9162e5daae87364cd33e8198a183ce11569f0c20612bc31*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.03\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.79\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "700276\t0.24\tray::_ray_fit\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 2753, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: 5219de44682d29430dfdbae6a667cfa044d7d18e01000000, name=_ray_fit, pid=700276, memory used=0.24GB) was running was 964.01GB / 1007.45GB (0.956882), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 42314c90e9162e5daae87364cd33e8198a183ce11569f0c20612bc31) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-42314c90e9162e5daae87364cd33e8198a183ce11569f0c20612bc31*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.03\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.79\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "700276\t0.24\tray::_ray_fit\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\tWarning: Exception caused CatBoost_r9_BAG_L2 to fail during training... Skipping this model.\n",
      "\t\tTask was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: 5219de44682d29430dfdbae6a667cfa044d7d18e01000000, name=_ray_fit, pid=700276, memory used=0.24GB) was running was 964.01GB / 1007.45GB (0.956882), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 42314c90e9162e5daae87364cd33e8198a183ce11569f0c20612bc31) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-42314c90e9162e5daae87364cd33e8198a183ce11569f0c20612bc31*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.03\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.79\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "700276\t0.24\tray::_ray_fit\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 2753, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: 5219de44682d29430dfdbae6a667cfa044d7d18e01000000, name=_ray_fit, pid=700276, memory used=0.24GB) was running was 964.01GB / 1007.45GB (0.956882), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 42314c90e9162e5daae87364cd33e8198a183ce11569f0c20612bc31) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-42314c90e9162e5daae87364cd33e8198a183ce11569f0c20612bc31*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.03\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.79\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "700276\t0.24\tray::_ray_fit\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Fitting model: LightGBM_r96_BAG_L2 ... Training model for up to 1754.07s of the 1754.04s of remaining time.\n",
      "Fitting model: LightGBM_r96_BAG_L2 ... Training model for up to 1754.07s of the 1754.04s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.01%)\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.01%)\n",
      "\tWarning: Exception caused LightGBM_r96_BAG_L2 to fail during training... Skipping this model.\n",
      "\t\tTask was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: d9898f1b1c8473154511097a4c908af6d2e9dcd601000000, name=_ray_fit, pid=702638, memory used=0.12GB) was running was 964.39GB / 1007.45GB (0.957254), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 177a610710e4608010e0c7fe96872daa7a8c2b940567d7f975e995e8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-177a610710e4608010e0c7fe96872daa7a8c2b940567d7f975e995e8*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.03\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.79\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "702638\t0.12\tray::IDLE\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 2753, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: d9898f1b1c8473154511097a4c908af6d2e9dcd601000000, name=_ray_fit, pid=702638, memory used=0.12GB) was running was 964.39GB / 1007.45GB (0.957254), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 177a610710e4608010e0c7fe96872daa7a8c2b940567d7f975e995e8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-177a610710e4608010e0c7fe96872daa7a8c2b940567d7f975e995e8*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.03\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.79\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "702638\t0.12\tray::IDLE\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\tWarning: Exception caused LightGBM_r96_BAG_L2 to fail during training... Skipping this model.\n",
      "\t\tTask was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: d9898f1b1c8473154511097a4c908af6d2e9dcd601000000, name=_ray_fit, pid=702638, memory used=0.12GB) was running was 964.39GB / 1007.45GB (0.957254), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 177a610710e4608010e0c7fe96872daa7a8c2b940567d7f975e995e8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-177a610710e4608010e0c7fe96872daa7a8c2b940567d7f975e995e8*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.03\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.79\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "702638\t0.12\tray::IDLE\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 2753, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: d9898f1b1c8473154511097a4c908af6d2e9dcd601000000, name=_ray_fit, pid=702638, memory used=0.12GB) was running was 964.39GB / 1007.45GB (0.957254), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 177a610710e4608010e0c7fe96872daa7a8c2b940567d7f975e995e8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-177a610710e4608010e0c7fe96872daa7a8c2b940567d7f975e995e8*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.03\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.79\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "702638\t0.12\tray::IDLE\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Fitting model: NeuralNetTorch_r22_BAG_L2 ... Training model for up to 1748.75s of the 1748.72s of remaining time.\n",
      "Fitting model: NeuralNetTorch_r22_BAG_L2 ... Training model for up to 1748.75s of the 1748.72s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.00%)\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.00%)\n",
      "\tWarning: Exception caused NeuralNetTorch_r22_BAG_L2 to fail during training... Skipping this model.\n",
      "\t\tTask was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: edacd8291fd0232cb479f5cf1784afd06f50681c01000000, name=_ray_fit, pid=702637, memory used=0.10GB) was running was 964.13GB / 1007.45GB (0.956996), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4a9c7f1003f3454c488c0ad67d75952ed048d9bd3bc62bdc0a0252a4) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-4a9c7f1003f3454c488c0ad67d75952ed048d9bd3bc62bdc0a0252a4*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.03\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.79\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "702637\t0.10\tray::IDLE\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 2753, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: edacd8291fd0232cb479f5cf1784afd06f50681c01000000, name=_ray_fit, pid=702637, memory used=0.10GB) was running was 964.13GB / 1007.45GB (0.956996), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4a9c7f1003f3454c488c0ad67d75952ed048d9bd3bc62bdc0a0252a4) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-4a9c7f1003f3454c488c0ad67d75952ed048d9bd3bc62bdc0a0252a4*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.03\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.79\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "702637\t0.10\tray::IDLE\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\tWarning: Exception caused NeuralNetTorch_r22_BAG_L2 to fail during training... Skipping this model.\n",
      "\t\tTask was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: edacd8291fd0232cb479f5cf1784afd06f50681c01000000, name=_ray_fit, pid=702637, memory used=0.10GB) was running was 964.13GB / 1007.45GB (0.956996), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4a9c7f1003f3454c488c0ad67d75952ed048d9bd3bc62bdc0a0252a4) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-4a9c7f1003f3454c488c0ad67d75952ed048d9bd3bc62bdc0a0252a4*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.03\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.79\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "702637\t0.10\tray::IDLE\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 2753, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: edacd8291fd0232cb479f5cf1784afd06f50681c01000000, name=_ray_fit, pid=702637, memory used=0.10GB) was running was 964.13GB / 1007.45GB (0.956996), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4a9c7f1003f3454c488c0ad67d75952ed048d9bd3bc62bdc0a0252a4) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-4a9c7f1003f3454c488c0ad67d75952ed048d9bd3bc62bdc0a0252a4*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.03\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.79\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "702637\t0.10\tray::IDLE\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Fitting model: XGBoost_r33_BAG_L2 ... Training model for up to 1748.24s of the 1748.21s of remaining time.\n",
      "Fitting model: XGBoost_r33_BAG_L2 ... Training model for up to 1748.24s of the 1748.21s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.24%)\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.24%)\n",
      "\tWarning: Exception caused XGBoost_r33_BAG_L2 to fail during training... Skipping this model.\n",
      "\t\tTask was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: 621f5d4007246888fd7746abca17be1780f59aba01000000, name=_ray_fit, pid=705201, memory used=0.16GB) was running was 964.31GB / 1007.45GB (0.957174), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e690b91c3c6c18f5c8edde692f9300c551b6ab7b613d7bcd2b881e6e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-e690b91c3c6c18f5c8edde692f9300c551b6ab7b613d7bcd2b881e6e*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.03\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.79\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "705201\t0.16\tray::_ray_fit\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 2753, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: 621f5d4007246888fd7746abca17be1780f59aba01000000, name=_ray_fit, pid=705201, memory used=0.16GB) was running was 964.31GB / 1007.45GB (0.957174), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e690b91c3c6c18f5c8edde692f9300c551b6ab7b613d7bcd2b881e6e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-e690b91c3c6c18f5c8edde692f9300c551b6ab7b613d7bcd2b881e6e*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.03\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.79\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "705201\t0.16\tray::_ray_fit\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\tWarning: Exception caused XGBoost_r33_BAG_L2 to fail during training... Skipping this model.\n",
      "\t\tTask was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: 621f5d4007246888fd7746abca17be1780f59aba01000000, name=_ray_fit, pid=705201, memory used=0.16GB) was running was 964.31GB / 1007.45GB (0.957174), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e690b91c3c6c18f5c8edde692f9300c551b6ab7b613d7bcd2b881e6e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-e690b91c3c6c18f5c8edde692f9300c551b6ab7b613d7bcd2b881e6e*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.03\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.79\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "705201\t0.16\tray::_ray_fit\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 2753, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: 621f5d4007246888fd7746abca17be1780f59aba01000000, name=_ray_fit, pid=705201, memory used=0.16GB) was running was 964.31GB / 1007.45GB (0.957174), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e690b91c3c6c18f5c8edde692f9300c551b6ab7b613d7bcd2b881e6e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-e690b91c3c6c18f5c8edde692f9300c551b6ab7b613d7bcd2b881e6e*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.03\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.79\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "705201\t0.16\tray::_ray_fit\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Fitting model: ExtraTrees_r42_BAG_L2 ... Training model for up to 1736.35s of the 1736.32s of remaining time.\n",
      "Fitting model: ExtraTrees_r42_BAG_L2 ... Training model for up to 1736.35s of the 1736.32s of remaining time.\n",
      "\t-0.115\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.3s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: CatBoost_r137_BAG_L2 ... Training model for up to 1735.94s of the 1735.92s of remaining time.\n",
      "\t-0.115\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.3s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: CatBoost_r137_BAG_L2 ... Training model for up to 1735.94s of the 1735.92s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.01%)\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.01%)\n",
      "\tWarning: Exception caused CatBoost_r137_BAG_L2 to fail during training... Skipping this model.\n",
      "\t\tTask was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: db80498fbe7e3986417514762cd7cbc0d978caed01000000, name=_ray_fit, pid=705655, memory used=0.16GB) was running was 965.13GB / 1007.45GB (0.957992), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e6950539c6b0cf042071d38e641577771b4f20b5725a92a92ad1d8de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-e6950539c6b0cf042071d38e641577771b4f20b5725a92a92ad1d8de*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.03\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.79\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "705653\t0.17\tray::IDLE\n",
      "705654\t0.16\t\n",
      "705655\t0.16\tray::_ray_fit\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 2753, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: db80498fbe7e3986417514762cd7cbc0d978caed01000000, name=_ray_fit, pid=705655, memory used=0.16GB) was running was 965.13GB / 1007.45GB (0.957992), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e6950539c6b0cf042071d38e641577771b4f20b5725a92a92ad1d8de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-e6950539c6b0cf042071d38e641577771b4f20b5725a92a92ad1d8de*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.03\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.79\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "705653\t0.17\tray::IDLE\n",
      "705654\t0.16\t\n",
      "705655\t0.16\tray::_ray_fit\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\tWarning: Exception caused CatBoost_r137_BAG_L2 to fail during training... Skipping this model.\n",
      "\t\tTask was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: db80498fbe7e3986417514762cd7cbc0d978caed01000000, name=_ray_fit, pid=705655, memory used=0.16GB) was running was 965.13GB / 1007.45GB (0.957992), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e6950539c6b0cf042071d38e641577771b4f20b5725a92a92ad1d8de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-e6950539c6b0cf042071d38e641577771b4f20b5725a92a92ad1d8de*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.03\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.79\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "705653\t0.17\tray::IDLE\n",
      "705654\t0.16\t\n",
      "705655\t0.16\tray::_ray_fit\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 2753, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: db80498fbe7e3986417514762cd7cbc0d978caed01000000, name=_ray_fit, pid=705655, memory used=0.16GB) was running was 965.13GB / 1007.45GB (0.957992), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e6950539c6b0cf042071d38e641577771b4f20b5725a92a92ad1d8de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-e6950539c6b0cf042071d38e641577771b4f20b5725a92a92ad1d8de*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.03\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.79\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "705653\t0.17\tray::IDLE\n",
      "705654\t0.16\t\n",
      "705655\t0.16\tray::_ray_fit\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Fitting model: NeuralNetFastAI_r102_BAG_L2 ... Training model for up to 1732.79s of the 1732.76s of remaining time.\n",
      "Fitting model: NeuralNetFastAI_r102_BAG_L2 ... Training model for up to 1732.79s of the 1732.76s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.00%)\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.00%)\n",
      "\tWarning: Exception caused NeuralNetFastAI_r102_BAG_L2 to fail during training... Skipping this model.\n",
      "\t\tTask was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: a336acc9e340716fbb65b7a70b557b05209e5ab501000000, name=_ray_fit, pid=710022, memory used=0.11GB) was running was 965.07GB / 1007.45GB (0.957934), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4a516b21ed63c5937e733c54e4486a34f6bcdeb8ed25554461358d7a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-4a516b21ed63c5937e733c54e4486a34f6bcdeb8ed25554461358d7a*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.04\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.80\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "710022\t0.11\tray::IDLE\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 2753, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: a336acc9e340716fbb65b7a70b557b05209e5ab501000000, name=_ray_fit, pid=710022, memory used=0.11GB) was running was 965.07GB / 1007.45GB (0.957934), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4a516b21ed63c5937e733c54e4486a34f6bcdeb8ed25554461358d7a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-4a516b21ed63c5937e733c54e4486a34f6bcdeb8ed25554461358d7a*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.04\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.80\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "710022\t0.11\tray::IDLE\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\tWarning: Exception caused NeuralNetFastAI_r102_BAG_L2 to fail during training... Skipping this model.\n",
      "\t\tTask was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: a336acc9e340716fbb65b7a70b557b05209e5ab501000000, name=_ray_fit, pid=710022, memory used=0.11GB) was running was 965.07GB / 1007.45GB (0.957934), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4a516b21ed63c5937e733c54e4486a34f6bcdeb8ed25554461358d7a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-4a516b21ed63c5937e733c54e4486a34f6bcdeb8ed25554461358d7a*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.04\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.80\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "710022\t0.11\tray::IDLE\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 2753, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: a336acc9e340716fbb65b7a70b557b05209e5ab501000000, name=_ray_fit, pid=710022, memory used=0.11GB) was running was 965.07GB / 1007.45GB (0.957934), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4a516b21ed63c5937e733c54e4486a34f6bcdeb8ed25554461358d7a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-4a516b21ed63c5937e733c54e4486a34f6bcdeb8ed25554461358d7a*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.04\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.80\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "710022\t0.11\tray::IDLE\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Fitting model: CatBoost_r13_BAG_L2 ... Training model for up to 1721.84s of the 1721.81s of remaining time.\n",
      "Fitting model: CatBoost_r13_BAG_L2 ... Training model for up to 1721.84s of the 1721.81s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.12%)\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.12%)\n",
      "\tWarning: Exception caused CatBoost_r13_BAG_L2 to fail during training... Skipping this model.\n",
      "\t\tTask was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: 9361de903b95f99b24290abe8f370f1c1ebb61f901000000, name=_ray_fit, pid=710021, memory used=0.10GB) was running was 965.10GB / 1007.45GB (0.957966), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0bc5b5cee75e2cbb8adddc106740df0ea10b6300dfef8389cb7c20a2) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-0bc5b5cee75e2cbb8adddc106740df0ea10b6300dfef8389cb7c20a2*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.04\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.80\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "710021\t0.10\tray::IDLE\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 2753, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: 9361de903b95f99b24290abe8f370f1c1ebb61f901000000, name=_ray_fit, pid=710021, memory used=0.10GB) was running was 965.10GB / 1007.45GB (0.957966), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0bc5b5cee75e2cbb8adddc106740df0ea10b6300dfef8389cb7c20a2) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-0bc5b5cee75e2cbb8adddc106740df0ea10b6300dfef8389cb7c20a2*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.04\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.80\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "710021\t0.10\tray::IDLE\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\tWarning: Exception caused CatBoost_r13_BAG_L2 to fail during training... Skipping this model.\n",
      "\t\tTask was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: 9361de903b95f99b24290abe8f370f1c1ebb61f901000000, name=_ray_fit, pid=710021, memory used=0.10GB) was running was 965.10GB / 1007.45GB (0.957966), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0bc5b5cee75e2cbb8adddc106740df0ea10b6300dfef8389cb7c20a2) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-0bc5b5cee75e2cbb8adddc106740df0ea10b6300dfef8389cb7c20a2*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.04\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.80\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "710021\t0.10\tray::IDLE\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 2753, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: 9361de903b95f99b24290abe8f370f1c1ebb61f901000000, name=_ray_fit, pid=710021, memory used=0.10GB) was running was 965.10GB / 1007.45GB (0.957966), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0bc5b5cee75e2cbb8adddc106740df0ea10b6300dfef8389cb7c20a2) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-0bc5b5cee75e2cbb8adddc106740df0ea10b6300dfef8389cb7c20a2*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.04\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.80\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "710021\t0.10\tray::IDLE\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Fitting model: RandomForest_r195_BAG_L2 ... Training model for up to 1721.28s of the 1721.26s of remaining time.\n",
      "Fitting model: RandomForest_r195_BAG_L2 ... Training model for up to 1721.28s of the 1721.26s of remaining time.\n",
      "\t-0.13\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.44s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBM_r188_BAG_L2 ... Training model for up to 1720.71s of the 1720.69s of remaining time.\n",
      "\t-0.13\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.44s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBM_r188_BAG_L2 ... Training model for up to 1720.71s of the 1720.69s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.06%)\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.06%)\n",
      "\tWarning: Exception caused LightGBM_r188_BAG_L2 to fail during training... Skipping this model.\n",
      "\t\tTask was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: bebbda9d190593ad25ad17fb6a9fdefe5e7ae58201000000, name=_ray_fit, pid=712137, memory used=0.11GB) was running was 965.57GB / 1007.45GB (0.958424), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 932a6e5af50774716650888f1906e0d79d231cdbf923c63ed8f21487) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-932a6e5af50774716650888f1906e0d79d231cdbf923c63ed8f21487*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.05\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.80\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "712137\t0.11\tray::IDLE\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 2753, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: bebbda9d190593ad25ad17fb6a9fdefe5e7ae58201000000, name=_ray_fit, pid=712137, memory used=0.11GB) was running was 965.57GB / 1007.45GB (0.958424), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 932a6e5af50774716650888f1906e0d79d231cdbf923c63ed8f21487) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-932a6e5af50774716650888f1906e0d79d231cdbf923c63ed8f21487*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.05\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.80\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "712137\t0.11\tray::IDLE\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\tWarning: Exception caused LightGBM_r188_BAG_L2 to fail during training... Skipping this model.\n",
      "\t\tTask was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: bebbda9d190593ad25ad17fb6a9fdefe5e7ae58201000000, name=_ray_fit, pid=712137, memory used=0.11GB) was running was 965.57GB / 1007.45GB (0.958424), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 932a6e5af50774716650888f1906e0d79d231cdbf923c63ed8f21487) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-932a6e5af50774716650888f1906e0d79d231cdbf923c63ed8f21487*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.05\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.80\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "712137\t0.11\tray::IDLE\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 2753, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: bebbda9d190593ad25ad17fb6a9fdefe5e7ae58201000000, name=_ray_fit, pid=712137, memory used=0.11GB) was running was 965.57GB / 1007.45GB (0.958424), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 932a6e5af50774716650888f1906e0d79d231cdbf923c63ed8f21487) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-932a6e5af50774716650888f1906e0d79d231cdbf923c63ed8f21487*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.05\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.80\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "712137\t0.11\tray::IDLE\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Fitting model: NeuralNetFastAI_r145_BAG_L2 ... Training model for up to 1711.91s of the 1711.88s of remaining time.\n",
      "Fitting model: NeuralNetFastAI_r145_BAG_L2 ... Training model for up to 1711.91s of the 1711.88s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.00%)\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.00%)\n",
      "\tWarning: Exception caused NeuralNetFastAI_r145_BAG_L2 to fail during training... Skipping this model.\n",
      "\t\tTask was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: a75cc61af3a747c6f9ce74bf6eda20c561233e6701000000, name=_ray_fit, pid=714890, memory used=0.39GB) was running was 966.67GB / 1007.45GB (0.959516), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: a6a06ad944cc5499a375a2f1f2f697d4b31bddb98f6eee7d73d689d6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-a6a06ad944cc5499a375a2f1f2f697d4b31bddb98f6eee7d73d689d6*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "521385\t0.91\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "522871\t0.80\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "714890\t0.39\tray::_ray_fit\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 2753, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: a75cc61af3a747c6f9ce74bf6eda20c561233e6701000000, name=_ray_fit, pid=714890, memory used=0.39GB) was running was 966.67GB / 1007.45GB (0.959516), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: a6a06ad944cc5499a375a2f1f2f697d4b31bddb98f6eee7d73d689d6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-a6a06ad944cc5499a375a2f1f2f697d4b31bddb98f6eee7d73d689d6*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "521385\t0.91\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "522871\t0.80\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "714890\t0.39\tray::_ray_fit\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\tWarning: Exception caused NeuralNetFastAI_r145_BAG_L2 to fail during training... Skipping this model.\n",
      "\t\tTask was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: a75cc61af3a747c6f9ce74bf6eda20c561233e6701000000, name=_ray_fit, pid=714890, memory used=0.39GB) was running was 966.67GB / 1007.45GB (0.959516), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: a6a06ad944cc5499a375a2f1f2f697d4b31bddb98f6eee7d73d689d6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-a6a06ad944cc5499a375a2f1f2f697d4b31bddb98f6eee7d73d689d6*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "521385\t0.91\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "522871\t0.80\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "714890\t0.39\tray::_ray_fit\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 2753, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: a75cc61af3a747c6f9ce74bf6eda20c561233e6701000000, name=_ray_fit, pid=714890, memory used=0.39GB) was running was 966.67GB / 1007.45GB (0.959516), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: a6a06ad944cc5499a375a2f1f2f697d4b31bddb98f6eee7d73d689d6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-a6a06ad944cc5499a375a2f1f2f697d4b31bddb98f6eee7d73d689d6*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "521385\t0.91\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "522871\t0.80\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "714890\t0.39\tray::_ray_fit\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Fitting model: XGBoost_r89_BAG_L2 ... Training model for up to 1696.49s of the 1696.47s of remaining time.\n",
      "Fitting model: XGBoost_r89_BAG_L2 ... Training model for up to 1696.49s of the 1696.47s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.03%)\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.03%)\n",
      "\tWarning: Exception caused XGBoost_r89_BAG_L2 to fail during training... Skipping this model.\n",
      "\t\tTask was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: 5e3a1bf681a894bb02ae3d4c693690b7f0f423b301000000, name=_ray_fit, pid=716927, memory used=0.13GB) was running was 966.32GB / 1007.45GB (0.959172), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 1ed9086463f794228a779c9c4f628cd4878ab95ac1f18eb4b3cddf11) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-1ed9086463f794228a779c9c4f628cd4878ab95ac1f18eb4b3cddf11*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.05\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.80\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "716927\t0.13\tray::IDLE\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 2753, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: 5e3a1bf681a894bb02ae3d4c693690b7f0f423b301000000, name=_ray_fit, pid=716927, memory used=0.13GB) was running was 966.32GB / 1007.45GB (0.959172), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 1ed9086463f794228a779c9c4f628cd4878ab95ac1f18eb4b3cddf11) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-1ed9086463f794228a779c9c4f628cd4878ab95ac1f18eb4b3cddf11*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.05\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.80\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "716927\t0.13\tray::IDLE\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\tWarning: Exception caused XGBoost_r89_BAG_L2 to fail during training... Skipping this model.\n",
      "\t\tTask was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: 5e3a1bf681a894bb02ae3d4c693690b7f0f423b301000000, name=_ray_fit, pid=716927, memory used=0.13GB) was running was 966.32GB / 1007.45GB (0.959172), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 1ed9086463f794228a779c9c4f628cd4878ab95ac1f18eb4b3cddf11) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-1ed9086463f794228a779c9c4f628cd4878ab95ac1f18eb4b3cddf11*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.05\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.80\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "716927\t0.13\tray::IDLE\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 2753, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: 5e3a1bf681a894bb02ae3d4c693690b7f0f423b301000000, name=_ray_fit, pid=716927, memory used=0.13GB) was running was 966.32GB / 1007.45GB (0.959172), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 1ed9086463f794228a779c9c4f628cd4878ab95ac1f18eb4b3cddf11) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-1ed9086463f794228a779c9c4f628cd4878ab95ac1f18eb4b3cddf11*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.05\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.80\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "716927\t0.13\tray::IDLE\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Fitting model: NeuralNetTorch_r30_BAG_L2 ... Training model for up to 1691.05s of the 1691.02s of remaining time.\n",
      "Fitting model: NeuralNetTorch_r30_BAG_L2 ... Training model for up to 1691.05s of the 1691.02s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.00%)\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.00%)\n",
      "\tWarning: Exception caused NeuralNetTorch_r30_BAG_L2 to fail during training... Skipping this model.\n",
      "\t\t\u001b[36mray::_ray_fit()\u001b[39m (pid=717256, ip=172.17.0.5)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 413, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 201, in _fit\n",
      "    train_dataset = self._generate_dataset(X, y, train_params=processor_kwargs, is_train=True)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 669, in _generate_dataset\n",
      "    dataset = self._process_train_data(\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 734, in _process_train_data\n",
      "    self.processor = create_preprocessor(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/utils/data_preprocessor.py\", line 40, in create_preprocessor\n",
      "    return ColumnTransformer(\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "TypeError: ColumnTransformer.__init__() got an unexpected keyword argument 'force_int_remainder_cols'\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 2753, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 904, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(TypeError): \u001b[36mray::_ray_fit()\u001b[39m (pid=717256, ip=172.17.0.5)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 413, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 201, in _fit\n",
      "    train_dataset = self._generate_dataset(X, y, train_params=processor_kwargs, is_train=True)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 669, in _generate_dataset\n",
      "    dataset = self._process_train_data(\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 734, in _process_train_data\n",
      "    self.processor = create_preprocessor(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/utils/data_preprocessor.py\", line 40, in create_preprocessor\n",
      "    return ColumnTransformer(\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "TypeError: ColumnTransformer.__init__() got an unexpected keyword argument 'force_int_remainder_cols'\n",
      "\tWarning: Exception caused NeuralNetTorch_r30_BAG_L2 to fail during training... Skipping this model.\n",
      "\t\t\u001b[36mray::_ray_fit()\u001b[39m (pid=717256, ip=172.17.0.5)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 413, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 201, in _fit\n",
      "    train_dataset = self._generate_dataset(X, y, train_params=processor_kwargs, is_train=True)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 669, in _generate_dataset\n",
      "    dataset = self._process_train_data(\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 734, in _process_train_data\n",
      "    self.processor = create_preprocessor(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/utils/data_preprocessor.py\", line 40, in create_preprocessor\n",
      "    return ColumnTransformer(\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "TypeError: ColumnTransformer.__init__() got an unexpected keyword argument 'force_int_remainder_cols'\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 2753, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 904, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(TypeError): \u001b[36mray::_ray_fit()\u001b[39m (pid=717256, ip=172.17.0.5)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 413, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 201, in _fit\n",
      "    train_dataset = self._generate_dataset(X, y, train_params=processor_kwargs, is_train=True)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 669, in _generate_dataset\n",
      "    dataset = self._process_train_data(\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 734, in _process_train_data\n",
      "    self.processor = create_preprocessor(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/utils/data_preprocessor.py\", line 40, in create_preprocessor\n",
      "    return ColumnTransformer(\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "TypeError: ColumnTransformer.__init__() got an unexpected keyword argument 'force_int_remainder_cols'\n",
      "Fitting model: LightGBM_r130_BAG_L2 ... Training model for up to 1687.63s of the 1687.61s of remaining time.\n",
      "Fitting model: LightGBM_r130_BAG_L2 ... Training model for up to 1687.63s of the 1687.61s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.04%)\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.04%)\n",
      "\tWarning: Exception caused LightGBM_r130_BAG_L2 to fail during training... Skipping this model.\n",
      "\t\tTask was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: d9fb4811694524f5c69605ed246818ca3a1b03b501000000, name=_ray_fit, pid=718674, memory used=0.12GB) was running was 966.17GB / 1007.45GB (0.95902), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 77eac58504fd114c95289db7bf1e654c5acd226f4b2ffb40381f218f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-77eac58504fd114c95289db7bf1e654c5acd226f4b2ffb40381f218f*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.05\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.80\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "718674\t0.12\tray::IDLE\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 2753, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: d9fb4811694524f5c69605ed246818ca3a1b03b501000000, name=_ray_fit, pid=718674, memory used=0.12GB) was running was 966.17GB / 1007.45GB (0.95902), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 77eac58504fd114c95289db7bf1e654c5acd226f4b2ffb40381f218f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-77eac58504fd114c95289db7bf1e654c5acd226f4b2ffb40381f218f*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.05\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.80\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "718674\t0.12\tray::IDLE\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\tWarning: Exception caused LightGBM_r130_BAG_L2 to fail during training... Skipping this model.\n",
      "\t\tTask was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: d9fb4811694524f5c69605ed246818ca3a1b03b501000000, name=_ray_fit, pid=718674, memory used=0.12GB) was running was 966.17GB / 1007.45GB (0.95902), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 77eac58504fd114c95289db7bf1e654c5acd226f4b2ffb40381f218f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-77eac58504fd114c95289db7bf1e654c5acd226f4b2ffb40381f218f*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.05\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.80\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "718674\t0.12\tray::IDLE\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 2753, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: d9fb4811694524f5c69605ed246818ca3a1b03b501000000, name=_ray_fit, pid=718674, memory used=0.12GB) was running was 966.17GB / 1007.45GB (0.95902), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 77eac58504fd114c95289db7bf1e654c5acd226f4b2ffb40381f218f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-77eac58504fd114c95289db7bf1e654c5acd226f4b2ffb40381f218f*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.05\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.80\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "718674\t0.12\tray::IDLE\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "2025-09-30 14:34:48,787\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-09-30 14:34:48,789\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "Fitting model: NeuralNetTorch_r86_BAG_L2 ... Training model for up to 1683.51s of the 1683.48s of remaining time.\n",
      "2025-09-30 14:34:48,787\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-09-30 14:34:48,789\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "Fitting model: NeuralNetTorch_r86_BAG_L2 ... Training model for up to 1683.51s of the 1683.48s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.00%)\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.00%)\n",
      "\tWarning: Exception caused NeuralNetTorch_r86_BAG_L2 to fail during training... Skipping this model.\n",
      "\t\t\u001b[36mray::_ray_fit()\u001b[39m (pid=720000, ip=172.17.0.5)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 413, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 201, in _fit\n",
      "    train_dataset = self._generate_dataset(X, y, train_params=processor_kwargs, is_train=True)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 669, in _generate_dataset\n",
      "    dataset = self._process_train_data(\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 734, in _process_train_data\n",
      "    self.processor = create_preprocessor(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/utils/data_preprocessor.py\", line 40, in create_preprocessor\n",
      "    return ColumnTransformer(\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "TypeError: ColumnTransformer.__init__() got an unexpected keyword argument 'force_int_remainder_cols'\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 2753, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 904, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(TypeError): \u001b[36mray::_ray_fit()\u001b[39m (pid=720000, ip=172.17.0.5)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 413, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 201, in _fit\n",
      "    train_dataset = self._generate_dataset(X, y, train_params=processor_kwargs, is_train=True)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 669, in _generate_dataset\n",
      "    dataset = self._process_train_data(\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 734, in _process_train_data\n",
      "    self.processor = create_preprocessor(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/utils/data_preprocessor.py\", line 40, in create_preprocessor\n",
      "    return ColumnTransformer(\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "TypeError: ColumnTransformer.__init__() got an unexpected keyword argument 'force_int_remainder_cols'\n",
      "\tWarning: Exception caused NeuralNetTorch_r86_BAG_L2 to fail during training... Skipping this model.\n",
      "\t\t\u001b[36mray::_ray_fit()\u001b[39m (pid=720000, ip=172.17.0.5)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 413, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 201, in _fit\n",
      "    train_dataset = self._generate_dataset(X, y, train_params=processor_kwargs, is_train=True)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 669, in _generate_dataset\n",
      "    dataset = self._process_train_data(\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 734, in _process_train_data\n",
      "    self.processor = create_preprocessor(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/utils/data_preprocessor.py\", line 40, in create_preprocessor\n",
      "    return ColumnTransformer(\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "TypeError: ColumnTransformer.__init__() got an unexpected keyword argument 'force_int_remainder_cols'\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 2753, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 904, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(TypeError): \u001b[36mray::_ray_fit()\u001b[39m (pid=720000, ip=172.17.0.5)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 413, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 201, in _fit\n",
      "    train_dataset = self._generate_dataset(X, y, train_params=processor_kwargs, is_train=True)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 669, in _generate_dataset\n",
      "    dataset = self._process_train_data(\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 734, in _process_train_data\n",
      "    self.processor = create_preprocessor(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/utils/data_preprocessor.py\", line 40, in create_preprocessor\n",
      "    return ColumnTransformer(\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "TypeError: ColumnTransformer.__init__() got an unexpected keyword argument 'force_int_remainder_cols'\n",
      "Fitting model: CatBoost_r50_BAG_L2 ... Training model for up to 1679.92s of the 1679.89s of remaining time.\n",
      "Fitting model: CatBoost_r50_BAG_L2 ... Training model for up to 1679.92s of the 1679.89s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.01%)\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.01%)\n",
      "2025-09-30 14:34:57,473\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-09-30 14:34:57,474\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-09-30 14:34:57,475\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-09-30 14:34:57,473\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-09-30 14:34:57,474\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-09-30 14:34:57,475\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "\tWarning: Exception caused CatBoost_r50_BAG_L2 to fail during training... Skipping this model.\n",
      "\t\tTask was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: 2205499ce88b05ae77259d191b755de9852484b901000000, name=_ray_fit, pid=723401, memory used=0.14GB) was running was 966.88GB / 1007.45GB (0.959724), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 1a4be7a5429adb7643299bf6685d6c45a851dabe2bd56c2ebf08d17a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-1a4be7a5429adb7643299bf6685d6c45a851dabe2bd56c2ebf08d17a*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.05\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.80\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "723401\t0.14\tray::_ray_fit\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 2753, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: 2205499ce88b05ae77259d191b755de9852484b901000000, name=_ray_fit, pid=723401, memory used=0.14GB) was running was 966.88GB / 1007.45GB (0.959724), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 1a4be7a5429adb7643299bf6685d6c45a851dabe2bd56c2ebf08d17a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-1a4be7a5429adb7643299bf6685d6c45a851dabe2bd56c2ebf08d17a*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.05\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.80\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "723401\t0.14\tray::_ray_fit\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\tWarning: Exception caused CatBoost_r50_BAG_L2 to fail during training... Skipping this model.\n",
      "\t\tTask was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: 2205499ce88b05ae77259d191b755de9852484b901000000, name=_ray_fit, pid=723401, memory used=0.14GB) was running was 966.88GB / 1007.45GB (0.959724), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 1a4be7a5429adb7643299bf6685d6c45a851dabe2bd56c2ebf08d17a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-1a4be7a5429adb7643299bf6685d6c45a851dabe2bd56c2ebf08d17a*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.05\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.80\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "723401\t0.14\tray::_ray_fit\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 2753, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: 2205499ce88b05ae77259d191b755de9852484b901000000, name=_ray_fit, pid=723401, memory used=0.14GB) was running was 966.88GB / 1007.45GB (0.959724), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 1a4be7a5429adb7643299bf6685d6c45a851dabe2bd56c2ebf08d17a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-1a4be7a5429adb7643299bf6685d6c45a851dabe2bd56c2ebf08d17a*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.05\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.80\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "723401\t0.14\tray::_ray_fit\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Fitting model: NeuralNetFastAI_r11_BAG_L2 ... Training model for up to 1674.09s of the 1674.07s of remaining time.\n",
      "Fitting model: NeuralNetFastAI_r11_BAG_L2 ... Training model for up to 1674.09s of the 1674.07s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.00%)\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.00%)\n",
      "\tWarning: Exception caused NeuralNetFastAI_r11_BAG_L2 to fail during training... Skipping this model.\n",
      "\t\tTask was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: 61624050c1a5e1d4e8061abc9d69e12336d4dd2001000000, name=_ray_fit, pid=725632, memory used=0.17GB) was running was 967.14GB / 1007.45GB (0.959991), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c2661dcd8a8ddfa8b5f63f943290b997f3cd34c67b6f352d36bbd061) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-c2661dcd8a8ddfa8b5f63f943290b997f3cd34c67b6f352d36bbd061*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.05\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.80\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "725632\t0.17\tray::_ray_fit\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 2753, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: 61624050c1a5e1d4e8061abc9d69e12336d4dd2001000000, name=_ray_fit, pid=725632, memory used=0.17GB) was running was 967.14GB / 1007.45GB (0.959991), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c2661dcd8a8ddfa8b5f63f943290b997f3cd34c67b6f352d36bbd061) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-c2661dcd8a8ddfa8b5f63f943290b997f3cd34c67b6f352d36bbd061*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.05\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.80\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "725632\t0.17\tray::_ray_fit\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\tWarning: Exception caused NeuralNetFastAI_r11_BAG_L2 to fail during training... Skipping this model.\n",
      "\t\tTask was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: 61624050c1a5e1d4e8061abc9d69e12336d4dd2001000000, name=_ray_fit, pid=725632, memory used=0.17GB) was running was 967.14GB / 1007.45GB (0.959991), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c2661dcd8a8ddfa8b5f63f943290b997f3cd34c67b6f352d36bbd061) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-c2661dcd8a8ddfa8b5f63f943290b997f3cd34c67b6f352d36bbd061*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.05\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.80\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "725632\t0.17\tray::_ray_fit\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 2753, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: 61624050c1a5e1d4e8061abc9d69e12336d4dd2001000000, name=_ray_fit, pid=725632, memory used=0.17GB) was running was 967.14GB / 1007.45GB (0.959991), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c2661dcd8a8ddfa8b5f63f943290b997f3cd34c67b6f352d36bbd061) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-c2661dcd8a8ddfa8b5f63f943290b997f3cd34c67b6f352d36bbd061*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.05\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.80\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "725632\t0.17\tray::_ray_fit\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Fitting model: XGBoost_r194_BAG_L2 ... Training model for up to 1666.52s of the 1666.49s of remaining time.\n",
      "Fitting model: XGBoost_r194_BAG_L2 ... Training model for up to 1666.52s of the 1666.49s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.06%)\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.06%)\n",
      "2025-09-30 14:35:11,475\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-09-30 14:35:11,475\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "\tWarning: Exception caused XGBoost_r194_BAG_L2 to fail during training... Skipping this model.\n",
      "\t\tTask was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: abc0482e34949713564969b8aa7f3afb7e7b016101000000, name=_ray_fit, pid=728160, memory used=0.11GB) was running was 967.40GB / 1007.45GB (0.960246), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: fe81712ee08e8b44e8a61902bb37ed71d8a7a8e2ab255bb38aa59552) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-fe81712ee08e8b44e8a61902bb37ed71d8a7a8e2ab255bb38aa59552*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.05\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.80\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "728160\t0.11\tray::IDLE\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 2753, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: abc0482e34949713564969b8aa7f3afb7e7b016101000000, name=_ray_fit, pid=728160, memory used=0.11GB) was running was 967.40GB / 1007.45GB (0.960246), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: fe81712ee08e8b44e8a61902bb37ed71d8a7a8e2ab255bb38aa59552) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-fe81712ee08e8b44e8a61902bb37ed71d8a7a8e2ab255bb38aa59552*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.05\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.80\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "728160\t0.11\tray::IDLE\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\tWarning: Exception caused XGBoost_r194_BAG_L2 to fail during training... Skipping this model.\n",
      "\t\tTask was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: abc0482e34949713564969b8aa7f3afb7e7b016101000000, name=_ray_fit, pid=728160, memory used=0.11GB) was running was 967.40GB / 1007.45GB (0.960246), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: fe81712ee08e8b44e8a61902bb37ed71d8a7a8e2ab255bb38aa59552) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-fe81712ee08e8b44e8a61902bb37ed71d8a7a8e2ab255bb38aa59552*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.05\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.80\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "728160\t0.11\tray::IDLE\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 2753, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: abc0482e34949713564969b8aa7f3afb7e7b016101000000, name=_ray_fit, pid=728160, memory used=0.11GB) was running was 967.40GB / 1007.45GB (0.960246), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: fe81712ee08e8b44e8a61902bb37ed71d8a7a8e2ab255bb38aa59552) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-fe81712ee08e8b44e8a61902bb37ed71d8a7a8e2ab255bb38aa59552*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.05\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.80\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "728160\t0.11\tray::IDLE\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Fitting model: ExtraTrees_r172_BAG_L2 ... Training model for up to 1656.61s of the 1656.59s of remaining time.\n",
      "Fitting model: ExtraTrees_r172_BAG_L2 ... Training model for up to 1656.61s of the 1656.59s of remaining time.\n",
      "\t-0.1199\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.27s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: CatBoost_r69_BAG_L2 ... Training model for up to 1656.24s of the 1656.22s of remaining time.\n",
      "\t-0.1199\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.27s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: CatBoost_r69_BAG_L2 ... Training model for up to 1656.24s of the 1656.22s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.02%)\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.02%)\n",
      "\tWarning: Exception caused CatBoost_r69_BAG_L2 to fail during training... Skipping this model.\n",
      "\t\tTask was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: dba6db33d99a24ceb24ae3250a4d0f0aa6ce8f0701000000, name=_ray_fit, pid=730690, memory used=0.11GB) was running was 967.38GB / 1007.45GB (0.960222), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ec3098e4c8cecb67a852c7756438cc78323ed12d529606cb254c75e0) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-ec3098e4c8cecb67a852c7756438cc78323ed12d529606cb254c75e0*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.05\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.80\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "730690\t0.11\tray::IDLE\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 2753, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: dba6db33d99a24ceb24ae3250a4d0f0aa6ce8f0701000000, name=_ray_fit, pid=730690, memory used=0.11GB) was running was 967.38GB / 1007.45GB (0.960222), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ec3098e4c8cecb67a852c7756438cc78323ed12d529606cb254c75e0) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-ec3098e4c8cecb67a852c7756438cc78323ed12d529606cb254c75e0*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.05\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.80\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "730690\t0.11\tray::IDLE\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\tWarning: Exception caused CatBoost_r69_BAG_L2 to fail during training... Skipping this model.\n",
      "\t\tTask was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: dba6db33d99a24ceb24ae3250a4d0f0aa6ce8f0701000000, name=_ray_fit, pid=730690, memory used=0.11GB) was running was 967.38GB / 1007.45GB (0.960222), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ec3098e4c8cecb67a852c7756438cc78323ed12d529606cb254c75e0) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-ec3098e4c8cecb67a852c7756438cc78323ed12d529606cb254c75e0*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.05\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.80\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "730690\t0.11\tray::IDLE\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 2753, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: dba6db33d99a24ceb24ae3250a4d0f0aa6ce8f0701000000, name=_ray_fit, pid=730690, memory used=0.11GB) was running was 967.38GB / 1007.45GB (0.960222), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ec3098e4c8cecb67a852c7756438cc78323ed12d529606cb254c75e0) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-ec3098e4c8cecb67a852c7756438cc78323ed12d529606cb254c75e0*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.05\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.80\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "730690\t0.11\tray::IDLE\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Fitting model: NeuralNetFastAI_r103_BAG_L2 ... Training model for up to 1651.21s of the 1651.18s of remaining time.\n",
      "Fitting model: NeuralNetFastAI_r103_BAG_L2 ... Training model for up to 1651.21s of the 1651.18s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.00%)\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.00%)\n",
      "\tWarning: Exception caused NeuralNetFastAI_r103_BAG_L2 to fail during training... Skipping this model.\n",
      "\t\tTask was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: e38690c3642fd63a3b83e7ce5a909555480d2e3501000000, name=_ray_fit, pid=730691, memory used=0.10GB) was running was 967.32GB / 1007.45GB (0.960168), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 58ac8865f36527bed775f4f84275a47a781cb3486f986552367839d0) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-58ac8865f36527bed775f4f84275a47a781cb3486f986552367839d0*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.05\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.80\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "730691\t0.10\tray::IDLE\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 2753, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: e38690c3642fd63a3b83e7ce5a909555480d2e3501000000, name=_ray_fit, pid=730691, memory used=0.10GB) was running was 967.32GB / 1007.45GB (0.960168), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 58ac8865f36527bed775f4f84275a47a781cb3486f986552367839d0) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-58ac8865f36527bed775f4f84275a47a781cb3486f986552367839d0*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.05\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.80\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "730691\t0.10\tray::IDLE\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\tWarning: Exception caused NeuralNetFastAI_r103_BAG_L2 to fail during training... Skipping this model.\n",
      "\t\tTask was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: e38690c3642fd63a3b83e7ce5a909555480d2e3501000000, name=_ray_fit, pid=730691, memory used=0.10GB) was running was 967.32GB / 1007.45GB (0.960168), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 58ac8865f36527bed775f4f84275a47a781cb3486f986552367839d0) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-58ac8865f36527bed775f4f84275a47a781cb3486f986552367839d0*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.05\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.80\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "730691\t0.10\tray::IDLE\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 2753, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: e38690c3642fd63a3b83e7ce5a909555480d2e3501000000, name=_ray_fit, pid=730691, memory used=0.10GB) was running was 967.32GB / 1007.45GB (0.960168), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 58ac8865f36527bed775f4f84275a47a781cb3486f986552367839d0) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-58ac8865f36527bed775f4f84275a47a781cb3486f986552367839d0*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.05\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.80\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "730691\t0.10\tray::IDLE\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Fitting model: NeuralNetTorch_r14_BAG_L2 ... Training model for up to 1650.68s of the 1650.66s of remaining time.\n",
      "Fitting model: NeuralNetTorch_r14_BAG_L2 ... Training model for up to 1650.68s of the 1650.66s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.00%)\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.00%)\n",
      "\tWarning: Exception caused NeuralNetTorch_r14_BAG_L2 to fail during training... Skipping this model.\n",
      "\t\t\u001b[36mray::_ray_fit()\u001b[39m (pid=730921, ip=172.17.0.5)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 413, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 201, in _fit\n",
      "    train_dataset = self._generate_dataset(X, y, train_params=processor_kwargs, is_train=True)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 669, in _generate_dataset\n",
      "    dataset = self._process_train_data(\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 734, in _process_train_data\n",
      "    self.processor = create_preprocessor(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/utils/data_preprocessor.py\", line 40, in create_preprocessor\n",
      "    return ColumnTransformer(\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "TypeError: ColumnTransformer.__init__() got an unexpected keyword argument 'force_int_remainder_cols'\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 2753, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 904, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(TypeError): \u001b[36mray::_ray_fit()\u001b[39m (pid=730921, ip=172.17.0.5)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 413, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 201, in _fit\n",
      "    train_dataset = self._generate_dataset(X, y, train_params=processor_kwargs, is_train=True)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 669, in _generate_dataset\n",
      "    dataset = self._process_train_data(\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 734, in _process_train_data\n",
      "    self.processor = create_preprocessor(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/utils/data_preprocessor.py\", line 40, in create_preprocessor\n",
      "    return ColumnTransformer(\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "TypeError: ColumnTransformer.__init__() got an unexpected keyword argument 'force_int_remainder_cols'\n",
      "\tWarning: Exception caused NeuralNetTorch_r14_BAG_L2 to fail during training... Skipping this model.\n",
      "\t\t\u001b[36mray::_ray_fit()\u001b[39m (pid=730921, ip=172.17.0.5)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 413, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 201, in _fit\n",
      "    train_dataset = self._generate_dataset(X, y, train_params=processor_kwargs, is_train=True)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 669, in _generate_dataset\n",
      "    dataset = self._process_train_data(\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 734, in _process_train_data\n",
      "    self.processor = create_preprocessor(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/utils/data_preprocessor.py\", line 40, in create_preprocessor\n",
      "    return ColumnTransformer(\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "TypeError: ColumnTransformer.__init__() got an unexpected keyword argument 'force_int_remainder_cols'\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 2753, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 904, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(TypeError): \u001b[36mray::_ray_fit()\u001b[39m (pid=730921, ip=172.17.0.5)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 413, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 201, in _fit\n",
      "    train_dataset = self._generate_dataset(X, y, train_params=processor_kwargs, is_train=True)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 669, in _generate_dataset\n",
      "    dataset = self._process_train_data(\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 734, in _process_train_data\n",
      "    self.processor = create_preprocessor(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/utils/data_preprocessor.py\", line 40, in create_preprocessor\n",
      "    return ColumnTransformer(\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "TypeError: ColumnTransformer.__init__() got an unexpected keyword argument 'force_int_remainder_cols'\n",
      "Fitting model: LightGBM_r161_BAG_L2 ... Training model for up to 1646.99s of the 1646.97s of remaining time.\n",
      "Fitting model: LightGBM_r161_BAG_L2 ... Training model for up to 1646.99s of the 1646.97s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.13%)\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.13%)\n",
      "2025-09-30 14:35:30,481\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-09-30 14:35:30,482\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-09-30 14:35:30,482\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-09-30 14:35:30,483\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-09-30 14:35:30,481\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-09-30 14:35:30,482\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-09-30 14:35:30,482\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-09-30 14:35:30,483\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "\tWarning: Exception caused LightGBM_r161_BAG_L2 to fail during training... Skipping this model.\n",
      "\t\tTask was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: d702aa2f5b4460e735644ea63e2c14488d900a1401000000, name=_ray_fit, pid=733983, memory used=0.10GB) was running was 967.90GB / 1007.45GB (0.96074), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 5e9818d1a9dfeb180be89934d9083ff067621e6067dda612de50f4c2) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-5e9818d1a9dfeb180be89934d9083ff067621e6067dda612de50f4c2*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.05\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.80\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "733983\t0.10\tray::IDLE\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 2753, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: d702aa2f5b4460e735644ea63e2c14488d900a1401000000, name=_ray_fit, pid=733983, memory used=0.10GB) was running was 967.90GB / 1007.45GB (0.96074), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 5e9818d1a9dfeb180be89934d9083ff067621e6067dda612de50f4c2) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-5e9818d1a9dfeb180be89934d9083ff067621e6067dda612de50f4c2*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.05\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.80\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "733983\t0.10\tray::IDLE\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\tWarning: Exception caused LightGBM_r161_BAG_L2 to fail during training... Skipping this model.\n",
      "\t\tTask was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: d702aa2f5b4460e735644ea63e2c14488d900a1401000000, name=_ray_fit, pid=733983, memory used=0.10GB) was running was 967.90GB / 1007.45GB (0.96074), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 5e9818d1a9dfeb180be89934d9083ff067621e6067dda612de50f4c2) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-5e9818d1a9dfeb180be89934d9083ff067621e6067dda612de50f4c2*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.05\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.80\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "733983\t0.10\tray::IDLE\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 2753, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: d702aa2f5b4460e735644ea63e2c14488d900a1401000000, name=_ray_fit, pid=733983, memory used=0.10GB) was running was 967.90GB / 1007.45GB (0.96074), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 5e9818d1a9dfeb180be89934d9083ff067621e6067dda612de50f4c2) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-5e9818d1a9dfeb180be89934d9083ff067621e6067dda612de50f4c2*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.05\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.80\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "733983\t0.10\tray::IDLE\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Fitting model: NeuralNetFastAI_r143_BAG_L2 ... Training model for up to 1639.21s of the 1639.18s of remaining time.\n",
      "Fitting model: NeuralNetFastAI_r143_BAG_L2 ... Training model for up to 1639.21s of the 1639.18s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.00%)\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.00%)\n",
      "\tWarning: Exception caused NeuralNetFastAI_r143_BAG_L2 to fail during training... Skipping this model.\n",
      "\t\tTask was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: 87d7092bf9eb21dc8643b3e11351ee315cfc9c7d01000000, name=_ray_fit, pid=734238, memory used=0.10GB) was running was 967.90GB / 1007.45GB (0.960742), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 1d8f4fd2c8c0e2b02f539b4e33f9825b99142a7e5f745ed2c91a3a1c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-1d8f4fd2c8c0e2b02f539b4e33f9825b99142a7e5f745ed2c91a3a1c*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.05\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.80\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "734238\t0.10\tray::IDLE\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 2753, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: 87d7092bf9eb21dc8643b3e11351ee315cfc9c7d01000000, name=_ray_fit, pid=734238, memory used=0.10GB) was running was 967.90GB / 1007.45GB (0.960742), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 1d8f4fd2c8c0e2b02f539b4e33f9825b99142a7e5f745ed2c91a3a1c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-1d8f4fd2c8c0e2b02f539b4e33f9825b99142a7e5f745ed2c91a3a1c*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.05\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.80\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "734238\t0.10\tray::IDLE\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\tWarning: Exception caused NeuralNetFastAI_r143_BAG_L2 to fail during training... Skipping this model.\n",
      "\t\tTask was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: 87d7092bf9eb21dc8643b3e11351ee315cfc9c7d01000000, name=_ray_fit, pid=734238, memory used=0.10GB) was running was 967.90GB / 1007.45GB (0.960742), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 1d8f4fd2c8c0e2b02f539b4e33f9825b99142a7e5f745ed2c91a3a1c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-1d8f4fd2c8c0e2b02f539b4e33f9825b99142a7e5f745ed2c91a3a1c*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.05\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.80\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "734238\t0.10\tray::IDLE\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 2753, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: 87d7092bf9eb21dc8643b3e11351ee315cfc9c7d01000000, name=_ray_fit, pid=734238, memory used=0.10GB) was running was 967.90GB / 1007.45GB (0.960742), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 1d8f4fd2c8c0e2b02f539b4e33f9825b99142a7e5f745ed2c91a3a1c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-1d8f4fd2c8c0e2b02f539b4e33f9825b99142a7e5f745ed2c91a3a1c*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.05\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.80\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "734238\t0.10\tray::IDLE\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "2025-09-30 14:35:33,620\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-09-30 14:35:33,621\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "Fitting model: CatBoost_r70_BAG_L2 ... Training model for up to 1638.67s of the 1638.65s of remaining time.\n",
      "2025-09-30 14:35:33,620\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-09-30 14:35:33,621\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "Fitting model: CatBoost_r70_BAG_L2 ... Training model for up to 1638.67s of the 1638.65s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.03%)\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.03%)\n",
      "\tWarning: Exception caused CatBoost_r70_BAG_L2 to fail during training... Skipping this model.\n",
      "\t\tTask was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: 99f4471f139cc2742dfe753a2744fbc0412a44e901000000, name=_ray_fit, pid=734596, memory used=0.17GB) was running was 968.50GB / 1007.45GB (0.961335), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: b3b13cb85d28826cce5005f7ee2d4462afb3b935e65dbc2115253d16) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-b3b13cb85d28826cce5005f7ee2d4462afb3b935e65dbc2115253d16*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.04\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.80\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "734594\t0.18\tray::IDLE\n",
      "734596\t0.17\tray::_ray_fit\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 2753, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: 99f4471f139cc2742dfe753a2744fbc0412a44e901000000, name=_ray_fit, pid=734596, memory used=0.17GB) was running was 968.50GB / 1007.45GB (0.961335), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: b3b13cb85d28826cce5005f7ee2d4462afb3b935e65dbc2115253d16) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-b3b13cb85d28826cce5005f7ee2d4462afb3b935e65dbc2115253d16*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.04\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.80\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "734594\t0.18\tray::IDLE\n",
      "734596\t0.17\tray::_ray_fit\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\tWarning: Exception caused CatBoost_r70_BAG_L2 to fail during training... Skipping this model.\n",
      "\t\tTask was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: 99f4471f139cc2742dfe753a2744fbc0412a44e901000000, name=_ray_fit, pid=734596, memory used=0.17GB) was running was 968.50GB / 1007.45GB (0.961335), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: b3b13cb85d28826cce5005f7ee2d4462afb3b935e65dbc2115253d16) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-b3b13cb85d28826cce5005f7ee2d4462afb3b935e65dbc2115253d16*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.04\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.80\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "734594\t0.18\tray::IDLE\n",
      "734596\t0.17\tray::_ray_fit\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 2753, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: 99f4471f139cc2742dfe753a2744fbc0412a44e901000000, name=_ray_fit, pid=734596, memory used=0.17GB) was running was 968.50GB / 1007.45GB (0.961335), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: b3b13cb85d28826cce5005f7ee2d4462afb3b935e65dbc2115253d16) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-b3b13cb85d28826cce5005f7ee2d4462afb3b935e65dbc2115253d16*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.04\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.80\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "734594\t0.18\tray::IDLE\n",
      "734596\t0.17\tray::_ray_fit\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Fitting model: NeuralNetFastAI_r156_BAG_L2 ... Training model for up to 1635.14s of the 1635.12s of remaining time.\n",
      "Fitting model: NeuralNetFastAI_r156_BAG_L2 ... Training model for up to 1635.14s of the 1635.12s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.00%)\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.00%)\n",
      "2025-09-30 14:35:42,484\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-09-30 14:35:42,485\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-09-30 14:35:42,484\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-09-30 14:35:42,485\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "\tWarning: Exception caused NeuralNetFastAI_r156_BAG_L2 to fail during training... Skipping this model.\n",
      "\t\tTask was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: ed2ad1e255703c341829601ffc84b8b2d5b0f72101000000, name=_ray_fit, pid=738442, memory used=0.08GB) was running was 968.50GB / 1007.45GB (0.961336), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e8125723273107416ca847386a4949a9d91d56c69dcad7f1b4c1fc0c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-e8125723273107416ca847386a4949a9d91d56c69dcad7f1b4c1fc0c*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "521385\t0.91\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "522871\t0.80\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "736979\t0.37\tray::IDLE\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 2753, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: ed2ad1e255703c341829601ffc84b8b2d5b0f72101000000, name=_ray_fit, pid=738442, memory used=0.08GB) was running was 968.50GB / 1007.45GB (0.961336), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e8125723273107416ca847386a4949a9d91d56c69dcad7f1b4c1fc0c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-e8125723273107416ca847386a4949a9d91d56c69dcad7f1b4c1fc0c*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "521385\t0.91\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "522871\t0.80\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "736979\t0.37\tray::IDLE\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\tWarning: Exception caused NeuralNetFastAI_r156_BAG_L2 to fail during training... Skipping this model.\n",
      "\t\tTask was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: ed2ad1e255703c341829601ffc84b8b2d5b0f72101000000, name=_ray_fit, pid=738442, memory used=0.08GB) was running was 968.50GB / 1007.45GB (0.961336), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e8125723273107416ca847386a4949a9d91d56c69dcad7f1b4c1fc0c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-e8125723273107416ca847386a4949a9d91d56c69dcad7f1b4c1fc0c*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "521385\t0.91\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "522871\t0.80\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "736979\t0.37\tray::IDLE\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 2753, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: ed2ad1e255703c341829601ffc84b8b2d5b0f72101000000, name=_ray_fit, pid=738442, memory used=0.08GB) was running was 968.50GB / 1007.45GB (0.961336), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e8125723273107416ca847386a4949a9d91d56c69dcad7f1b4c1fc0c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-e8125723273107416ca847386a4949a9d91d56c69dcad7f1b4c1fc0c*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "521385\t0.91\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "522871\t0.80\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "736979\t0.37\tray::IDLE\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Fitting model: LightGBM_r196_BAG_L2 ... Training model for up to 1627.06s of the 1627.04s of remaining time.\n",
      "Fitting model: LightGBM_r196_BAG_L2 ... Training model for up to 1627.06s of the 1627.04s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.07%)\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.07%)\n",
      "2025-09-30 14:35:50,486\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-09-30 14:35:50,487\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-09-30 14:35:50,486\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-09-30 14:35:50,487\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "\tWarning: Exception caused LightGBM_r196_BAG_L2 to fail during training... Skipping this model.\n",
      "\t\tTask was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: 6d4db6f1a5eb3bb9da3a8997b4c9a27c7919fc5e01000000, name=_ray_fit, pid=740556, memory used=0.11GB) was running was 968.12GB / 1007.45GB (0.960957), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d2333bc657a6ba898e8cb3280f1848b9647dc3f9e22f53a65f1db836) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-d2333bc657a6ba898e8cb3280f1848b9647dc3f9e22f53a65f1db836*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.05\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.80\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "740556\t0.11\tray::IDLE\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 2753, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: 6d4db6f1a5eb3bb9da3a8997b4c9a27c7919fc5e01000000, name=_ray_fit, pid=740556, memory used=0.11GB) was running was 968.12GB / 1007.45GB (0.960957), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d2333bc657a6ba898e8cb3280f1848b9647dc3f9e22f53a65f1db836) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-d2333bc657a6ba898e8cb3280f1848b9647dc3f9e22f53a65f1db836*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.05\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.80\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "740556\t0.11\tray::IDLE\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\tWarning: Exception caused LightGBM_r196_BAG_L2 to fail during training... Skipping this model.\n",
      "\t\tTask was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: 6d4db6f1a5eb3bb9da3a8997b4c9a27c7919fc5e01000000, name=_ray_fit, pid=740556, memory used=0.11GB) was running was 968.12GB / 1007.45GB (0.960957), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d2333bc657a6ba898e8cb3280f1848b9647dc3f9e22f53a65f1db836) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-d2333bc657a6ba898e8cb3280f1848b9647dc3f9e22f53a65f1db836*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.05\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.80\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "740556\t0.11\tray::IDLE\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 2753, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: 6d4db6f1a5eb3bb9da3a8997b4c9a27c7919fc5e01000000, name=_ray_fit, pid=740556, memory used=0.11GB) was running was 968.12GB / 1007.45GB (0.960957), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d2333bc657a6ba898e8cb3280f1848b9647dc3f9e22f53a65f1db836) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-d2333bc657a6ba898e8cb3280f1848b9647dc3f9e22f53a65f1db836*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.05\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.80\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "740556\t0.11\tray::IDLE\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Fitting model: RandomForest_r39_BAG_L2 ... Training model for up to 1619.68s of the 1619.66s of remaining time.\n",
      "Fitting model: RandomForest_r39_BAG_L2 ... Training model for up to 1619.68s of the 1619.66s of remaining time.\n",
      "\t-0.1264\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.41s\t = Training   runtime\n",
      "\t0.09s\t = Validation runtime\n",
      "Fitting model: CatBoost_r167_BAG_L2 ... Training model for up to 1619.12s of the 1619.09s of remaining time.\n",
      "\t-0.1264\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.41s\t = Training   runtime\n",
      "\t0.09s\t = Validation runtime\n",
      "Fitting model: CatBoost_r167_BAG_L2 ... Training model for up to 1619.12s of the 1619.09s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.07%)\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.07%)\n",
      "\tWarning: Exception caused CatBoost_r167_BAG_L2 to fail during training... Skipping this model.\n",
      "\t\tTask was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: 47e9c6f2306d4cbef79455c26585e92032635cd101000000, name=_ray_fit, pid=740798, memory used=0.16GB) was running was 969.25GB / 1007.45GB (0.962083), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e7f8fcf0fe2da700095f7369b6bbac7e13e50aefac8aaf72277cd0bc) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-e7f8fcf0fe2da700095f7369b6bbac7e13e50aefac8aaf72277cd0bc*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.04\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.80\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "740796\t0.17\tray::IDLE\n",
      "740803\t0.17\tray::IDLE\n",
      "740798\t0.16\tray::_ray_fit\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 2753, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: 47e9c6f2306d4cbef79455c26585e92032635cd101000000, name=_ray_fit, pid=740798, memory used=0.16GB) was running was 969.25GB / 1007.45GB (0.962083), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e7f8fcf0fe2da700095f7369b6bbac7e13e50aefac8aaf72277cd0bc) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-e7f8fcf0fe2da700095f7369b6bbac7e13e50aefac8aaf72277cd0bc*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.04\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.80\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "740796\t0.17\tray::IDLE\n",
      "740803\t0.17\tray::IDLE\n",
      "740798\t0.16\tray::_ray_fit\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\tWarning: Exception caused CatBoost_r167_BAG_L2 to fail during training... Skipping this model.\n",
      "\t\tTask was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: 47e9c6f2306d4cbef79455c26585e92032635cd101000000, name=_ray_fit, pid=740798, memory used=0.16GB) was running was 969.25GB / 1007.45GB (0.962083), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e7f8fcf0fe2da700095f7369b6bbac7e13e50aefac8aaf72277cd0bc) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-e7f8fcf0fe2da700095f7369b6bbac7e13e50aefac8aaf72277cd0bc*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.04\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.80\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "740796\t0.17\tray::IDLE\n",
      "740803\t0.17\tray::IDLE\n",
      "740798\t0.16\tray::_ray_fit\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 2753, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: 47e9c6f2306d4cbef79455c26585e92032635cd101000000, name=_ray_fit, pid=740798, memory used=0.16GB) was running was 969.25GB / 1007.45GB (0.962083), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e7f8fcf0fe2da700095f7369b6bbac7e13e50aefac8aaf72277cd0bc) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-e7f8fcf0fe2da700095f7369b6bbac7e13e50aefac8aaf72277cd0bc*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.04\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.80\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "740796\t0.17\tray::IDLE\n",
      "740803\t0.17\tray::IDLE\n",
      "740798\t0.16\tray::_ray_fit\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Fitting model: NeuralNetFastAI_r95_BAG_L2 ... Training model for up to 1615.69s of the 1615.66s of remaining time.\n",
      "Fitting model: NeuralNetFastAI_r95_BAG_L2 ... Training model for up to 1615.69s of the 1615.66s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.00%)\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.00%)\n",
      "\tWarning: Exception caused NeuralNetFastAI_r95_BAG_L2 to fail during training... Skipping this model.\n",
      "\t\tTask was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: b28f6848cd6f0db33d4728bdbd16c9e5aab5fe0c01000000, name=_ray_fit, pid=744907, memory used=0.09GB) was running was 968.62GB / 1007.45GB (0.961455), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 2e0a76f8fc7bc49ac78c23c6d10df645554cfc12b3b15683d9838a05) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-2e0a76f8fc7bc49ac78c23c6d10df645554cfc12b3b15683d9838a05*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "521385\t0.91\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "522871\t0.80\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "742821\t0.38\t\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 2753, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: b28f6848cd6f0db33d4728bdbd16c9e5aab5fe0c01000000, name=_ray_fit, pid=744907, memory used=0.09GB) was running was 968.62GB / 1007.45GB (0.961455), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 2e0a76f8fc7bc49ac78c23c6d10df645554cfc12b3b15683d9838a05) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-2e0a76f8fc7bc49ac78c23c6d10df645554cfc12b3b15683d9838a05*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "521385\t0.91\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "522871\t0.80\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "742821\t0.38\t\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\tWarning: Exception caused NeuralNetFastAI_r95_BAG_L2 to fail during training... Skipping this model.\n",
      "\t\tTask was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: b28f6848cd6f0db33d4728bdbd16c9e5aab5fe0c01000000, name=_ray_fit, pid=744907, memory used=0.09GB) was running was 968.62GB / 1007.45GB (0.961455), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 2e0a76f8fc7bc49ac78c23c6d10df645554cfc12b3b15683d9838a05) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-2e0a76f8fc7bc49ac78c23c6d10df645554cfc12b3b15683d9838a05*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "521385\t0.91\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "522871\t0.80\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "742821\t0.38\t\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 2753, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: b28f6848cd6f0db33d4728bdbd16c9e5aab5fe0c01000000, name=_ray_fit, pid=744907, memory used=0.09GB) was running was 968.62GB / 1007.45GB (0.961455), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 2e0a76f8fc7bc49ac78c23c6d10df645554cfc12b3b15683d9838a05) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-2e0a76f8fc7bc49ac78c23c6d10df645554cfc12b3b15683d9838a05*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "521385\t0.91\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "522871\t0.80\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "742821\t0.38\t\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Fitting model: NeuralNetTorch_r41_BAG_L2 ... Training model for up to 1606.94s of the 1606.92s of remaining time.\n",
      "Fitting model: NeuralNetTorch_r41_BAG_L2 ... Training model for up to 1606.94s of the 1606.92s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.00%)\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.00%)\n",
      "\tWarning: Exception caused NeuralNetTorch_r41_BAG_L2 to fail during training... Skipping this model.\n",
      "\t\tTask was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: 2c9b6aa919db83cd5aa3884c9c2b462e021e197a01000000, name=_ray_fit, pid=745094, memory used=0.10GB) was running was 968.35GB / 1007.45GB (0.961192), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e792a77e9c0c165eb03d5d0e6e28015dcc5ce7388af5a3e5cbf4c41c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-e792a77e9c0c165eb03d5d0e6e28015dcc5ce7388af5a3e5cbf4c41c*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.05\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.80\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "745094\t0.10\tray::IDLE\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 2753, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: 2c9b6aa919db83cd5aa3884c9c2b462e021e197a01000000, name=_ray_fit, pid=745094, memory used=0.10GB) was running was 968.35GB / 1007.45GB (0.961192), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e792a77e9c0c165eb03d5d0e6e28015dcc5ce7388af5a3e5cbf4c41c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-e792a77e9c0c165eb03d5d0e6e28015dcc5ce7388af5a3e5cbf4c41c*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.05\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.80\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "745094\t0.10\tray::IDLE\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\tWarning: Exception caused NeuralNetTorch_r41_BAG_L2 to fail during training... Skipping this model.\n",
      "\t\tTask was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: 2c9b6aa919db83cd5aa3884c9c2b462e021e197a01000000, name=_ray_fit, pid=745094, memory used=0.10GB) was running was 968.35GB / 1007.45GB (0.961192), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e792a77e9c0c165eb03d5d0e6e28015dcc5ce7388af5a3e5cbf4c41c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-e792a77e9c0c165eb03d5d0e6e28015dcc5ce7388af5a3e5cbf4c41c*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.05\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.80\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "745094\t0.10\tray::IDLE\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 2753, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: 2c9b6aa919db83cd5aa3884c9c2b462e021e197a01000000, name=_ray_fit, pid=745094, memory used=0.10GB) was running was 968.35GB / 1007.45GB (0.961192), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e792a77e9c0c165eb03d5d0e6e28015dcc5ce7388af5a3e5cbf4c41c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-e792a77e9c0c165eb03d5d0e6e28015dcc5ce7388af5a3e5cbf4c41c*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.05\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.80\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "745094\t0.10\tray::IDLE\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "2025-09-30 14:36:05,888\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "Fitting model: XGBoost_r98_BAG_L2 ... Training model for up to 1606.41s of the 1606.38s of remaining time.\n",
      "2025-09-30 14:36:05,888\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "Fitting model: XGBoost_r98_BAG_L2 ... Training model for up to 1606.41s of the 1606.38s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.13%)\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.13%)\n",
      "\tWarning: Exception caused XGBoost_r98_BAG_L2 to fail during training... Skipping this model.\n",
      "\t\tTask was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: 070b3993b33230c5caf341c1e46babb5d1bcd72301000000, name=_ray_fit, pid=747295, memory used=0.11GB) was running was 967.84GB / 1007.45GB (0.96068), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d32e8f3201285d32aadf54ee634d3c1436558c5c4b235374f4e65a57) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-d32e8f3201285d32aadf54ee634d3c1436558c5c4b235374f4e65a57*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.05\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.80\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "747295\t0.11\tray::IDLE\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 2753, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: 070b3993b33230c5caf341c1e46babb5d1bcd72301000000, name=_ray_fit, pid=747295, memory used=0.11GB) was running was 967.84GB / 1007.45GB (0.96068), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d32e8f3201285d32aadf54ee634d3c1436558c5c4b235374f4e65a57) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-d32e8f3201285d32aadf54ee634d3c1436558c5c4b235374f4e65a57*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.05\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.80\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "747295\t0.11\tray::IDLE\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\tWarning: Exception caused XGBoost_r98_BAG_L2 to fail during training... Skipping this model.\n",
      "\t\tTask was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: 070b3993b33230c5caf341c1e46babb5d1bcd72301000000, name=_ray_fit, pid=747295, memory used=0.11GB) was running was 967.84GB / 1007.45GB (0.96068), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d32e8f3201285d32aadf54ee634d3c1436558c5c4b235374f4e65a57) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-d32e8f3201285d32aadf54ee634d3c1436558c5c4b235374f4e65a57*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.05\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.80\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "747295\t0.11\tray::IDLE\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 2753, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: 070b3993b33230c5caf341c1e46babb5d1bcd72301000000, name=_ray_fit, pid=747295, memory used=0.11GB) was running was 967.84GB / 1007.45GB (0.96068), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d32e8f3201285d32aadf54ee634d3c1436558c5c4b235374f4e65a57) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-d32e8f3201285d32aadf54ee634d3c1436558c5c4b235374f4e65a57*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.05\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.80\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "747295\t0.11\tray::IDLE\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Fitting model: LightGBM_r15_BAG_L2 ... Training model for up to 1595.91s of the 1595.88s of remaining time.\n",
      "Fitting model: LightGBM_r15_BAG_L2 ... Training model for up to 1595.91s of the 1595.88s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.01%)\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.01%)\n",
      "\tWarning: Exception caused LightGBM_r15_BAG_L2 to fail during training... Skipping this model.\n",
      "\t\tTask was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: a538f383a19dcba37068ab169ac227c564b4849a01000000, name=_ray_fit, pid=747296, memory used=0.10GB) was running was 967.93GB / 1007.45GB (0.96077), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 36393f8d62c081ccdfce8d23d7be715d594ae6c160713b1f490c8e9a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-36393f8d62c081ccdfce8d23d7be715d594ae6c160713b1f490c8e9a*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.58\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.05\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.80\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "747296\t0.10\tray::IDLE\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 2753, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: a538f383a19dcba37068ab169ac227c564b4849a01000000, name=_ray_fit, pid=747296, memory used=0.10GB) was running was 967.93GB / 1007.45GB (0.96077), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 36393f8d62c081ccdfce8d23d7be715d594ae6c160713b1f490c8e9a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-36393f8d62c081ccdfce8d23d7be715d594ae6c160713b1f490c8e9a*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.58\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.05\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.80\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "747296\t0.10\tray::IDLE\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\tWarning: Exception caused LightGBM_r15_BAG_L2 to fail during training... Skipping this model.\n",
      "\t\tTask was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: a538f383a19dcba37068ab169ac227c564b4849a01000000, name=_ray_fit, pid=747296, memory used=0.10GB) was running was 967.93GB / 1007.45GB (0.96077), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 36393f8d62c081ccdfce8d23d7be715d594ae6c160713b1f490c8e9a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-36393f8d62c081ccdfce8d23d7be715d594ae6c160713b1f490c8e9a*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.58\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.05\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.80\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "747296\t0.10\tray::IDLE\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 2753, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: a538f383a19dcba37068ab169ac227c564b4849a01000000, name=_ray_fit, pid=747296, memory used=0.10GB) was running was 967.93GB / 1007.45GB (0.96077), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 36393f8d62c081ccdfce8d23d7be715d594ae6c160713b1f490c8e9a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-36393f8d62c081ccdfce8d23d7be715d594ae6c160713b1f490c8e9a*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.58\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.05\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.80\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "747296\t0.10\tray::IDLE\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Fitting model: NeuralNetTorch_r158_BAG_L2 ... Training model for up to 1595.34s of the 1595.32s of remaining time.\n",
      "Fitting model: NeuralNetTorch_r158_BAG_L2 ... Training model for up to 1595.34s of the 1595.32s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.00%)\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.00%)\n",
      "\tWarning: Exception caused NeuralNetTorch_r158_BAG_L2 to fail during training... Skipping this model.\n",
      "\t\t\u001b[36mray::_ray_fit()\u001b[39m (pid=747451, ip=172.17.0.5)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 413, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 201, in _fit\n",
      "    train_dataset = self._generate_dataset(X, y, train_params=processor_kwargs, is_train=True)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 669, in _generate_dataset\n",
      "    dataset = self._process_train_data(\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 734, in _process_train_data\n",
      "    self.processor = create_preprocessor(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/utils/data_preprocessor.py\", line 40, in create_preprocessor\n",
      "    return ColumnTransformer(\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "TypeError: ColumnTransformer.__init__() got an unexpected keyword argument 'force_int_remainder_cols'\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 2753, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 904, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(TypeError): \u001b[36mray::_ray_fit()\u001b[39m (pid=747451, ip=172.17.0.5)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 413, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 201, in _fit\n",
      "    train_dataset = self._generate_dataset(X, y, train_params=processor_kwargs, is_train=True)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 669, in _generate_dataset\n",
      "    dataset = self._process_train_data(\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 734, in _process_train_data\n",
      "    self.processor = create_preprocessor(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/utils/data_preprocessor.py\", line 40, in create_preprocessor\n",
      "    return ColumnTransformer(\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "TypeError: ColumnTransformer.__init__() got an unexpected keyword argument 'force_int_remainder_cols'\n",
      "\tWarning: Exception caused NeuralNetTorch_r158_BAG_L2 to fail during training... Skipping this model.\n",
      "\t\t\u001b[36mray::_ray_fit()\u001b[39m (pid=747451, ip=172.17.0.5)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 413, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 201, in _fit\n",
      "    train_dataset = self._generate_dataset(X, y, train_params=processor_kwargs, is_train=True)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 669, in _generate_dataset\n",
      "    dataset = self._process_train_data(\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 734, in _process_train_data\n",
      "    self.processor = create_preprocessor(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/utils/data_preprocessor.py\", line 40, in create_preprocessor\n",
      "    return ColumnTransformer(\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "TypeError: ColumnTransformer.__init__() got an unexpected keyword argument 'force_int_remainder_cols'\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 2753, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 904, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(TypeError): \u001b[36mray::_ray_fit()\u001b[39m (pid=747451, ip=172.17.0.5)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 413, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 201, in _fit\n",
      "    train_dataset = self._generate_dataset(X, y, train_params=processor_kwargs, is_train=True)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 669, in _generate_dataset\n",
      "    dataset = self._process_train_data(\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 734, in _process_train_data\n",
      "    self.processor = create_preprocessor(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/utils/data_preprocessor.py\", line 40, in create_preprocessor\n",
      "    return ColumnTransformer(\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "TypeError: ColumnTransformer.__init__() got an unexpected keyword argument 'force_int_remainder_cols'\n",
      "Fitting model: CatBoost_r86_BAG_L2 ... Training model for up to 1592.06s of the 1592.04s of remaining time.\n",
      "Fitting model: CatBoost_r86_BAG_L2 ... Training model for up to 1592.06s of the 1592.04s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.13%)\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.13%)\n",
      "\tWarning: Exception caused CatBoost_r86_BAG_L2 to fail during training... Skipping this model.\n",
      "\t\tTask was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: 109c214e656416856455d5f68905f612f86a783401000000, name=_ray_fit, pid=748386, memory used=0.08GB) was running was 968.65GB / 1007.45GB (0.961483), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 5e973fb7a82d6c7d259809aa602aca4ac281e8562fd0cc36adfd01fc) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-5e973fb7a82d6c7d259809aa602aca4ac281e8562fd0cc36adfd01fc*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.58\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "521385\t0.91\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "522871\t0.80\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "747451\t0.32\tray::IDLE\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 2753, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: 109c214e656416856455d5f68905f612f86a783401000000, name=_ray_fit, pid=748386, memory used=0.08GB) was running was 968.65GB / 1007.45GB (0.961483), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 5e973fb7a82d6c7d259809aa602aca4ac281e8562fd0cc36adfd01fc) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-5e973fb7a82d6c7d259809aa602aca4ac281e8562fd0cc36adfd01fc*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.58\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "521385\t0.91\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "522871\t0.80\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "747451\t0.32\tray::IDLE\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\tWarning: Exception caused CatBoost_r86_BAG_L2 to fail during training... Skipping this model.\n",
      "\t\tTask was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: 109c214e656416856455d5f68905f612f86a783401000000, name=_ray_fit, pid=748386, memory used=0.08GB) was running was 968.65GB / 1007.45GB (0.961483), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 5e973fb7a82d6c7d259809aa602aca4ac281e8562fd0cc36adfd01fc) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-5e973fb7a82d6c7d259809aa602aca4ac281e8562fd0cc36adfd01fc*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.58\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "521385\t0.91\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "522871\t0.80\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "747451\t0.32\tray::IDLE\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 2753, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: 109c214e656416856455d5f68905f612f86a783401000000, name=_ray_fit, pid=748386, memory used=0.08GB) was running was 968.65GB / 1007.45GB (0.961483), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 5e973fb7a82d6c7d259809aa602aca4ac281e8562fd0cc36adfd01fc) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-5e973fb7a82d6c7d259809aa602aca4ac281e8562fd0cc36adfd01fc*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.58\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "521385\t0.91\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "522871\t0.80\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "747451\t0.32\tray::IDLE\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "2025-09-30 14:36:20,951\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-09-30 14:36:20,952\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-09-30 14:36:20,953\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "Fitting model: NeuralNetFastAI_r37_BAG_L2 ... Training model for up to 1591.34s of the 1591.32s of remaining time.\n",
      "2025-09-30 14:36:20,951\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-09-30 14:36:20,952\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-09-30 14:36:20,953\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "Fitting model: NeuralNetFastAI_r37_BAG_L2 ... Training model for up to 1591.34s of the 1591.32s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.00%)\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.00%)\n",
      "\tWarning: Exception caused NeuralNetFastAI_r37_BAG_L2 to fail during training... Skipping this model.\n",
      "\t\tTask was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: a4e4e6a4542a3133640cf72ba3f37566c29f66f201000000, name=_ray_fit, pid=749084, memory used=0.10GB) was running was 968.14GB / 1007.45GB (0.960978), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 04386f5d1fce1bea03e7c7b3d69964a8bad05b28f700333f802708fa) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-04386f5d1fce1bea03e7c7b3d69964a8bad05b28f700333f802708fa*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.58\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.05\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.80\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "749084\t0.10\tray::IDLE\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 2753, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: a4e4e6a4542a3133640cf72ba3f37566c29f66f201000000, name=_ray_fit, pid=749084, memory used=0.10GB) was running was 968.14GB / 1007.45GB (0.960978), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 04386f5d1fce1bea03e7c7b3d69964a8bad05b28f700333f802708fa) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-04386f5d1fce1bea03e7c7b3d69964a8bad05b28f700333f802708fa*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.58\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.05\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.80\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "749084\t0.10\tray::IDLE\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\tWarning: Exception caused NeuralNetFastAI_r37_BAG_L2 to fail during training... Skipping this model.\n",
      "\t\tTask was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: a4e4e6a4542a3133640cf72ba3f37566c29f66f201000000, name=_ray_fit, pid=749084, memory used=0.10GB) was running was 968.14GB / 1007.45GB (0.960978), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 04386f5d1fce1bea03e7c7b3d69964a8bad05b28f700333f802708fa) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-04386f5d1fce1bea03e7c7b3d69964a8bad05b28f700333f802708fa*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.58\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.05\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.80\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "749084\t0.10\tray::IDLE\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 2753, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: a4e4e6a4542a3133640cf72ba3f37566c29f66f201000000, name=_ray_fit, pid=749084, memory used=0.10GB) was running was 968.14GB / 1007.45GB (0.960978), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 04386f5d1fce1bea03e7c7b3d69964a8bad05b28f700333f802708fa) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-04386f5d1fce1bea03e7c7b3d69964a8bad05b28f700333f802708fa*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.58\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.05\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.80\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "749084\t0.10\tray::IDLE\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "2025-09-30 14:36:21,477\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-09-30 14:36:21,478\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-09-30 14:36:21,479\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-09-30 14:36:21,480\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-09-30 14:36:21,480\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-09-30 14:36:21,481\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-09-30 14:36:21,481\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "Fitting model: NeuralNetTorch_r197_BAG_L2 ... Training model for up to 1590.81s of the 1590.79s of remaining time.\n",
      "2025-09-30 14:36:21,477\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-09-30 14:36:21,478\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-09-30 14:36:21,479\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-09-30 14:36:21,480\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-09-30 14:36:21,480\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-09-30 14:36:21,481\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-09-30 14:36:21,481\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "Fitting model: NeuralNetTorch_r197_BAG_L2 ... Training model for up to 1590.81s of the 1590.79s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.00%)\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.00%)\n",
      "\tWarning: Exception caused NeuralNetTorch_r197_BAG_L2 to fail during training... Skipping this model.\n",
      "\t\t\u001b[36mray::_ray_fit()\u001b[39m (pid=750045, ip=172.17.0.5)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 413, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 201, in _fit\n",
      "    train_dataset = self._generate_dataset(X, y, train_params=processor_kwargs, is_train=True)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 669, in _generate_dataset\n",
      "    dataset = self._process_train_data(\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 734, in _process_train_data\n",
      "    self.processor = create_preprocessor(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/utils/data_preprocessor.py\", line 40, in create_preprocessor\n",
      "    return ColumnTransformer(\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "TypeError: ColumnTransformer.__init__() got an unexpected keyword argument 'force_int_remainder_cols'\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 2753, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 904, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(TypeError): \u001b[36mray::_ray_fit()\u001b[39m (pid=750045, ip=172.17.0.5)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 413, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 201, in _fit\n",
      "    train_dataset = self._generate_dataset(X, y, train_params=processor_kwargs, is_train=True)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 669, in _generate_dataset\n",
      "    dataset = self._process_train_data(\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 734, in _process_train_data\n",
      "    self.processor = create_preprocessor(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/utils/data_preprocessor.py\", line 40, in create_preprocessor\n",
      "    return ColumnTransformer(\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "TypeError: ColumnTransformer.__init__() got an unexpected keyword argument 'force_int_remainder_cols'\n",
      "\tWarning: Exception caused NeuralNetTorch_r197_BAG_L2 to fail during training... Skipping this model.\n",
      "\t\t\u001b[36mray::_ray_fit()\u001b[39m (pid=750045, ip=172.17.0.5)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 413, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 201, in _fit\n",
      "    train_dataset = self._generate_dataset(X, y, train_params=processor_kwargs, is_train=True)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 669, in _generate_dataset\n",
      "    dataset = self._process_train_data(\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 734, in _process_train_data\n",
      "    self.processor = create_preprocessor(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/utils/data_preprocessor.py\", line 40, in create_preprocessor\n",
      "    return ColumnTransformer(\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "TypeError: ColumnTransformer.__init__() got an unexpected keyword argument 'force_int_remainder_cols'\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 2753, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 904, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(TypeError): \u001b[36mray::_ray_fit()\u001b[39m (pid=750045, ip=172.17.0.5)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 413, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 201, in _fit\n",
      "    train_dataset = self._generate_dataset(X, y, train_params=processor_kwargs, is_train=True)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 669, in _generate_dataset\n",
      "    dataset = self._process_train_data(\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 734, in _process_train_data\n",
      "    self.processor = create_preprocessor(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/utils/data_preprocessor.py\", line 40, in create_preprocessor\n",
      "    return ColumnTransformer(\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "TypeError: ColumnTransformer.__init__() got an unexpected keyword argument 'force_int_remainder_cols'\n",
      "Fitting model: CatBoost_r49_BAG_L2 ... Training model for up to 1587.73s of the 1587.70s of remaining time.\n",
      "Fitting model: CatBoost_r49_BAG_L2 ... Training model for up to 1587.73s of the 1587.70s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.01%)\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.01%)\n",
      "\tWarning: Exception caused CatBoost_r49_BAG_L2 to fail during training... Skipping this model.\n",
      "\t\tTask was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: f8c9bc211ebe48434b11dd78fbeec8bbf4051aea01000000, name=_ray_fit, pid=751294, memory used=0.24GB) was running was 968.82GB / 1007.45GB (0.961651), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: bfa0c9316995ce1289a301f45f7e6ef23f574177852ebf06d724241f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-bfa0c9316995ce1289a301f45f7e6ef23f574177852ebf06d724241f*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.58\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.05\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.80\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "751294\t0.24\tray::_ray_fit\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 2753, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: f8c9bc211ebe48434b11dd78fbeec8bbf4051aea01000000, name=_ray_fit, pid=751294, memory used=0.24GB) was running was 968.82GB / 1007.45GB (0.961651), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: bfa0c9316995ce1289a301f45f7e6ef23f574177852ebf06d724241f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-bfa0c9316995ce1289a301f45f7e6ef23f574177852ebf06d724241f*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.58\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.05\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.80\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "751294\t0.24\tray::_ray_fit\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\tWarning: Exception caused CatBoost_r49_BAG_L2 to fail during training... Skipping this model.\n",
      "\t\tTask was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: f8c9bc211ebe48434b11dd78fbeec8bbf4051aea01000000, name=_ray_fit, pid=751294, memory used=0.24GB) was running was 968.82GB / 1007.45GB (0.961651), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: bfa0c9316995ce1289a301f45f7e6ef23f574177852ebf06d724241f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-bfa0c9316995ce1289a301f45f7e6ef23f574177852ebf06d724241f*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.58\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.05\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.80\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "751294\t0.24\tray::_ray_fit\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 2753, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: f8c9bc211ebe48434b11dd78fbeec8bbf4051aea01000000, name=_ray_fit, pid=751294, memory used=0.24GB) was running was 968.82GB / 1007.45GB (0.961651), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: bfa0c9316995ce1289a301f45f7e6ef23f574177852ebf06d724241f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-bfa0c9316995ce1289a301f45f7e6ef23f574177852ebf06d724241f*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.58\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.05\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.80\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "751294\t0.24\tray::_ray_fit\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "2025-09-30 14:36:29,493\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-09-30 14:36:29,494\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-09-30 14:36:29,495\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "Fitting model: ExtraTrees_r49_BAG_L2 ... Training model for up to 1582.80s of the 1582.78s of remaining time.\n",
      "2025-09-30 14:36:29,493\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-09-30 14:36:29,494\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-09-30 14:36:29,495\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "Fitting model: ExtraTrees_r49_BAG_L2 ... Training model for up to 1582.80s of the 1582.78s of remaining time.\n",
      "\t-0.1105\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.34s\t = Training   runtime\n",
      "\t0.09s\t = Validation runtime\n",
      "Fitting model: LightGBM_r143_BAG_L2 ... Training model for up to 1582.33s of the 1582.31s of remaining time.\n",
      "\t-0.1105\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.34s\t = Training   runtime\n",
      "\t0.09s\t = Validation runtime\n",
      "Fitting model: LightGBM_r143_BAG_L2 ... Training model for up to 1582.33s of the 1582.31s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.09%)\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.09%)\n",
      "\tWarning: Exception caused LightGBM_r143_BAG_L2 to fail during training... Skipping this model.\n",
      "\t\tTask was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: f68e20f19d130d322b2386aeeacc6b3625b0b26a01000000, name=_ray_fit, pid=754862, memory used=0.11GB) was running was 969.35GB / 1007.45GB (0.962181), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4c9972325122b784349bb0e6a4f46b6eb886b2327867e1c21ec99c73) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-4c9972325122b784349bb0e6a4f46b6eb886b2327867e1c21ec99c73*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.58\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.05\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.80\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "754862\t0.11\tray::IDLE\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 2753, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: f68e20f19d130d322b2386aeeacc6b3625b0b26a01000000, name=_ray_fit, pid=754862, memory used=0.11GB) was running was 969.35GB / 1007.45GB (0.962181), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4c9972325122b784349bb0e6a4f46b6eb886b2327867e1c21ec99c73) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-4c9972325122b784349bb0e6a4f46b6eb886b2327867e1c21ec99c73*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.58\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.05\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.80\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "754862\t0.11\tray::IDLE\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\tWarning: Exception caused LightGBM_r143_BAG_L2 to fail during training... Skipping this model.\n",
      "\t\tTask was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: f68e20f19d130d322b2386aeeacc6b3625b0b26a01000000, name=_ray_fit, pid=754862, memory used=0.11GB) was running was 969.35GB / 1007.45GB (0.962181), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4c9972325122b784349bb0e6a4f46b6eb886b2327867e1c21ec99c73) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-4c9972325122b784349bb0e6a4f46b6eb886b2327867e1c21ec99c73*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.58\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.05\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.80\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "754862\t0.11\tray::IDLE\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 2753, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: f68e20f19d130d322b2386aeeacc6b3625b0b26a01000000, name=_ray_fit, pid=754862, memory used=0.11GB) was running was 969.35GB / 1007.45GB (0.962181), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4c9972325122b784349bb0e6a4f46b6eb886b2327867e1c21ec99c73) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-4c9972325122b784349bb0e6a4f46b6eb886b2327867e1c21ec99c73*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.58\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.05\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.80\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "754862\t0.11\tray::IDLE\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Fitting model: RandomForest_r127_BAG_L2 ... Training model for up to 1575.77s of the 1575.74s of remaining time.\n",
      "Fitting model: RandomForest_r127_BAG_L2 ... Training model for up to 1575.77s of the 1575.74s of remaining time.\n",
      "\t-0.1258\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.39s\t = Training   runtime\n",
      "\t0.1s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_r134_BAG_L2 ... Training model for up to 1575.22s of the 1575.20s of remaining time.\n",
      "\t-0.1258\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.39s\t = Training   runtime\n",
      "\t0.1s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_r134_BAG_L2 ... Training model for up to 1575.22s of the 1575.20s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.00%)\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.00%)\n",
      "\tWarning: Exception caused NeuralNetFastAI_r134_BAG_L2 to fail during training... Skipping this model.\n",
      "\t\tTask was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: 0b4d68d5834ee396698a49cbf9246adcca7e104401000000, name=_ray_fit, pid=756976, memory used=0.23GB) was running was 969.52GB / 1007.45GB (0.962349), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 365f715887daa2bbe792aa995b59f41abeede1a0b952f809036437a9) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-365f715887daa2bbe792aa995b59f41abeede1a0b952f809036437a9*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.58\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.01\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.80\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "756976\t0.23\tray::_ray_fit\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 2753, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: 0b4d68d5834ee396698a49cbf9246adcca7e104401000000, name=_ray_fit, pid=756976, memory used=0.23GB) was running was 969.52GB / 1007.45GB (0.962349), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 365f715887daa2bbe792aa995b59f41abeede1a0b952f809036437a9) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-365f715887daa2bbe792aa995b59f41abeede1a0b952f809036437a9*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.58\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.01\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.80\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "756976\t0.23\tray::_ray_fit\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\tWarning: Exception caused NeuralNetFastAI_r134_BAG_L2 to fail during training... Skipping this model.\n",
      "\t\tTask was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: 0b4d68d5834ee396698a49cbf9246adcca7e104401000000, name=_ray_fit, pid=756976, memory used=0.23GB) was running was 969.52GB / 1007.45GB (0.962349), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 365f715887daa2bbe792aa995b59f41abeede1a0b952f809036437a9) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-365f715887daa2bbe792aa995b59f41abeede1a0b952f809036437a9*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.58\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.01\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.80\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "756976\t0.23\tray::_ray_fit\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 2753, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: 0b4d68d5834ee396698a49cbf9246adcca7e104401000000, name=_ray_fit, pid=756976, memory used=0.23GB) was running was 969.52GB / 1007.45GB (0.962349), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 365f715887daa2bbe792aa995b59f41abeede1a0b952f809036437a9) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-365f715887daa2bbe792aa995b59f41abeede1a0b952f809036437a9*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.58\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.01\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.80\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "756976\t0.23\tray::_ray_fit\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Fitting model: RandomForest_r34_BAG_L2 ... Training model for up to 1565.19s of the 1565.16s of remaining time.\n",
      "Fitting model: RandomForest_r34_BAG_L2 ... Training model for up to 1565.19s of the 1565.16s of remaining time.\n",
      "\t-0.1731\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.26s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBM_r94_BAG_L2 ... Training model for up to 1564.81s of the 1564.79s of remaining time.\n",
      "\t-0.1731\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.26s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBM_r94_BAG_L2 ... Training model for up to 1564.81s of the 1564.79s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.01%)\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.01%)\n",
      "\tWarning: Exception caused LightGBM_r94_BAG_L2 to fail during training... Skipping this model.\n",
      "\t\tTask was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: c641991d91abdf86164d72c7fa2ba49151de19db01000000, name=_ray_fit, pid=758621, memory used=0.18GB) was running was 969.63GB / 1007.45GB (0.962456), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 1789d0b31d5e34b834a0573c0d7352b85a66f17038ec97b4e80e4456) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-1789d0b31d5e34b834a0573c0d7352b85a66f17038ec97b4e80e4456*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.58\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.05\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.80\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "758621\t0.18\tray::_ray_fit\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 2753, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: c641991d91abdf86164d72c7fa2ba49151de19db01000000, name=_ray_fit, pid=758621, memory used=0.18GB) was running was 969.63GB / 1007.45GB (0.962456), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 1789d0b31d5e34b834a0573c0d7352b85a66f17038ec97b4e80e4456) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-1789d0b31d5e34b834a0573c0d7352b85a66f17038ec97b4e80e4456*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.58\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.05\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.80\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "758621\t0.18\tray::_ray_fit\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\tWarning: Exception caused LightGBM_r94_BAG_L2 to fail during training... Skipping this model.\n",
      "\t\tTask was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: c641991d91abdf86164d72c7fa2ba49151de19db01000000, name=_ray_fit, pid=758621, memory used=0.18GB) was running was 969.63GB / 1007.45GB (0.962456), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 1789d0b31d5e34b834a0573c0d7352b85a66f17038ec97b4e80e4456) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-1789d0b31d5e34b834a0573c0d7352b85a66f17038ec97b4e80e4456*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.58\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.05\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.80\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "758621\t0.18\tray::_ray_fit\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 2753, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: c641991d91abdf86164d72c7fa2ba49151de19db01000000, name=_ray_fit, pid=758621, memory used=0.18GB) was running was 969.63GB / 1007.45GB (0.962456), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 1789d0b31d5e34b834a0573c0d7352b85a66f17038ec97b4e80e4456) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-1789d0b31d5e34b834a0573c0d7352b85a66f17038ec97b4e80e4456*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.58\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.05\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.80\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "758621\t0.18\tray::_ray_fit\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "2025-09-30 14:36:52,601\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "Fitting model: NeuralNetTorch_r143_BAG_L2 ... Training model for up to 1559.69s of the 1559.67s of remaining time.\n",
      "2025-09-30 14:36:52,601\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "Fitting model: NeuralNetTorch_r143_BAG_L2 ... Training model for up to 1559.69s of the 1559.67s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.00%)\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.00%)\n",
      "\tWarning: Exception caused NeuralNetTorch_r143_BAG_L2 to fail during training... Skipping this model.\n",
      "\t\t\u001b[36mray::_ray_fit()\u001b[39m (pid=759053, ip=172.17.0.5)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 413, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 201, in _fit\n",
      "    train_dataset = self._generate_dataset(X, y, train_params=processor_kwargs, is_train=True)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 669, in _generate_dataset\n",
      "    dataset = self._process_train_data(\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 734, in _process_train_data\n",
      "    self.processor = create_preprocessor(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/utils/data_preprocessor.py\", line 40, in create_preprocessor\n",
      "    return ColumnTransformer(\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "TypeError: ColumnTransformer.__init__() got an unexpected keyword argument 'force_int_remainder_cols'\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 2753, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 904, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(TypeError): \u001b[36mray::_ray_fit()\u001b[39m (pid=759053, ip=172.17.0.5)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 413, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 201, in _fit\n",
      "    train_dataset = self._generate_dataset(X, y, train_params=processor_kwargs, is_train=True)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 669, in _generate_dataset\n",
      "    dataset = self._process_train_data(\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 734, in _process_train_data\n",
      "    self.processor = create_preprocessor(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/utils/data_preprocessor.py\", line 40, in create_preprocessor\n",
      "    return ColumnTransformer(\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "TypeError: ColumnTransformer.__init__() got an unexpected keyword argument 'force_int_remainder_cols'\n",
      "\tWarning: Exception caused NeuralNetTorch_r143_BAG_L2 to fail during training... Skipping this model.\n",
      "\t\t\u001b[36mray::_ray_fit()\u001b[39m (pid=759053, ip=172.17.0.5)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 413, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 201, in _fit\n",
      "    train_dataset = self._generate_dataset(X, y, train_params=processor_kwargs, is_train=True)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 669, in _generate_dataset\n",
      "    dataset = self._process_train_data(\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 734, in _process_train_data\n",
      "    self.processor = create_preprocessor(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/utils/data_preprocessor.py\", line 40, in create_preprocessor\n",
      "    return ColumnTransformer(\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "TypeError: ColumnTransformer.__init__() got an unexpected keyword argument 'force_int_remainder_cols'\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 2753, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 904, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(TypeError): \u001b[36mray::_ray_fit()\u001b[39m (pid=759053, ip=172.17.0.5)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 413, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 201, in _fit\n",
      "    train_dataset = self._generate_dataset(X, y, train_params=processor_kwargs, is_train=True)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 669, in _generate_dataset\n",
      "    dataset = self._process_train_data(\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 734, in _process_train_data\n",
      "    self.processor = create_preprocessor(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/utils/data_preprocessor.py\", line 40, in create_preprocessor\n",
      "    return ColumnTransformer(\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "TypeError: ColumnTransformer.__init__() got an unexpected keyword argument 'force_int_remainder_cols'\n",
      "Fitting model: CatBoost_r128_BAG_L2 ... Training model for up to 1556.29s of the 1556.27s of remaining time.\n",
      "Fitting model: CatBoost_r128_BAG_L2 ... Training model for up to 1556.29s of the 1556.27s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.14%)\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.14%)\n",
      "2025-09-30 14:37:01,504\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-09-30 14:37:01,506\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-09-30 14:37:01,504\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-09-30 14:37:01,506\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "\tWarning: Exception caused CatBoost_r128_BAG_L2 to fail during training... Skipping this model.\n",
      "\t\tTask was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: 480d0e9c3bd80f66429a85b3ab53ccd5224f6a0001000000, name=_ray_fit, pid=760503, memory used=0.26GB) was running was 970.06GB / 1007.45GB (0.962881), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ee70fc1609a84a530a3c16f8c3c0edc7624ac96edf118219ec3641cb) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-ee70fc1609a84a530a3c16f8c3c0edc7624ac96edf118219ec3641cb*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.58\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.05\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.80\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "760503\t0.26\tray::_ray_fit\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 2753, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: 480d0e9c3bd80f66429a85b3ab53ccd5224f6a0001000000, name=_ray_fit, pid=760503, memory used=0.26GB) was running was 970.06GB / 1007.45GB (0.962881), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ee70fc1609a84a530a3c16f8c3c0edc7624ac96edf118219ec3641cb) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-ee70fc1609a84a530a3c16f8c3c0edc7624ac96edf118219ec3641cb*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.58\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.05\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.80\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "760503\t0.26\tray::_ray_fit\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\tWarning: Exception caused CatBoost_r128_BAG_L2 to fail during training... Skipping this model.\n",
      "\t\tTask was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: 480d0e9c3bd80f66429a85b3ab53ccd5224f6a0001000000, name=_ray_fit, pid=760503, memory used=0.26GB) was running was 970.06GB / 1007.45GB (0.962881), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ee70fc1609a84a530a3c16f8c3c0edc7624ac96edf118219ec3641cb) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-ee70fc1609a84a530a3c16f8c3c0edc7624ac96edf118219ec3641cb*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.58\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.05\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.80\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "760503\t0.26\tray::_ray_fit\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 2753, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: 480d0e9c3bd80f66429a85b3ab53ccd5224f6a0001000000, name=_ray_fit, pid=760503, memory used=0.26GB) was running was 970.06GB / 1007.45GB (0.962881), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ee70fc1609a84a530a3c16f8c3c0edc7624ac96edf118219ec3641cb) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-ee70fc1609a84a530a3c16f8c3c0edc7624ac96edf118219ec3641cb*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.58\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.05\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.80\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "760503\t0.26\tray::_ray_fit\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Fitting model: NeuralNetFastAI_r111_BAG_L2 ... Training model for up to 1550.60s of the 1550.58s of remaining time.\n",
      "Fitting model: NeuralNetFastAI_r111_BAG_L2 ... Training model for up to 1550.60s of the 1550.58s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.00%)\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.00%)\n",
      "\tWarning: Exception caused NeuralNetFastAI_r111_BAG_L2 to fail during training... Skipping this model.\n",
      "\t\tTask was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: f7fc288288694ff883da11ec79b2ac4d99ba285e01000000, name=_ray_fit, pid=763891, memory used=0.12GB) was running was 970.32GB / 1007.45GB (0.963143), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7d7aac38ec2f23bb9720515b6dea3cc8b3c943e257b3790ba1ba500c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-7d7aac38ec2f23bb9720515b6dea3cc8b3c943e257b3790ba1ba500c*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.58\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.05\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.80\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "763891\t0.12\tray::IDLE\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 2753, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: f7fc288288694ff883da11ec79b2ac4d99ba285e01000000, name=_ray_fit, pid=763891, memory used=0.12GB) was running was 970.32GB / 1007.45GB (0.963143), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7d7aac38ec2f23bb9720515b6dea3cc8b3c943e257b3790ba1ba500c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-7d7aac38ec2f23bb9720515b6dea3cc8b3c943e257b3790ba1ba500c*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.58\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.05\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.80\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "763891\t0.12\tray::IDLE\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\tWarning: Exception caused NeuralNetFastAI_r111_BAG_L2 to fail during training... Skipping this model.\n",
      "\t\tTask was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: f7fc288288694ff883da11ec79b2ac4d99ba285e01000000, name=_ray_fit, pid=763891, memory used=0.12GB) was running was 970.32GB / 1007.45GB (0.963143), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7d7aac38ec2f23bb9720515b6dea3cc8b3c943e257b3790ba1ba500c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-7d7aac38ec2f23bb9720515b6dea3cc8b3c943e257b3790ba1ba500c*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.58\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.05\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.80\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "763891\t0.12\tray::IDLE\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 2753, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: f7fc288288694ff883da11ec79b2ac4d99ba285e01000000, name=_ray_fit, pid=763891, memory used=0.12GB) was running was 970.32GB / 1007.45GB (0.963143), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7d7aac38ec2f23bb9720515b6dea3cc8b3c943e257b3790ba1ba500c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-7d7aac38ec2f23bb9720515b6dea3cc8b3c943e257b3790ba1ba500c*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.58\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.05\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.80\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "763891\t0.12\tray::IDLE\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Fitting model: NeuralNetTorch_r31_BAG_L2 ... Training model for up to 1542.48s of the 1542.45s of remaining time.\n",
      "Fitting model: NeuralNetTorch_r31_BAG_L2 ... Training model for up to 1542.48s of the 1542.45s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.00%)\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.00%)\n",
      "\tWarning: Exception caused NeuralNetTorch_r31_BAG_L2 to fail during training... Skipping this model.\n",
      "\t\t\u001b[36mray::_ray_fit()\u001b[39m (pid=764180, ip=172.17.0.5)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 413, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 201, in _fit\n",
      "    train_dataset = self._generate_dataset(X, y, train_params=processor_kwargs, is_train=True)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 669, in _generate_dataset\n",
      "    dataset = self._process_train_data(\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 734, in _process_train_data\n",
      "    self.processor = create_preprocessor(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/utils/data_preprocessor.py\", line 40, in create_preprocessor\n",
      "    return ColumnTransformer(\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "TypeError: ColumnTransformer.__init__() got an unexpected keyword argument 'force_int_remainder_cols'\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 2753, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 904, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(TypeError): \u001b[36mray::_ray_fit()\u001b[39m (pid=764180, ip=172.17.0.5)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 413, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 201, in _fit\n",
      "    train_dataset = self._generate_dataset(X, y, train_params=processor_kwargs, is_train=True)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 669, in _generate_dataset\n",
      "    dataset = self._process_train_data(\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 734, in _process_train_data\n",
      "    self.processor = create_preprocessor(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/utils/data_preprocessor.py\", line 40, in create_preprocessor\n",
      "    return ColumnTransformer(\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "TypeError: ColumnTransformer.__init__() got an unexpected keyword argument 'force_int_remainder_cols'\n",
      "\tWarning: Exception caused NeuralNetTorch_r31_BAG_L2 to fail during training... Skipping this model.\n",
      "\t\t\u001b[36mray::_ray_fit()\u001b[39m (pid=764180, ip=172.17.0.5)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 413, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 201, in _fit\n",
      "    train_dataset = self._generate_dataset(X, y, train_params=processor_kwargs, is_train=True)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 669, in _generate_dataset\n",
      "    dataset = self._process_train_data(\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 734, in _process_train_data\n",
      "    self.processor = create_preprocessor(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/utils/data_preprocessor.py\", line 40, in create_preprocessor\n",
      "    return ColumnTransformer(\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "TypeError: ColumnTransformer.__init__() got an unexpected keyword argument 'force_int_remainder_cols'\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 2753, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 904, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(TypeError): \u001b[36mray::_ray_fit()\u001b[39m (pid=764180, ip=172.17.0.5)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 413, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 201, in _fit\n",
      "    train_dataset = self._generate_dataset(X, y, train_params=processor_kwargs, is_train=True)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 669, in _generate_dataset\n",
      "    dataset = self._process_train_data(\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 734, in _process_train_data\n",
      "    self.processor = create_preprocessor(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/utils/data_preprocessor.py\", line 40, in create_preprocessor\n",
      "    return ColumnTransformer(\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "TypeError: ColumnTransformer.__init__() got an unexpected keyword argument 'force_int_remainder_cols'\n",
      "Fitting model: ExtraTrees_r4_BAG_L2 ... Training model for up to 1538.93s of the 1538.91s of remaining time.\n",
      "Fitting model: ExtraTrees_r4_BAG_L2 ... Training model for up to 1538.93s of the 1538.91s of remaining time.\n",
      "\t-0.1376\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.3s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_r65_BAG_L2 ... Training model for up to 1538.51s of the 1538.48s of remaining time.\n",
      "\t-0.1376\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.3s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_r65_BAG_L2 ... Training model for up to 1538.51s of the 1538.48s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.00%)\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.00%)\n",
      "\tWarning: Exception caused NeuralNetFastAI_r65_BAG_L2 to fail during training... Skipping this model.\n",
      "\t\tTask was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: 7b856fce5fb29e86852702075ff8fca2eaeeed4a01000000, name=_ray_fit, pid=764299, memory used=0.11GB) was running was 969.84GB / 1007.45GB (0.962667), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c33c844673f2b96444f013b66cc0cafa39d7e3f3c4debac2aff8127c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-c33c844673f2b96444f013b66cc0cafa39d7e3f3c4debac2aff8127c*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.58\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.05\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.80\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "764299\t0.11\tray::IDLE\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 2753, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: 7b856fce5fb29e86852702075ff8fca2eaeeed4a01000000, name=_ray_fit, pid=764299, memory used=0.11GB) was running was 969.84GB / 1007.45GB (0.962667), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c33c844673f2b96444f013b66cc0cafa39d7e3f3c4debac2aff8127c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-c33c844673f2b96444f013b66cc0cafa39d7e3f3c4debac2aff8127c*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.58\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.05\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.80\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "764299\t0.11\tray::IDLE\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\tWarning: Exception caused NeuralNetFastAI_r65_BAG_L2 to fail during training... Skipping this model.\n",
      "\t\tTask was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: 7b856fce5fb29e86852702075ff8fca2eaeeed4a01000000, name=_ray_fit, pid=764299, memory used=0.11GB) was running was 969.84GB / 1007.45GB (0.962667), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c33c844673f2b96444f013b66cc0cafa39d7e3f3c4debac2aff8127c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-c33c844673f2b96444f013b66cc0cafa39d7e3f3c4debac2aff8127c*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.58\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.05\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.80\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "764299\t0.11\tray::IDLE\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 2753, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: 7b856fce5fb29e86852702075ff8fca2eaeeed4a01000000, name=_ray_fit, pid=764299, memory used=0.11GB) was running was 969.84GB / 1007.45GB (0.962667), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c33c844673f2b96444f013b66cc0cafa39d7e3f3c4debac2aff8127c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-c33c844673f2b96444f013b66cc0cafa39d7e3f3c4debac2aff8127c*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.58\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.05\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.80\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "764299\t0.11\tray::IDLE\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "2025-09-30 14:37:14,322\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-09-30 14:37:14,324\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-09-30 14:37:14,325\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "Fitting model: NeuralNetFastAI_r88_BAG_L2 ... Training model for up to 1537.97s of the 1537.95s of remaining time.\n",
      "2025-09-30 14:37:14,322\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-09-30 14:37:14,324\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-09-30 14:37:14,325\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "Fitting model: NeuralNetFastAI_r88_BAG_L2 ... Training model for up to 1537.97s of the 1537.95s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.00%)\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.00%)\n",
      "\tWarning: Exception caused NeuralNetFastAI_r88_BAG_L2 to fail during training... Skipping this model.\n",
      "\t\tTask was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: 387775cae34f82807d34e4027257463fa1b092fa01000000, name=_ray_fit, pid=767529, memory used=0.07GB) was running was 970.13GB / 1007.45GB (0.962956), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 1077713a50a406e2ae4a591d4b017aa9abc8f89c613223f68625d02f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-1077713a50a406e2ae4a591d4b017aa9abc8f89c613223f68625d02f*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.58\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "521385\t0.91\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "522871\t0.81\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "765673\t0.37\tray::IDLE\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 2753, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: 387775cae34f82807d34e4027257463fa1b092fa01000000, name=_ray_fit, pid=767529, memory used=0.07GB) was running was 970.13GB / 1007.45GB (0.962956), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 1077713a50a406e2ae4a591d4b017aa9abc8f89c613223f68625d02f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-1077713a50a406e2ae4a591d4b017aa9abc8f89c613223f68625d02f*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.58\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "521385\t0.91\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "522871\t0.81\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "765673\t0.37\tray::IDLE\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\tWarning: Exception caused NeuralNetFastAI_r88_BAG_L2 to fail during training... Skipping this model.\n",
      "\t\tTask was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: 387775cae34f82807d34e4027257463fa1b092fa01000000, name=_ray_fit, pid=767529, memory used=0.07GB) was running was 970.13GB / 1007.45GB (0.962956), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 1077713a50a406e2ae4a591d4b017aa9abc8f89c613223f68625d02f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-1077713a50a406e2ae4a591d4b017aa9abc8f89c613223f68625d02f*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.58\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "521385\t0.91\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "522871\t0.81\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "765673\t0.37\tray::IDLE\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 2753, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: 387775cae34f82807d34e4027257463fa1b092fa01000000, name=_ray_fit, pid=767529, memory used=0.07GB) was running was 970.13GB / 1007.45GB (0.962956), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 1077713a50a406e2ae4a591d4b017aa9abc8f89c613223f68625d02f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-1077713a50a406e2ae4a591d4b017aa9abc8f89c613223f68625d02f*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.58\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "521385\t0.91\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "522871\t0.81\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "765673\t0.37\tray::IDLE\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Fitting model: LightGBM_r30_BAG_L2 ... Training model for up to 1530.33s of the 1530.31s of remaining time.\n",
      "Fitting model: LightGBM_r30_BAG_L2 ... Training model for up to 1530.33s of the 1530.31s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.06%)\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.06%)\n",
      "\tWarning: Exception caused LightGBM_r30_BAG_L2 to fail during training... Skipping this model.\n",
      "\t\tTask was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: 6514add9ea2964bbfb39e51f74c3d4cda7b534c601000000, name=_ray_fit, pid=768847, memory used=0.12GB) was running was 969.88GB / 1007.45GB (0.962711), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 97f9cc9a0ea137a3a585f09cfe49ed95eabd2961665b320e1f6940c4) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-97f9cc9a0ea137a3a585f09cfe49ed95eabd2961665b320e1f6940c4*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.58\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.05\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.81\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "768847\t0.12\tray::IDLE\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 2753, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: 6514add9ea2964bbfb39e51f74c3d4cda7b534c601000000, name=_ray_fit, pid=768847, memory used=0.12GB) was running was 969.88GB / 1007.45GB (0.962711), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 97f9cc9a0ea137a3a585f09cfe49ed95eabd2961665b320e1f6940c4) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-97f9cc9a0ea137a3a585f09cfe49ed95eabd2961665b320e1f6940c4*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.58\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.05\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.81\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "768847\t0.12\tray::IDLE\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\tWarning: Exception caused LightGBM_r30_BAG_L2 to fail during training... Skipping this model.\n",
      "\t\tTask was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: 6514add9ea2964bbfb39e51f74c3d4cda7b534c601000000, name=_ray_fit, pid=768847, memory used=0.12GB) was running was 969.88GB / 1007.45GB (0.962711), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 97f9cc9a0ea137a3a585f09cfe49ed95eabd2961665b320e1f6940c4) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-97f9cc9a0ea137a3a585f09cfe49ed95eabd2961665b320e1f6940c4*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.58\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.05\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.81\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "768847\t0.12\tray::IDLE\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 2753, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: 6514add9ea2964bbfb39e51f74c3d4cda7b534c601000000, name=_ray_fit, pid=768847, memory used=0.12GB) was running was 969.88GB / 1007.45GB (0.962711), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 97f9cc9a0ea137a3a585f09cfe49ed95eabd2961665b320e1f6940c4) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-97f9cc9a0ea137a3a585f09cfe49ed95eabd2961665b320e1f6940c4*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.58\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.05\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.81\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "768847\t0.12\tray::IDLE\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Fitting model: XGBoost_r49_BAG_L2 ... Training model for up to 1525.88s of the 1525.86s of remaining time.\n",
      "Fitting model: XGBoost_r49_BAG_L2 ... Training model for up to 1525.88s of the 1525.86s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.07%)\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.07%)\n",
      "\tWarning: Exception caused XGBoost_r49_BAG_L2 to fail during training... Skipping this model.\n",
      "\t\tTask was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: 2a5c1c6d5a86103ba470670327345d460ab377fd01000000, name=_ray_fit, pid=770804, memory used=0.08GB) was running was 971.60GB / 1007.45GB (0.964418), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: de2085ccb58b7251a3a7763219e06262b75f655ff8d403e46d13d3ca) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-de2085ccb58b7251a3a7763219e06262b75f655ff8d403e46d13d3ca*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.58\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.05\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.81\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "769265\t0.17\tray::IDLE\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 2753, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: 2a5c1c6d5a86103ba470670327345d460ab377fd01000000, name=_ray_fit, pid=770804, memory used=0.08GB) was running was 971.60GB / 1007.45GB (0.964418), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: de2085ccb58b7251a3a7763219e06262b75f655ff8d403e46d13d3ca) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-de2085ccb58b7251a3a7763219e06262b75f655ff8d403e46d13d3ca*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.58\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.05\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.81\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "769265\t0.17\tray::IDLE\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\tWarning: Exception caused XGBoost_r49_BAG_L2 to fail during training... Skipping this model.\n",
      "\t\tTask was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: 2a5c1c6d5a86103ba470670327345d460ab377fd01000000, name=_ray_fit, pid=770804, memory used=0.08GB) was running was 971.60GB / 1007.45GB (0.964418), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: de2085ccb58b7251a3a7763219e06262b75f655ff8d403e46d13d3ca) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-de2085ccb58b7251a3a7763219e06262b75f655ff8d403e46d13d3ca*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.58\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.05\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.81\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "769265\t0.17\tray::IDLE\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 2753, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: 2a5c1c6d5a86103ba470670327345d460ab377fd01000000, name=_ray_fit, pid=770804, memory used=0.08GB) was running was 971.60GB / 1007.45GB (0.964418), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: de2085ccb58b7251a3a7763219e06262b75f655ff8d403e46d13d3ca) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-de2085ccb58b7251a3a7763219e06262b75f655ff8d403e46d13d3ca*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.58\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.05\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.81\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "769265\t0.17\tray::IDLE\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Fitting model: CatBoost_r5_BAG_L2 ... Training model for up to 1518.39s of the 1518.37s of remaining time.\n",
      "Fitting model: CatBoost_r5_BAG_L2 ... Training model for up to 1518.39s of the 1518.37s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.01%)\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.01%)\n",
      "\tWarning: Exception caused CatBoost_r5_BAG_L2 to fail during training... Skipping this model.\n",
      "\t\tTask was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: 280baf3519a021365d65080190d3576001221c2001000000, name=_ray_fit, pid=770991, memory used=0.23GB) was running was 972.05GB / 1007.45GB (0.964864), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: a69e1673dfe5c53739fcc6e96dc6c9628fa1c08d1bca1462fe76a7e6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-a69e1673dfe5c53739fcc6e96dc6c9628fa1c08d1bca1462fe76a7e6*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.58\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.05\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.81\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "770991\t0.23\tray::_ray_fit\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 2753, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: 280baf3519a021365d65080190d3576001221c2001000000, name=_ray_fit, pid=770991, memory used=0.23GB) was running was 972.05GB / 1007.45GB (0.964864), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: a69e1673dfe5c53739fcc6e96dc6c9628fa1c08d1bca1462fe76a7e6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-a69e1673dfe5c53739fcc6e96dc6c9628fa1c08d1bca1462fe76a7e6*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.58\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.05\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.81\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "770991\t0.23\tray::_ray_fit\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\tWarning: Exception caused CatBoost_r5_BAG_L2 to fail during training... Skipping this model.\n",
      "\t\tTask was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: 280baf3519a021365d65080190d3576001221c2001000000, name=_ray_fit, pid=770991, memory used=0.23GB) was running was 972.05GB / 1007.45GB (0.964864), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: a69e1673dfe5c53739fcc6e96dc6c9628fa1c08d1bca1462fe76a7e6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-a69e1673dfe5c53739fcc6e96dc6c9628fa1c08d1bca1462fe76a7e6*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.58\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.05\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.81\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "770991\t0.23\tray::_ray_fit\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 2753, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: 280baf3519a021365d65080190d3576001221c2001000000, name=_ray_fit, pid=770991, memory used=0.23GB) was running was 972.05GB / 1007.45GB (0.964864), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: a69e1673dfe5c53739fcc6e96dc6c9628fa1c08d1bca1462fe76a7e6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-a69e1673dfe5c53739fcc6e96dc6c9628fa1c08d1bca1462fe76a7e6*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.58\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.05\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.81\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "770991\t0.23\tray::_ray_fit\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "2025-09-30 14:37:36,957\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "Fitting model: NeuralNetTorch_r87_BAG_L2 ... Training model for up to 1515.34s of the 1515.31s of remaining time.\n",
      "2025-09-30 14:37:36,957\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "Fitting model: NeuralNetTorch_r87_BAG_L2 ... Training model for up to 1515.34s of the 1515.31s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.00%)\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.00%)\n",
      "\tWarning: Exception caused NeuralNetTorch_r87_BAG_L2 to fail during training... Skipping this model.\n",
      "\t\t\u001b[36mray::_ray_fit()\u001b[39m (pid=773245, ip=172.17.0.5)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 413, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 201, in _fit\n",
      "    train_dataset = self._generate_dataset(X, y, train_params=processor_kwargs, is_train=True)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 669, in _generate_dataset\n",
      "    dataset = self._process_train_data(\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 734, in _process_train_data\n",
      "    self.processor = create_preprocessor(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/utils/data_preprocessor.py\", line 40, in create_preprocessor\n",
      "    return ColumnTransformer(\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "TypeError: ColumnTransformer.__init__() got an unexpected keyword argument 'force_int_remainder_cols'\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 2753, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 904, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(TypeError): \u001b[36mray::_ray_fit()\u001b[39m (pid=773245, ip=172.17.0.5)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 413, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 201, in _fit\n",
      "    train_dataset = self._generate_dataset(X, y, train_params=processor_kwargs, is_train=True)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 669, in _generate_dataset\n",
      "    dataset = self._process_train_data(\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 734, in _process_train_data\n",
      "    self.processor = create_preprocessor(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/utils/data_preprocessor.py\", line 40, in create_preprocessor\n",
      "    return ColumnTransformer(\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "TypeError: ColumnTransformer.__init__() got an unexpected keyword argument 'force_int_remainder_cols'\n",
      "\tWarning: Exception caused NeuralNetTorch_r87_BAG_L2 to fail during training... Skipping this model.\n",
      "\t\t\u001b[36mray::_ray_fit()\u001b[39m (pid=773245, ip=172.17.0.5)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 413, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 201, in _fit\n",
      "    train_dataset = self._generate_dataset(X, y, train_params=processor_kwargs, is_train=True)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 669, in _generate_dataset\n",
      "    dataset = self._process_train_data(\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 734, in _process_train_data\n",
      "    self.processor = create_preprocessor(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/utils/data_preprocessor.py\", line 40, in create_preprocessor\n",
      "    return ColumnTransformer(\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "TypeError: ColumnTransformer.__init__() got an unexpected keyword argument 'force_int_remainder_cols'\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 2753, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 904, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(TypeError): \u001b[36mray::_ray_fit()\u001b[39m (pid=773245, ip=172.17.0.5)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 413, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 201, in _fit\n",
      "    train_dataset = self._generate_dataset(X, y, train_params=processor_kwargs, is_train=True)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 669, in _generate_dataset\n",
      "    dataset = self._process_train_data(\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 734, in _process_train_data\n",
      "    self.processor = create_preprocessor(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/utils/data_preprocessor.py\", line 40, in create_preprocessor\n",
      "    return ColumnTransformer(\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "TypeError: ColumnTransformer.__init__() got an unexpected keyword argument 'force_int_remainder_cols'\n",
      "2025-09-30 14:37:40,609\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-09-30 14:37:40,610\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "Fitting model: NeuralNetTorch_r71_BAG_L2 ... Training model for up to 1511.68s of the 1511.66s of remaining time.\n",
      "2025-09-30 14:37:40,609\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-09-30 14:37:40,610\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "Fitting model: NeuralNetTorch_r71_BAG_L2 ... Training model for up to 1511.68s of the 1511.66s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.00%)\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.00%)\n",
      "\tWarning: Exception caused NeuralNetTorch_r71_BAG_L2 to fail during training... Skipping this model.\n",
      "\t\t\u001b[36mray::_ray_fit()\u001b[39m (pid=774795, ip=172.17.0.5)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 413, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 201, in _fit\n",
      "    train_dataset = self._generate_dataset(X, y, train_params=processor_kwargs, is_train=True)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 669, in _generate_dataset\n",
      "    dataset = self._process_train_data(\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 734, in _process_train_data\n",
      "    self.processor = create_preprocessor(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/utils/data_preprocessor.py\", line 40, in create_preprocessor\n",
      "    return ColumnTransformer(\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "TypeError: ColumnTransformer.__init__() got an unexpected keyword argument 'force_int_remainder_cols'\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 2753, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 904, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(TypeError): \u001b[36mray::_ray_fit()\u001b[39m (pid=774795, ip=172.17.0.5)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 413, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 201, in _fit\n",
      "    train_dataset = self._generate_dataset(X, y, train_params=processor_kwargs, is_train=True)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 669, in _generate_dataset\n",
      "    dataset = self._process_train_data(\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 734, in _process_train_data\n",
      "    self.processor = create_preprocessor(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/utils/data_preprocessor.py\", line 40, in create_preprocessor\n",
      "    return ColumnTransformer(\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "TypeError: ColumnTransformer.__init__() got an unexpected keyword argument 'force_int_remainder_cols'\n",
      "\tWarning: Exception caused NeuralNetTorch_r71_BAG_L2 to fail during training... Skipping this model.\n",
      "\t\t\u001b[36mray::_ray_fit()\u001b[39m (pid=774795, ip=172.17.0.5)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 413, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 201, in _fit\n",
      "    train_dataset = self._generate_dataset(X, y, train_params=processor_kwargs, is_train=True)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 669, in _generate_dataset\n",
      "    dataset = self._process_train_data(\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 734, in _process_train_data\n",
      "    self.processor = create_preprocessor(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/utils/data_preprocessor.py\", line 40, in create_preprocessor\n",
      "    return ColumnTransformer(\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "TypeError: ColumnTransformer.__init__() got an unexpected keyword argument 'force_int_remainder_cols'\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 2753, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 904, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(TypeError): \u001b[36mray::_ray_fit()\u001b[39m (pid=774795, ip=172.17.0.5)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 413, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 201, in _fit\n",
      "    train_dataset = self._generate_dataset(X, y, train_params=processor_kwargs, is_train=True)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 669, in _generate_dataset\n",
      "    dataset = self._process_train_data(\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 734, in _process_train_data\n",
      "    self.processor = create_preprocessor(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/utils/data_preprocessor.py\", line 40, in create_preprocessor\n",
      "    return ColumnTransformer(\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "TypeError: ColumnTransformer.__init__() got an unexpected keyword argument 'force_int_remainder_cols'\n",
      "2025-09-30 14:37:44,447\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::_ray_fit()\u001b[39m (pid=773246, ip=172.17.0.5)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 413, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 201, in _fit\n",
      "    train_dataset = self._generate_dataset(X, y, train_params=processor_kwargs, is_train=True)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 669, in _generate_dataset\n",
      "    dataset = self._process_train_data(\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 734, in _process_train_data\n",
      "    self.processor = create_preprocessor(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/utils/data_preprocessor.py\", line 40, in create_preprocessor\n",
      "    return ColumnTransformer(\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "TypeError: ColumnTransformer.__init__() got an unexpected keyword argument 'force_int_remainder_cols'\n",
      "2025-09-30 14:37:44,448\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::_ray_fit()\u001b[39m (pid=773247, ip=172.17.0.5)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 413, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 201, in _fit\n",
      "    train_dataset = self._generate_dataset(X, y, train_params=processor_kwargs, is_train=True)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 669, in _generate_dataset\n",
      "    dataset = self._process_train_data(\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 734, in _process_train_data\n",
      "    self.processor = create_preprocessor(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/utils/data_preprocessor.py\", line 40, in create_preprocessor\n",
      "    return ColumnTransformer(\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "TypeError: ColumnTransformer.__init__() got an unexpected keyword argument 'force_int_remainder_cols'\n",
      "2025-09-30 14:37:44,449\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::_ray_fit()\u001b[39m (pid=773244, ip=172.17.0.5)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 413, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 201, in _fit\n",
      "    train_dataset = self._generate_dataset(X, y, train_params=processor_kwargs, is_train=True)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 669, in _generate_dataset\n",
      "    dataset = self._process_train_data(\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 734, in _process_train_data\n",
      "    self.processor = create_preprocessor(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/utils/data_preprocessor.py\", line 40, in create_preprocessor\n",
      "    return ColumnTransformer(\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "TypeError: ColumnTransformer.__init__() got an unexpected keyword argument 'force_int_remainder_cols'\n",
      "2025-09-30 14:37:44,447\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::_ray_fit()\u001b[39m (pid=773246, ip=172.17.0.5)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 413, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 201, in _fit\n",
      "    train_dataset = self._generate_dataset(X, y, train_params=processor_kwargs, is_train=True)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 669, in _generate_dataset\n",
      "    dataset = self._process_train_data(\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 734, in _process_train_data\n",
      "    self.processor = create_preprocessor(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/utils/data_preprocessor.py\", line 40, in create_preprocessor\n",
      "    return ColumnTransformer(\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "TypeError: ColumnTransformer.__init__() got an unexpected keyword argument 'force_int_remainder_cols'\n",
      "2025-09-30 14:37:44,448\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::_ray_fit()\u001b[39m (pid=773247, ip=172.17.0.5)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 413, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 201, in _fit\n",
      "    train_dataset = self._generate_dataset(X, y, train_params=processor_kwargs, is_train=True)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 669, in _generate_dataset\n",
      "    dataset = self._process_train_data(\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 734, in _process_train_data\n",
      "    self.processor = create_preprocessor(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/utils/data_preprocessor.py\", line 40, in create_preprocessor\n",
      "    return ColumnTransformer(\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "TypeError: ColumnTransformer.__init__() got an unexpected keyword argument 'force_int_remainder_cols'\n",
      "2025-09-30 14:37:44,449\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::_ray_fit()\u001b[39m (pid=773244, ip=172.17.0.5)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 413, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 201, in _fit\n",
      "    train_dataset = self._generate_dataset(X, y, train_params=processor_kwargs, is_train=True)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 669, in _generate_dataset\n",
      "    dataset = self._process_train_data(\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 734, in _process_train_data\n",
      "    self.processor = create_preprocessor(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/utils/data_preprocessor.py\", line 40, in create_preprocessor\n",
      "    return ColumnTransformer(\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "TypeError: ColumnTransformer.__init__() got an unexpected keyword argument 'force_int_remainder_cols'\n",
      "Fitting model: CatBoost_r143_BAG_L2 ... Training model for up to 1507.84s of the 1507.82s of remaining time.\n",
      "Fitting model: CatBoost_r143_BAG_L2 ... Training model for up to 1507.84s of the 1507.82s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.07%)\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.07%)\n",
      "\tWarning: Exception caused CatBoost_r143_BAG_L2 to fail during training... Skipping this model.\n",
      "\t\tTask was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: 0e23b8a2ef22246404619cc953a2a327a7b9a17b01000000, name=_ray_fit, pid=775653, memory used=0.08GB) was running was 971.87GB / 1007.45GB (0.964686), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 8fd3a9c8aa791f5101980250a9d528510c62a009d56bd2e710ecfc59) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-8fd3a9c8aa791f5101980250a9d528510c62a009d56bd2e710ecfc59*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.58\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "521385\t0.91\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "522871\t0.81\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "774795\t0.32\tray::IDLE\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 2753, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: 0e23b8a2ef22246404619cc953a2a327a7b9a17b01000000, name=_ray_fit, pid=775653, memory used=0.08GB) was running was 971.87GB / 1007.45GB (0.964686), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 8fd3a9c8aa791f5101980250a9d528510c62a009d56bd2e710ecfc59) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-8fd3a9c8aa791f5101980250a9d528510c62a009d56bd2e710ecfc59*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.58\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "521385\t0.91\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "522871\t0.81\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "774795\t0.32\tray::IDLE\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\tWarning: Exception caused CatBoost_r143_BAG_L2 to fail during training... Skipping this model.\n",
      "\t\tTask was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: 0e23b8a2ef22246404619cc953a2a327a7b9a17b01000000, name=_ray_fit, pid=775653, memory used=0.08GB) was running was 971.87GB / 1007.45GB (0.964686), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 8fd3a9c8aa791f5101980250a9d528510c62a009d56bd2e710ecfc59) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-8fd3a9c8aa791f5101980250a9d528510c62a009d56bd2e710ecfc59*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.58\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "521385\t0.91\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "522871\t0.81\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "774795\t0.32\tray::IDLE\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 2753, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: 0e23b8a2ef22246404619cc953a2a327a7b9a17b01000000, name=_ray_fit, pid=775653, memory used=0.08GB) was running was 971.87GB / 1007.45GB (0.964686), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 8fd3a9c8aa791f5101980250a9d528510c62a009d56bd2e710ecfc59) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-8fd3a9c8aa791f5101980250a9d528510c62a009d56bd2e710ecfc59*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.58\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "521385\t0.91\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "522871\t0.81\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "774795\t0.32\tray::IDLE\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "2025-09-30 14:37:45,108\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-09-30 14:37:45,109\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-09-30 14:37:45,109\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "Fitting model: ExtraTrees_r178_BAG_L2 ... Training model for up to 1507.19s of the 1507.16s of remaining time.\n",
      "2025-09-30 14:37:45,108\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-09-30 14:37:45,109\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-09-30 14:37:45,109\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "Fitting model: ExtraTrees_r178_BAG_L2 ... Training model for up to 1507.19s of the 1507.16s of remaining time.\n",
      "\t-0.1171\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.33s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: RandomForest_r166_BAG_L2 ... Training model for up to 1506.74s of the 1506.72s of remaining time.\n",
      "\t-0.1171\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.33s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: RandomForest_r166_BAG_L2 ... Training model for up to 1506.74s of the 1506.72s of remaining time.\n",
      "\t-0.1169\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.4s\t = Training   runtime\n",
      "\t0.1s\t = Validation runtime\n",
      "Fitting model: XGBoost_r31_BAG_L2 ... Training model for up to 1506.19s of the 1506.16s of remaining time.\n",
      "\t-0.1169\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.4s\t = Training   runtime\n",
      "\t0.1s\t = Validation runtime\n",
      "Fitting model: XGBoost_r31_BAG_L2 ... Training model for up to 1506.19s of the 1506.16s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.04%)\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.04%)\n",
      "2025-09-30 14:37:50,518\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-09-30 14:37:50,519\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-09-30 14:37:50,520\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-09-30 14:37:50,520\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-09-30 14:37:50,521\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-09-30 14:37:50,521\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-09-30 14:37:50,522\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-09-30 14:37:50,518\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-09-30 14:37:50,519\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-09-30 14:37:50,520\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-09-30 14:37:50,520\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-09-30 14:37:50,521\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-09-30 14:37:50,521\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-09-30 14:37:50,522\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "\tWarning: Exception caused XGBoost_r31_BAG_L2 to fail during training... Skipping this model.\n",
      "\t\tTask was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: 1746603057f678a96527a96ff73bd4ef7c75eb3d01000000, name=_ray_fit, pid=778789, memory used=0.19GB) was running was 971.79GB / 1007.45GB (0.964599), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: a258e25062f4481d8c64f5e7c5537433a2d30a17466bfc5a02a96b38) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-a258e25062f4481d8c64f5e7c5537433a2d30a17466bfc5a02a96b38*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.58\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.07\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.81\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "778789\t0.19\tray::_ray_fit\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 2753, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: 1746603057f678a96527a96ff73bd4ef7c75eb3d01000000, name=_ray_fit, pid=778789, memory used=0.19GB) was running was 971.79GB / 1007.45GB (0.964599), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: a258e25062f4481d8c64f5e7c5537433a2d30a17466bfc5a02a96b38) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-a258e25062f4481d8c64f5e7c5537433a2d30a17466bfc5a02a96b38*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.58\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.07\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.81\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "778789\t0.19\tray::_ray_fit\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\tWarning: Exception caused XGBoost_r31_BAG_L2 to fail during training... Skipping this model.\n",
      "\t\tTask was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: 1746603057f678a96527a96ff73bd4ef7c75eb3d01000000, name=_ray_fit, pid=778789, memory used=0.19GB) was running was 971.79GB / 1007.45GB (0.964599), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: a258e25062f4481d8c64f5e7c5537433a2d30a17466bfc5a02a96b38) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-a258e25062f4481d8c64f5e7c5537433a2d30a17466bfc5a02a96b38*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.58\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.07\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.81\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "778789\t0.19\tray::_ray_fit\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 2753, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: 1746603057f678a96527a96ff73bd4ef7c75eb3d01000000, name=_ray_fit, pid=778789, memory used=0.19GB) was running was 971.79GB / 1007.45GB (0.964599), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: a258e25062f4481d8c64f5e7c5537433a2d30a17466bfc5a02a96b38) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-a258e25062f4481d8c64f5e7c5537433a2d30a17466bfc5a02a96b38*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.58\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.07\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.81\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "778789\t0.19\tray::_ray_fit\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Fitting model: NeuralNetTorch_r185_BAG_L2 ... Training model for up to 1496.26s of the 1496.23s of remaining time.\n",
      "Fitting model: NeuralNetTorch_r185_BAG_L2 ... Training model for up to 1496.26s of the 1496.23s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.00%)\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.00%)\n",
      "\tWarning: Exception caused NeuralNetTorch_r185_BAG_L2 to fail during training... Skipping this model.\n",
      "\t\t\u001b[36mray::_ray_fit()\u001b[39m (pid=779424, ip=172.17.0.5)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 413, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 201, in _fit\n",
      "    train_dataset = self._generate_dataset(X, y, train_params=processor_kwargs, is_train=True)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 669, in _generate_dataset\n",
      "    dataset = self._process_train_data(\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 734, in _process_train_data\n",
      "    self.processor = create_preprocessor(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/utils/data_preprocessor.py\", line 40, in create_preprocessor\n",
      "    return ColumnTransformer(\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "TypeError: ColumnTransformer.__init__() got an unexpected keyword argument 'force_int_remainder_cols'\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 2753, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 904, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(TypeError): \u001b[36mray::_ray_fit()\u001b[39m (pid=779424, ip=172.17.0.5)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 413, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 201, in _fit\n",
      "    train_dataset = self._generate_dataset(X, y, train_params=processor_kwargs, is_train=True)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 669, in _generate_dataset\n",
      "    dataset = self._process_train_data(\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 734, in _process_train_data\n",
      "    self.processor = create_preprocessor(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/utils/data_preprocessor.py\", line 40, in create_preprocessor\n",
      "    return ColumnTransformer(\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "TypeError: ColumnTransformer.__init__() got an unexpected keyword argument 'force_int_remainder_cols'\n",
      "\tWarning: Exception caused NeuralNetTorch_r185_BAG_L2 to fail during training... Skipping this model.\n",
      "\t\t\u001b[36mray::_ray_fit()\u001b[39m (pid=779424, ip=172.17.0.5)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 413, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 201, in _fit\n",
      "    train_dataset = self._generate_dataset(X, y, train_params=processor_kwargs, is_train=True)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 669, in _generate_dataset\n",
      "    dataset = self._process_train_data(\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 734, in _process_train_data\n",
      "    self.processor = create_preprocessor(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/utils/data_preprocessor.py\", line 40, in create_preprocessor\n",
      "    return ColumnTransformer(\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "TypeError: ColumnTransformer.__init__() got an unexpected keyword argument 'force_int_remainder_cols'\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 2753, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 904, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(TypeError): \u001b[36mray::_ray_fit()\u001b[39m (pid=779424, ip=172.17.0.5)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 413, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 201, in _fit\n",
      "    train_dataset = self._generate_dataset(X, y, train_params=processor_kwargs, is_train=True)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 669, in _generate_dataset\n",
      "    dataset = self._process_train_data(\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 734, in _process_train_data\n",
      "    self.processor = create_preprocessor(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/utils/data_preprocessor.py\", line 40, in create_preprocessor\n",
      "    return ColumnTransformer(\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "TypeError: ColumnTransformer.__init__() got an unexpected keyword argument 'force_int_remainder_cols'\n",
      "2025-09-30 14:37:59,771\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "Fitting model: NeuralNetFastAI_r160_BAG_L2 ... Training model for up to 1492.52s of the 1492.49s of remaining time.\n",
      "2025-09-30 14:37:59,771\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "Fitting model: NeuralNetFastAI_r160_BAG_L2 ... Training model for up to 1492.52s of the 1492.49s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.00%)\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.00%)\n",
      "2025-09-30 14:38:05,522\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-09-30 14:38:05,524\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-09-30 14:38:05,522\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-09-30 14:38:05,524\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "\tWarning: Exception caused NeuralNetFastAI_r160_BAG_L2 to fail during training... Skipping this model.\n",
      "\t\tTask was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: 4e476372fbf528a820a088ceb7704e9b166946c101000000, name=_ray_fit, pid=782712, memory used=0.32GB) was running was 972.27GB / 1007.45GB (0.965082), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: a8b298b2d3e1d9d8d31f65aed28174afaa6daa0a6c43161975b4259d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-a8b298b2d3e1d9d8d31f65aed28174afaa6daa0a6c43161975b4259d*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.58\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.81\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "782712\t0.32\tray::_ray_fit\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 2753, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: 4e476372fbf528a820a088ceb7704e9b166946c101000000, name=_ray_fit, pid=782712, memory used=0.32GB) was running was 972.27GB / 1007.45GB (0.965082), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: a8b298b2d3e1d9d8d31f65aed28174afaa6daa0a6c43161975b4259d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-a8b298b2d3e1d9d8d31f65aed28174afaa6daa0a6c43161975b4259d*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.58\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.81\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "782712\t0.32\tray::_ray_fit\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\tWarning: Exception caused NeuralNetFastAI_r160_BAG_L2 to fail during training... Skipping this model.\n",
      "\t\tTask was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: 4e476372fbf528a820a088ceb7704e9b166946c101000000, name=_ray_fit, pid=782712, memory used=0.32GB) was running was 972.27GB / 1007.45GB (0.965082), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: a8b298b2d3e1d9d8d31f65aed28174afaa6daa0a6c43161975b4259d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-a8b298b2d3e1d9d8d31f65aed28174afaa6daa0a6c43161975b4259d*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.58\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.81\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "782712\t0.32\tray::_ray_fit\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 2753, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: 4e476372fbf528a820a088ceb7704e9b166946c101000000, name=_ray_fit, pid=782712, memory used=0.32GB) was running was 972.27GB / 1007.45GB (0.965082), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: a8b298b2d3e1d9d8d31f65aed28174afaa6daa0a6c43161975b4259d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-a8b298b2d3e1d9d8d31f65aed28174afaa6daa0a6c43161975b4259d*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.58\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.81\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "782712\t0.32\tray::_ray_fit\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Fitting model: CatBoost_r60_BAG_L2 ... Training model for up to 1482.69s of the 1482.66s of remaining time.\n",
      "Fitting model: CatBoost_r60_BAG_L2 ... Training model for up to 1482.69s of the 1482.66s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.02%)\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.02%)\n",
      "\tWarning: Exception caused CatBoost_r60_BAG_L2 to fail during training... Skipping this model.\n",
      "\t\tTask was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: be70b1b8b71b14303f4fc8f73a919627d2a3d5d001000000, name=_ray_fit, pid=783211, memory used=0.24GB) was running was 972.56GB / 1007.45GB (0.965365), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 102cf270825838c634d49e5167f39d4a3f95c96539e666ba831128e7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-102cf270825838c634d49e5167f39d4a3f95c96539e666ba831128e7*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.58\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.07\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.81\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "783211\t0.24\tray::_ray_fit\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 2753, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: be70b1b8b71b14303f4fc8f73a919627d2a3d5d001000000, name=_ray_fit, pid=783211, memory used=0.24GB) was running was 972.56GB / 1007.45GB (0.965365), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 102cf270825838c634d49e5167f39d4a3f95c96539e666ba831128e7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-102cf270825838c634d49e5167f39d4a3f95c96539e666ba831128e7*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.58\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.07\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.81\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "783211\t0.24\tray::_ray_fit\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\tWarning: Exception caused CatBoost_r60_BAG_L2 to fail during training... Skipping this model.\n",
      "\t\tTask was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: be70b1b8b71b14303f4fc8f73a919627d2a3d5d001000000, name=_ray_fit, pid=783211, memory used=0.24GB) was running was 972.56GB / 1007.45GB (0.965365), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 102cf270825838c634d49e5167f39d4a3f95c96539e666ba831128e7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-102cf270825838c634d49e5167f39d4a3f95c96539e666ba831128e7*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.58\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.07\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.81\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "783211\t0.24\tray::_ray_fit\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 2753, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: be70b1b8b71b14303f4fc8f73a919627d2a3d5d001000000, name=_ray_fit, pid=783211, memory used=0.24GB) was running was 972.56GB / 1007.45GB (0.965365), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 102cf270825838c634d49e5167f39d4a3f95c96539e666ba831128e7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-102cf270825838c634d49e5167f39d4a3f95c96539e666ba831128e7*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.58\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.07\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.81\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "783211\t0.24\tray::_ray_fit\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Fitting model: RandomForest_r15_BAG_L2 ... Training model for up to 1477.38s of the 1477.36s of remaining time.\n",
      "Fitting model: RandomForest_r15_BAG_L2 ... Training model for up to 1477.38s of the 1477.36s of remaining time.\n",
      "\t-0.1246\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.34s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: LightGBM_r135_BAG_L2 ... Training model for up to 1476.96s of the 1476.93s of remaining time.\n",
      "\t-0.1246\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.34s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: LightGBM_r135_BAG_L2 ... Training model for up to 1476.96s of the 1476.93s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.12%)\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.12%)\n",
      "\tWarning: Exception caused LightGBM_r135_BAG_L2 to fail during training... Skipping this model.\n",
      "\t\tTask was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: 4d805368e4dffb8caf8c1b5dc2b314f3ccf5172001000000, name=_ray_fit, pid=786669, memory used=0.12GB) was running was 972.58GB / 1007.45GB (0.965391), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 22986c0aa7221a6f7f782d0dad18fd5fa35e27602cf9a44d1063ee05) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-22986c0aa7221a6f7f782d0dad18fd5fa35e27602cf9a44d1063ee05*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.58\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.07\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.81\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "786669\t0.12\tray::IDLE\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 2753, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: 4d805368e4dffb8caf8c1b5dc2b314f3ccf5172001000000, name=_ray_fit, pid=786669, memory used=0.12GB) was running was 972.58GB / 1007.45GB (0.965391), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 22986c0aa7221a6f7f782d0dad18fd5fa35e27602cf9a44d1063ee05) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-22986c0aa7221a6f7f782d0dad18fd5fa35e27602cf9a44d1063ee05*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.58\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.07\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.81\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "786669\t0.12\tray::IDLE\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\tWarning: Exception caused LightGBM_r135_BAG_L2 to fail during training... Skipping this model.\n",
      "\t\tTask was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: 4d805368e4dffb8caf8c1b5dc2b314f3ccf5172001000000, name=_ray_fit, pid=786669, memory used=0.12GB) was running was 972.58GB / 1007.45GB (0.965391), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 22986c0aa7221a6f7f782d0dad18fd5fa35e27602cf9a44d1063ee05) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-22986c0aa7221a6f7f782d0dad18fd5fa35e27602cf9a44d1063ee05*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.58\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.07\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.81\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "786669\t0.12\tray::IDLE\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 2753, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: 4d805368e4dffb8caf8c1b5dc2b314f3ccf5172001000000, name=_ray_fit, pid=786669, memory used=0.12GB) was running was 972.58GB / 1007.45GB (0.965391), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 22986c0aa7221a6f7f782d0dad18fd5fa35e27602cf9a44d1063ee05) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-22986c0aa7221a6f7f782d0dad18fd5fa35e27602cf9a44d1063ee05*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.58\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.07\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.81\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "786669\t0.12\tray::IDLE\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Fitting model: XGBoost_r22_BAG_L2 ... Training model for up to 1469.85s of the 1469.82s of remaining time.\n",
      "Fitting model: XGBoost_r22_BAG_L2 ... Training model for up to 1469.85s of the 1469.82s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.04%)\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.04%)\n",
      "\tWarning: Exception caused XGBoost_r22_BAG_L2 to fail during training... Skipping this model.\n",
      "\t\tTask was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: 033b08840cf49840a1518dcc9e72b7a7b332b68a01000000, name=_ray_fit, pid=788849, memory used=0.12GB) was running was 973.18GB / 1007.45GB (0.965981), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 8b783dc8e199f62b43f28c6665cbced233ba5f39b6f2e96e586db505) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-8b783dc8e199f62b43f28c6665cbced233ba5f39b6f2e96e586db505*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.58\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.07\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.81\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "788849\t0.12\tray::IDLE\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 2753, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: 033b08840cf49840a1518dcc9e72b7a7b332b68a01000000, name=_ray_fit, pid=788849, memory used=0.12GB) was running was 973.18GB / 1007.45GB (0.965981), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 8b783dc8e199f62b43f28c6665cbced233ba5f39b6f2e96e586db505) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-8b783dc8e199f62b43f28c6665cbced233ba5f39b6f2e96e586db505*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.58\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.07\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.81\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "788849\t0.12\tray::IDLE\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\tWarning: Exception caused XGBoost_r22_BAG_L2 to fail during training... Skipping this model.\n",
      "\t\tTask was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: 033b08840cf49840a1518dcc9e72b7a7b332b68a01000000, name=_ray_fit, pid=788849, memory used=0.12GB) was running was 973.18GB / 1007.45GB (0.965981), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 8b783dc8e199f62b43f28c6665cbced233ba5f39b6f2e96e586db505) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-8b783dc8e199f62b43f28c6665cbced233ba5f39b6f2e96e586db505*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.58\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.07\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.81\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "788849\t0.12\tray::IDLE\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 2753, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: 033b08840cf49840a1518dcc9e72b7a7b332b68a01000000, name=_ray_fit, pid=788849, memory used=0.12GB) was running was 973.18GB / 1007.45GB (0.965981), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 8b783dc8e199f62b43f28c6665cbced233ba5f39b6f2e96e586db505) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-8b783dc8e199f62b43f28c6665cbced233ba5f39b6f2e96e586db505*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.58\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.07\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.81\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "788849\t0.12\tray::IDLE\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Fitting model: NeuralNetFastAI_r69_BAG_L2 ... Training model for up to 1459.45s of the 1459.43s of remaining time.\n",
      "Fitting model: NeuralNetFastAI_r69_BAG_L2 ... Training model for up to 1459.45s of the 1459.43s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.00%)\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.00%)\n",
      "\tWarning: Exception caused NeuralNetFastAI_r69_BAG_L2 to fail during training... Skipping this model.\n",
      "\t\tTask was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: f5149a23d3acc4a53b77b3cb602ae65b942025b201000000, name=_ray_fit, pid=788851, memory used=0.10GB) was running was 973.14GB / 1007.45GB (0.965941), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 2526d4f109fe3e2b6d879c66211fdff94820b1e4cc866899f15f3e34) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-2526d4f109fe3e2b6d879c66211fdff94820b1e4cc866899f15f3e34*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.07\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.81\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "788851\t0.10\tray::IDLE\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 2753, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: f5149a23d3acc4a53b77b3cb602ae65b942025b201000000, name=_ray_fit, pid=788851, memory used=0.10GB) was running was 973.14GB / 1007.45GB (0.965941), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 2526d4f109fe3e2b6d879c66211fdff94820b1e4cc866899f15f3e34) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-2526d4f109fe3e2b6d879c66211fdff94820b1e4cc866899f15f3e34*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.07\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.81\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "788851\t0.10\tray::IDLE\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\tWarning: Exception caused NeuralNetFastAI_r69_BAG_L2 to fail during training... Skipping this model.\n",
      "\t\tTask was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: f5149a23d3acc4a53b77b3cb602ae65b942025b201000000, name=_ray_fit, pid=788851, memory used=0.10GB) was running was 973.14GB / 1007.45GB (0.965941), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 2526d4f109fe3e2b6d879c66211fdff94820b1e4cc866899f15f3e34) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-2526d4f109fe3e2b6d879c66211fdff94820b1e4cc866899f15f3e34*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.07\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.81\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "788851\t0.10\tray::IDLE\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 2753, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: f5149a23d3acc4a53b77b3cb602ae65b942025b201000000, name=_ray_fit, pid=788851, memory used=0.10GB) was running was 973.14GB / 1007.45GB (0.965941), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 2526d4f109fe3e2b6d879c66211fdff94820b1e4cc866899f15f3e34) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-2526d4f109fe3e2b6d879c66211fdff94820b1e4cc866899f15f3e34*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.07\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.81\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "788851\t0.10\tray::IDLE\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Fitting model: CatBoost_r6_BAG_L2 ... Training model for up to 1458.93s of the 1458.91s of remaining time.\n",
      "Fitting model: CatBoost_r6_BAG_L2 ... Training model for up to 1458.93s of the 1458.91s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.01%)\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.01%)\n",
      "\tWarning: Exception caused CatBoost_r6_BAG_L2 to fail during training... Skipping this model.\n",
      "\t\tTask was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: 7f9a2b2d987c95e77f2c0c543338eeb874c7d09b01000000, name=_ray_fit, pid=789196, memory used=0.25GB) was running was 973.00GB / 1007.45GB (0.965806), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d2dde66068be6e357bc70d9ec75dbdc261c894640cd6be97baa9a931) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-d2dde66068be6e357bc70d9ec75dbdc261c894640cd6be97baa9a931*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.07\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.81\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "789196\t0.25\tray::_ray_fit\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 2753, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: 7f9a2b2d987c95e77f2c0c543338eeb874c7d09b01000000, name=_ray_fit, pid=789196, memory used=0.25GB) was running was 973.00GB / 1007.45GB (0.965806), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d2dde66068be6e357bc70d9ec75dbdc261c894640cd6be97baa9a931) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-d2dde66068be6e357bc70d9ec75dbdc261c894640cd6be97baa9a931*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.07\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.81\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "789196\t0.25\tray::_ray_fit\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\tWarning: Exception caused CatBoost_r6_BAG_L2 to fail during training... Skipping this model.\n",
      "\t\tTask was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: 7f9a2b2d987c95e77f2c0c543338eeb874c7d09b01000000, name=_ray_fit, pid=789196, memory used=0.25GB) was running was 973.00GB / 1007.45GB (0.965806), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d2dde66068be6e357bc70d9ec75dbdc261c894640cd6be97baa9a931) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-d2dde66068be6e357bc70d9ec75dbdc261c894640cd6be97baa9a931*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.07\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.81\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "789196\t0.25\tray::_ray_fit\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 2753, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: 7f9a2b2d987c95e77f2c0c543338eeb874c7d09b01000000, name=_ray_fit, pid=789196, memory used=0.25GB) was running was 973.00GB / 1007.45GB (0.965806), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d2dde66068be6e357bc70d9ec75dbdc261c894640cd6be97baa9a931) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-d2dde66068be6e357bc70d9ec75dbdc261c894640cd6be97baa9a931*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.07\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.81\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "789196\t0.25\tray::_ray_fit\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Fitting model: NeuralNetFastAI_r138_BAG_L2 ... Training model for up to 1455.50s of the 1455.48s of remaining time.\n",
      "Fitting model: NeuralNetFastAI_r138_BAG_L2 ... Training model for up to 1455.50s of the 1455.48s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.00%)\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.00%)\n",
      "\tWarning: Exception caused NeuralNetFastAI_r138_BAG_L2 to fail during training... Skipping this model.\n",
      "\t\tTask was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: e85b3828c7c53b2de35a90175414873ab0b953a501000000, name=_ray_fit, pid=791282, memory used=0.41GB) was running was 973.29GB / 1007.45GB (0.966089), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 1f860ae840ec55215cb2a9468f40c51c1b6915c1de02cd1f47f50d72) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-1f860ae840ec55215cb2a9468f40c51c1b6915c1de02cd1f47f50d72*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.81\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "791282\t0.41\tray::_ray_fit\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 2753, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: e85b3828c7c53b2de35a90175414873ab0b953a501000000, name=_ray_fit, pid=791282, memory used=0.41GB) was running was 973.29GB / 1007.45GB (0.966089), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 1f860ae840ec55215cb2a9468f40c51c1b6915c1de02cd1f47f50d72) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-1f860ae840ec55215cb2a9468f40c51c1b6915c1de02cd1f47f50d72*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.81\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "791282\t0.41\tray::_ray_fit\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\tWarning: Exception caused NeuralNetFastAI_r138_BAG_L2 to fail during training... Skipping this model.\n",
      "\t\tTask was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: e85b3828c7c53b2de35a90175414873ab0b953a501000000, name=_ray_fit, pid=791282, memory used=0.41GB) was running was 973.29GB / 1007.45GB (0.966089), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 1f860ae840ec55215cb2a9468f40c51c1b6915c1de02cd1f47f50d72) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-1f860ae840ec55215cb2a9468f40c51c1b6915c1de02cd1f47f50d72*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.81\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "791282\t0.41\tray::_ray_fit\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 2753, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: e85b3828c7c53b2de35a90175414873ab0b953a501000000, name=_ray_fit, pid=791282, memory used=0.41GB) was running was 973.29GB / 1007.45GB (0.966089), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 1f860ae840ec55215cb2a9468f40c51c1b6915c1de02cd1f47f50d72) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-1f860ae840ec55215cb2a9468f40c51c1b6915c1de02cd1f47f50d72*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.81\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "791282\t0.41\tray::_ray_fit\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Fitting model: LightGBM_r121_BAG_L2 ... Training model for up to 1444.89s of the 1444.86s of remaining time.\n",
      "Fitting model: LightGBM_r121_BAG_L2 ... Training model for up to 1444.89s of the 1444.86s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.12%)\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.12%)\n",
      "\tWarning: Exception caused LightGBM_r121_BAG_L2 to fail during training... Skipping this model.\n",
      "\t\tTask was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: b10e826c72fc2f6f307ebca3c8974960b57f859d01000000, name=_ray_fit, pid=795011, memory used=0.11GB) was running was 973.20GB / 1007.45GB (0.966002), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 2161c217889f36b811b7a7d5a7d8d9920a027d21d5c1ae3300581686) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-2161c217889f36b811b7a7d5a7d8d9920a027d21d5c1ae3300581686*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.06\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.81\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "795011\t0.11\tray::IDLE\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 2753, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: b10e826c72fc2f6f307ebca3c8974960b57f859d01000000, name=_ray_fit, pid=795011, memory used=0.11GB) was running was 973.20GB / 1007.45GB (0.966002), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 2161c217889f36b811b7a7d5a7d8d9920a027d21d5c1ae3300581686) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-2161c217889f36b811b7a7d5a7d8d9920a027d21d5c1ae3300581686*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.06\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.81\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "795011\t0.11\tray::IDLE\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\tWarning: Exception caused LightGBM_r121_BAG_L2 to fail during training... Skipping this model.\n",
      "\t\tTask was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: b10e826c72fc2f6f307ebca3c8974960b57f859d01000000, name=_ray_fit, pid=795011, memory used=0.11GB) was running was 973.20GB / 1007.45GB (0.966002), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 2161c217889f36b811b7a7d5a7d8d9920a027d21d5c1ae3300581686) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-2161c217889f36b811b7a7d5a7d8d9920a027d21d5c1ae3300581686*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.06\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.81\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "795011\t0.11\tray::IDLE\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 2753, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: b10e826c72fc2f6f307ebca3c8974960b57f859d01000000, name=_ray_fit, pid=795011, memory used=0.11GB) was running was 973.20GB / 1007.45GB (0.966002), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 2161c217889f36b811b7a7d5a7d8d9920a027d21d5c1ae3300581686) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-2161c217889f36b811b7a7d5a7d8d9920a027d21d5c1ae3300581686*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.06\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.81\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "795011\t0.11\tray::IDLE\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Fitting model: NeuralNetFastAI_r172_BAG_L2 ... Training model for up to 1437.50s of the 1437.47s of remaining time.\n",
      "Fitting model: NeuralNetFastAI_r172_BAG_L2 ... Training model for up to 1437.50s of the 1437.47s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.00%)\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.00%)\n",
      "2025-09-30 14:39:00,538\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-09-30 14:39:00,538\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "\tWarning: Exception caused NeuralNetFastAI_r172_BAG_L2 to fail during training... Skipping this model.\n",
      "\t\tTask was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: 8b397aa4b943fdd70e6b77787880533a1e82b79901000000, name=_ray_fit, pid=796780, memory used=0.12GB) was running was 974.04GB / 1007.45GB (0.966838), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: bc7a21742c6f4e0dc9e21b6465ccd6e038b8af097de30e8b123b33fb) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-bc7a21742c6f4e0dc9e21b6465ccd6e038b8af097de30e8b123b33fb*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.07\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.81\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "796780\t0.12\tray::IDLE\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 2753, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: 8b397aa4b943fdd70e6b77787880533a1e82b79901000000, name=_ray_fit, pid=796780, memory used=0.12GB) was running was 974.04GB / 1007.45GB (0.966838), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: bc7a21742c6f4e0dc9e21b6465ccd6e038b8af097de30e8b123b33fb) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-bc7a21742c6f4e0dc9e21b6465ccd6e038b8af097de30e8b123b33fb*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.07\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.81\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "796780\t0.12\tray::IDLE\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\tWarning: Exception caused NeuralNetFastAI_r172_BAG_L2 to fail during training... Skipping this model.\n",
      "\t\tTask was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: 8b397aa4b943fdd70e6b77787880533a1e82b79901000000, name=_ray_fit, pid=796780, memory used=0.12GB) was running was 974.04GB / 1007.45GB (0.966838), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: bc7a21742c6f4e0dc9e21b6465ccd6e038b8af097de30e8b123b33fb) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-bc7a21742c6f4e0dc9e21b6465ccd6e038b8af097de30e8b123b33fb*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.07\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.81\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "796780\t0.12\tray::IDLE\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 2753, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: 8b397aa4b943fdd70e6b77787880533a1e82b79901000000, name=_ray_fit, pid=796780, memory used=0.12GB) was running was 974.04GB / 1007.45GB (0.966838), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: bc7a21742c6f4e0dc9e21b6465ccd6e038b8af097de30e8b123b33fb) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-bc7a21742c6f4e0dc9e21b6465ccd6e038b8af097de30e8b123b33fb*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.07\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.81\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "796780\t0.12\tray::IDLE\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Fitting model: CatBoost_r180_BAG_L2 ... Training model for up to 1429.47s of the 1429.44s of remaining time.\n",
      "Fitting model: CatBoost_r180_BAG_L2 ... Training model for up to 1429.47s of the 1429.44s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.08%)\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.08%)\n",
      "\tWarning: Exception caused CatBoost_r180_BAG_L2 to fail during training... Skipping this model.\n",
      "\t\tTask was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: 2555098dc90ce186b321a9425dfa17db75399c1d01000000, name=_ray_fit, pid=797240, memory used=0.26GB) was running was 974.21GB / 1007.45GB (0.967005), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 977688320a644320106b3c0ef13da32e7bca8cc3f68f130dd22de489) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-977688320a644320106b3c0ef13da32e7bca8cc3f68f130dd22de489*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.07\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.81\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "797240\t0.26\tray::_ray_fit\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "412520\t0.10\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 2753, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: 2555098dc90ce186b321a9425dfa17db75399c1d01000000, name=_ray_fit, pid=797240, memory used=0.26GB) was running was 974.21GB / 1007.45GB (0.967005), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 977688320a644320106b3c0ef13da32e7bca8cc3f68f130dd22de489) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-977688320a644320106b3c0ef13da32e7bca8cc3f68f130dd22de489*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.07\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.81\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "797240\t0.26\tray::_ray_fit\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "412520\t0.10\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\tWarning: Exception caused CatBoost_r180_BAG_L2 to fail during training... Skipping this model.\n",
      "\t\tTask was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: 2555098dc90ce186b321a9425dfa17db75399c1d01000000, name=_ray_fit, pid=797240, memory used=0.26GB) was running was 974.21GB / 1007.45GB (0.967005), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 977688320a644320106b3c0ef13da32e7bca8cc3f68f130dd22de489) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-977688320a644320106b3c0ef13da32e7bca8cc3f68f130dd22de489*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.07\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.81\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "797240\t0.26\tray::_ray_fit\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "412520\t0.10\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 2753, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: 2555098dc90ce186b321a9425dfa17db75399c1d01000000, name=_ray_fit, pid=797240, memory used=0.26GB) was running was 974.21GB / 1007.45GB (0.967005), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 977688320a644320106b3c0ef13da32e7bca8cc3f68f130dd22de489) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-977688320a644320106b3c0ef13da32e7bca8cc3f68f130dd22de489*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.07\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.81\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "797240\t0.26\tray::_ray_fit\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "412520\t0.10\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Fitting model: NeuralNetTorch_r76_BAG_L2 ... Training model for up to 1423.57s of the 1423.54s of remaining time.\n",
      "Fitting model: NeuralNetTorch_r76_BAG_L2 ... Training model for up to 1423.57s of the 1423.54s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.00%)\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.00%)\n",
      "\tWarning: Exception caused NeuralNetTorch_r76_BAG_L2 to fail during training... Skipping this model.\n",
      "\t\t\u001b[36mray::_ray_fit()\u001b[39m (pid=798959, ip=172.17.0.5)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 413, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 201, in _fit\n",
      "    train_dataset = self._generate_dataset(X, y, train_params=processor_kwargs, is_train=True)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 669, in _generate_dataset\n",
      "    dataset = self._process_train_data(\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 734, in _process_train_data\n",
      "    self.processor = create_preprocessor(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/utils/data_preprocessor.py\", line 40, in create_preprocessor\n",
      "    return ColumnTransformer(\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "TypeError: ColumnTransformer.__init__() got an unexpected keyword argument 'force_int_remainder_cols'\n",
      "\tWarning: Exception caused NeuralNetTorch_r76_BAG_L2 to fail during training... Skipping this model.\n",
      "\t\t\u001b[36mray::_ray_fit()\u001b[39m (pid=798959, ip=172.17.0.5)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 413, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 201, in _fit\n",
      "    train_dataset = self._generate_dataset(X, y, train_params=processor_kwargs, is_train=True)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 669, in _generate_dataset\n",
      "    dataset = self._process_train_data(\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 734, in _process_train_data\n",
      "    self.processor = create_preprocessor(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/utils/data_preprocessor.py\", line 40, in create_preprocessor\n",
      "    return ColumnTransformer(\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "TypeError: ColumnTransformer.__init__() got an unexpected keyword argument 'force_int_remainder_cols'\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 2753, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 904, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(TypeError): \u001b[36mray::_ray_fit()\u001b[39m (pid=798959, ip=172.17.0.5)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 413, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 201, in _fit\n",
      "    train_dataset = self._generate_dataset(X, y, train_params=processor_kwargs, is_train=True)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 669, in _generate_dataset\n",
      "    dataset = self._process_train_data(\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 734, in _process_train_data\n",
      "    self.processor = create_preprocessor(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/utils/data_preprocessor.py\", line 40, in create_preprocessor\n",
      "    return ColumnTransformer(\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "TypeError: ColumnTransformer.__init__() got an unexpected keyword argument 'force_int_remainder_cols'\n",
      "Fitting model: ExtraTrees_r197_BAG_L2 ... Training model for up to 1420.16s of the 1420.14s of remaining time.\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 2753, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 904, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(TypeError): \u001b[36mray::_ray_fit()\u001b[39m (pid=798959, ip=172.17.0.5)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 413, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 201, in _fit\n",
      "    train_dataset = self._generate_dataset(X, y, train_params=processor_kwargs, is_train=True)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 669, in _generate_dataset\n",
      "    dataset = self._process_train_data(\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 734, in _process_train_data\n",
      "    self.processor = create_preprocessor(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/utils/data_preprocessor.py\", line 40, in create_preprocessor\n",
      "    return ColumnTransformer(\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "TypeError: ColumnTransformer.__init__() got an unexpected keyword argument 'force_int_remainder_cols'\n",
      "Fitting model: ExtraTrees_r197_BAG_L2 ... Training model for up to 1420.16s of the 1420.14s of remaining time.\n",
      "\t-0.1161\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.42s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_r121_BAG_L2 ... Training model for up to 1419.61s of the 1419.58s of remaining time.\n",
      "\t-0.1161\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.42s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_r121_BAG_L2 ... Training model for up to 1419.61s of the 1419.58s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.00%)\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.00%)\n",
      "\tWarning: Exception caused NeuralNetTorch_r121_BAG_L2 to fail during training... Skipping this model.\n",
      "\t\t\u001b[36mray::_ray_fit()\u001b[39m (pid=800457, ip=172.17.0.5)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 413, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 201, in _fit\n",
      "    train_dataset = self._generate_dataset(X, y, train_params=processor_kwargs, is_train=True)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 669, in _generate_dataset\n",
      "    dataset = self._process_train_data(\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 734, in _process_train_data\n",
      "    self.processor = create_preprocessor(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/utils/data_preprocessor.py\", line 40, in create_preprocessor\n",
      "    return ColumnTransformer(\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "TypeError: ColumnTransformer.__init__() got an unexpected keyword argument 'force_int_remainder_cols'\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 2753, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 904, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(TypeError): \u001b[36mray::_ray_fit()\u001b[39m (pid=800457, ip=172.17.0.5)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 413, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 201, in _fit\n",
      "    train_dataset = self._generate_dataset(X, y, train_params=processor_kwargs, is_train=True)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 669, in _generate_dataset\n",
      "    dataset = self._process_train_data(\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 734, in _process_train_data\n",
      "    self.processor = create_preprocessor(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/utils/data_preprocessor.py\", line 40, in create_preprocessor\n",
      "    return ColumnTransformer(\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "TypeError: ColumnTransformer.__init__() got an unexpected keyword argument 'force_int_remainder_cols'\n",
      "\tWarning: Exception caused NeuralNetTorch_r121_BAG_L2 to fail during training... Skipping this model.\n",
      "\t\t\u001b[36mray::_ray_fit()\u001b[39m (pid=800457, ip=172.17.0.5)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 413, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 201, in _fit\n",
      "    train_dataset = self._generate_dataset(X, y, train_params=processor_kwargs, is_train=True)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 669, in _generate_dataset\n",
      "    dataset = self._process_train_data(\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 734, in _process_train_data\n",
      "    self.processor = create_preprocessor(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/utils/data_preprocessor.py\", line 40, in create_preprocessor\n",
      "    return ColumnTransformer(\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "TypeError: ColumnTransformer.__init__() got an unexpected keyword argument 'force_int_remainder_cols'\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 2753, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 904, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(TypeError): \u001b[36mray::_ray_fit()\u001b[39m (pid=800457, ip=172.17.0.5)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 413, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 201, in _fit\n",
      "    train_dataset = self._generate_dataset(X, y, train_params=processor_kwargs, is_train=True)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 669, in _generate_dataset\n",
      "    dataset = self._process_train_data(\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 734, in _process_train_data\n",
      "    self.processor = create_preprocessor(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/utils/data_preprocessor.py\", line 40, in create_preprocessor\n",
      "    return ColumnTransformer(\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "TypeError: ColumnTransformer.__init__() got an unexpected keyword argument 'force_int_remainder_cols'\n",
      "2025-09-30 14:39:16,297\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-09-30 14:39:16,298\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-09-30 14:39:16,299\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "Fitting model: NeuralNetFastAI_r127_BAG_L2 ... Training model for up to 1415.99s of the 1415.97s of remaining time.\n",
      "2025-09-30 14:39:16,297\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-09-30 14:39:16,298\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-09-30 14:39:16,299\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "Fitting model: NeuralNetFastAI_r127_BAG_L2 ... Training model for up to 1415.99s of the 1415.97s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.00%)\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.00%)\n",
      "2025-09-30 14:39:21,545\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-09-30 14:39:21,545\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-09-30 14:39:21,545\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-09-30 14:39:21,545\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "\tWarning: Exception caused NeuralNetFastAI_r127_BAG_L2 to fail during training... Skipping this model.\n",
      "\t\tTask was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: f78910d7dfcaac69dbef40d7c1440a0fbe7d964a01000000, name=_ray_fit, pid=803563, memory used=0.08GB) was running was 974.53GB / 1007.45GB (0.967318), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: b4cc317b0241be4cf1b14eb89dd2ea677e1d61d3943005567966601b) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-b4cc317b0241be4cf1b14eb89dd2ea677e1d61d3943005567966601b*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t0.95\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.81\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "801328\t0.37\tray::IDLE\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "412520\t0.10\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 2753, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: f78910d7dfcaac69dbef40d7c1440a0fbe7d964a01000000, name=_ray_fit, pid=803563, memory used=0.08GB) was running was 974.53GB / 1007.45GB (0.967318), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: b4cc317b0241be4cf1b14eb89dd2ea677e1d61d3943005567966601b) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-b4cc317b0241be4cf1b14eb89dd2ea677e1d61d3943005567966601b*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t0.95\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.81\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "801328\t0.37\tray::IDLE\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "412520\t0.10\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\tWarning: Exception caused NeuralNetFastAI_r127_BAG_L2 to fail during training... Skipping this model.\n",
      "\t\tTask was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: f78910d7dfcaac69dbef40d7c1440a0fbe7d964a01000000, name=_ray_fit, pid=803563, memory used=0.08GB) was running was 974.53GB / 1007.45GB (0.967318), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: b4cc317b0241be4cf1b14eb89dd2ea677e1d61d3943005567966601b) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-b4cc317b0241be4cf1b14eb89dd2ea677e1d61d3943005567966601b*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t0.95\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.81\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "801328\t0.37\tray::IDLE\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "412520\t0.10\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 2753, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: f78910d7dfcaac69dbef40d7c1440a0fbe7d964a01000000, name=_ray_fit, pid=803563, memory used=0.08GB) was running was 974.53GB / 1007.45GB (0.967318), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: b4cc317b0241be4cf1b14eb89dd2ea677e1d61d3943005567966601b) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-b4cc317b0241be4cf1b14eb89dd2ea677e1d61d3943005567966601b*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t0.95\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.81\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "801328\t0.37\tray::IDLE\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "412520\t0.10\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Fitting model: RandomForest_r16_BAG_L2 ... Training model for up to 1408.57s of the 1408.54s of remaining time.\n",
      "Fitting model: RandomForest_r16_BAG_L2 ... Training model for up to 1408.57s of the 1408.54s of remaining time.\n",
      "\t-0.1296\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.47s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_r194_BAG_L2 ... Training model for up to 1407.97s of the 1407.95s of remaining time.\n",
      "\t-0.1296\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.47s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_r194_BAG_L2 ... Training model for up to 1407.97s of the 1407.95s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.00%)\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.00%)\n",
      "\tWarning: Exception caused NeuralNetFastAI_r194_BAG_L2 to fail during training... Skipping this model.\n",
      "\t\tTask was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: b2616b0bac6a94362e2186b6c26bf4c3f5e49d4a01000000, name=_ray_fit, pid=803792, memory used=0.10GB) was running was 974.51GB / 1007.45GB (0.967297), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 5811f452935e9f62bd28dd81b92b324dee67ba6a9c2cc3d101aa53be) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-5811f452935e9f62bd28dd81b92b324dee67ba6a9c2cc3d101aa53be*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.12\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.81\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "803792\t0.10\tray::IDLE\n",
      "412520\t0.10\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 2753, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: b2616b0bac6a94362e2186b6c26bf4c3f5e49d4a01000000, name=_ray_fit, pid=803792, memory used=0.10GB) was running was 974.51GB / 1007.45GB (0.967297), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 5811f452935e9f62bd28dd81b92b324dee67ba6a9c2cc3d101aa53be) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-5811f452935e9f62bd28dd81b92b324dee67ba6a9c2cc3d101aa53be*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.12\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.81\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "803792\t0.10\tray::IDLE\n",
      "412520\t0.10\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\tWarning: Exception caused NeuralNetFastAI_r194_BAG_L2 to fail during training... Skipping this model.\n",
      "\t\tTask was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: b2616b0bac6a94362e2186b6c26bf4c3f5e49d4a01000000, name=_ray_fit, pid=803792, memory used=0.10GB) was running was 974.51GB / 1007.45GB (0.967297), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 5811f452935e9f62bd28dd81b92b324dee67ba6a9c2cc3d101aa53be) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-5811f452935e9f62bd28dd81b92b324dee67ba6a9c2cc3d101aa53be*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.12\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.81\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "803792\t0.10\tray::IDLE\n",
      "412520\t0.10\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 2753, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: b2616b0bac6a94362e2186b6c26bf4c3f5e49d4a01000000, name=_ray_fit, pid=803792, memory used=0.10GB) was running was 974.51GB / 1007.45GB (0.967297), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 5811f452935e9f62bd28dd81b92b324dee67ba6a9c2cc3d101aa53be) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-5811f452935e9f62bd28dd81b92b324dee67ba6a9c2cc3d101aa53be*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.12\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.81\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "803792\t0.10\tray::IDLE\n",
      "412520\t0.10\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "2025-09-30 14:39:25,160\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "Fitting model: CatBoost_r12_BAG_L2 ... Training model for up to 1407.13s of the 1407.11s of remaining time.\n",
      "2025-09-30 14:39:25,160\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "Fitting model: CatBoost_r12_BAG_L2 ... Training model for up to 1407.13s of the 1407.11s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.08%)\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.08%)\n",
      "\tWarning: Exception caused CatBoost_r12_BAG_L2 to fail during training... Skipping this model.\n",
      "\t\tTask was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: e613283029ddf3fb97b264257845402cf9b2751401000000, name=_ray_fit, pid=803994, memory used=0.10GB) was running was 974.40GB / 1007.45GB (0.967191), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 9ac3c53b430542e8b62029c22ea30d36c5acea3f6fc86c302f473d08) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-9ac3c53b430542e8b62029c22ea30d36c5acea3f6fc86c302f473d08*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.12\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.81\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "803994\t0.10\tray::IDLE\n",
      "412520\t0.10\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 2753, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: e613283029ddf3fb97b264257845402cf9b2751401000000, name=_ray_fit, pid=803994, memory used=0.10GB) was running was 974.40GB / 1007.45GB (0.967191), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 9ac3c53b430542e8b62029c22ea30d36c5acea3f6fc86c302f473d08) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-9ac3c53b430542e8b62029c22ea30d36c5acea3f6fc86c302f473d08*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.12\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.81\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "803994\t0.10\tray::IDLE\n",
      "412520\t0.10\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\tWarning: Exception caused CatBoost_r12_BAG_L2 to fail during training... Skipping this model.\n",
      "\t\tTask was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: e613283029ddf3fb97b264257845402cf9b2751401000000, name=_ray_fit, pid=803994, memory used=0.10GB) was running was 974.40GB / 1007.45GB (0.967191), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 9ac3c53b430542e8b62029c22ea30d36c5acea3f6fc86c302f473d08) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-9ac3c53b430542e8b62029c22ea30d36c5acea3f6fc86c302f473d08*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.12\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.81\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "803994\t0.10\tray::IDLE\n",
      "412520\t0.10\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 2753, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: e613283029ddf3fb97b264257845402cf9b2751401000000, name=_ray_fit, pid=803994, memory used=0.10GB) was running was 974.40GB / 1007.45GB (0.967191), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 9ac3c53b430542e8b62029c22ea30d36c5acea3f6fc86c302f473d08) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-9ac3c53b430542e8b62029c22ea30d36c5acea3f6fc86c302f473d08*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.12\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.81\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "803994\t0.10\tray::IDLE\n",
      "412520\t0.10\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "2025-09-30 14:39:25,680\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-09-30 14:39:25,680\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-09-30 14:39:25,681\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-09-30 14:39:25,682\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-09-30 14:39:25,682\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-09-30 14:39:25,683\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-09-30 14:39:25,684\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "Fitting model: NeuralNetTorch_r135_BAG_L2 ... Training model for up to 1406.61s of the 1406.59s of remaining time.\n",
      "2025-09-30 14:39:25,680\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-09-30 14:39:25,680\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-09-30 14:39:25,681\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-09-30 14:39:25,682\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-09-30 14:39:25,682\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-09-30 14:39:25,683\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-09-30 14:39:25,684\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "Fitting model: NeuralNetTorch_r135_BAG_L2 ... Training model for up to 1406.61s of the 1406.59s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.00%)\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.00%)\n",
      "\tWarning: Exception caused NeuralNetTorch_r135_BAG_L2 to fail during training... Skipping this model.\n",
      "\t\t\u001b[36mray::_ray_fit()\u001b[39m (pid=804956, ip=172.17.0.5)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 413, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 201, in _fit\n",
      "    train_dataset = self._generate_dataset(X, y, train_params=processor_kwargs, is_train=True)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 669, in _generate_dataset\n",
      "    dataset = self._process_train_data(\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 734, in _process_train_data\n",
      "    self.processor = create_preprocessor(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/utils/data_preprocessor.py\", line 40, in create_preprocessor\n",
      "    return ColumnTransformer(\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "TypeError: ColumnTransformer.__init__() got an unexpected keyword argument 'force_int_remainder_cols'\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 2753, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 904, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(TypeError): \u001b[36mray::_ray_fit()\u001b[39m (pid=804956, ip=172.17.0.5)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 413, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 201, in _fit\n",
      "    train_dataset = self._generate_dataset(X, y, train_params=processor_kwargs, is_train=True)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 669, in _generate_dataset\n",
      "    dataset = self._process_train_data(\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 734, in _process_train_data\n",
      "    self.processor = create_preprocessor(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/utils/data_preprocessor.py\", line 40, in create_preprocessor\n",
      "    return ColumnTransformer(\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "TypeError: ColumnTransformer.__init__() got an unexpected keyword argument 'force_int_remainder_cols'\n",
      "\tWarning: Exception caused NeuralNetTorch_r135_BAG_L2 to fail during training... Skipping this model.\n",
      "\t\t\u001b[36mray::_ray_fit()\u001b[39m (pid=804956, ip=172.17.0.5)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 413, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 201, in _fit\n",
      "    train_dataset = self._generate_dataset(X, y, train_params=processor_kwargs, is_train=True)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 669, in _generate_dataset\n",
      "    dataset = self._process_train_data(\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 734, in _process_train_data\n",
      "    self.processor = create_preprocessor(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/utils/data_preprocessor.py\", line 40, in create_preprocessor\n",
      "    return ColumnTransformer(\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "TypeError: ColumnTransformer.__init__() got an unexpected keyword argument 'force_int_remainder_cols'\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 2753, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 904, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(TypeError): \u001b[36mray::_ray_fit()\u001b[39m (pid=804956, ip=172.17.0.5)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 413, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 201, in _fit\n",
      "    train_dataset = self._generate_dataset(X, y, train_params=processor_kwargs, is_train=True)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 669, in _generate_dataset\n",
      "    dataset = self._process_train_data(\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 734, in _process_train_data\n",
      "    self.processor = create_preprocessor(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/utils/data_preprocessor.py\", line 40, in create_preprocessor\n",
      "    return ColumnTransformer(\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "TypeError: ColumnTransformer.__init__() got an unexpected keyword argument 'force_int_remainder_cols'\n",
      "Fitting model: NeuralNetFastAI_r4_BAG_L2 ... Training model for up to 1403.46s of the 1403.43s of remaining time.\n",
      "Fitting model: NeuralNetFastAI_r4_BAG_L2 ... Training model for up to 1403.46s of the 1403.43s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.00%)\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.00%)\n",
      "2025-09-30 14:39:34,548\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-09-30 14:39:34,549\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-09-30 14:39:34,550\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-09-30 14:39:34,550\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-09-30 14:39:34,548\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-09-30 14:39:34,549\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-09-30 14:39:34,550\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-09-30 14:39:34,550\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "\tWarning: Exception caused NeuralNetFastAI_r4_BAG_L2 to fail during training... Skipping this model.\n",
      "\t\tTask was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: bd8adf19712678accf8ba3ab5e0c01776197ac0701000000, name=_ray_fit, pid=808296, memory used=0.33GB) was running was 974.25GB / 1007.45GB (0.967044), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 261e6abf88d9d88c5ad19573e7fe6f5d519c094889a69fc04e9ee0fa) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-261e6abf88d9d88c5ad19573e7fe6f5d519c094889a69fc04e9ee0fa*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t0.98\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.81\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "808296\t0.33\tray::_ray_fit\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "412520\t0.10\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 2753, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: bd8adf19712678accf8ba3ab5e0c01776197ac0701000000, name=_ray_fit, pid=808296, memory used=0.33GB) was running was 974.25GB / 1007.45GB (0.967044), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 261e6abf88d9d88c5ad19573e7fe6f5d519c094889a69fc04e9ee0fa) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-261e6abf88d9d88c5ad19573e7fe6f5d519c094889a69fc04e9ee0fa*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t0.98\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.81\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "808296\t0.33\tray::_ray_fit\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "412520\t0.10\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\tWarning: Exception caused NeuralNetFastAI_r4_BAG_L2 to fail during training... Skipping this model.\n",
      "\t\tTask was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: bd8adf19712678accf8ba3ab5e0c01776197ac0701000000, name=_ray_fit, pid=808296, memory used=0.33GB) was running was 974.25GB / 1007.45GB (0.967044), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 261e6abf88d9d88c5ad19573e7fe6f5d519c094889a69fc04e9ee0fa) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-261e6abf88d9d88c5ad19573e7fe6f5d519c094889a69fc04e9ee0fa*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t0.98\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.81\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "808296\t0.33\tray::_ray_fit\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "412520\t0.10\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 2753, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: bd8adf19712678accf8ba3ab5e0c01776197ac0701000000, name=_ray_fit, pid=808296, memory used=0.33GB) was running was 974.25GB / 1007.45GB (0.967044), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 261e6abf88d9d88c5ad19573e7fe6f5d519c094889a69fc04e9ee0fa) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-261e6abf88d9d88c5ad19573e7fe6f5d519c094889a69fc04e9ee0fa*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t0.98\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.81\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "808296\t0.33\tray::_ray_fit\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "412520\t0.10\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Fitting model: ExtraTrees_r126_BAG_L2 ... Training model for up to 1392.87s of the 1392.85s of remaining time.\n",
      "Fitting model: ExtraTrees_r126_BAG_L2 ... Training model for up to 1392.87s of the 1392.85s of remaining time.\n",
      "\t-0.1162\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.32s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_r36_BAG_L2 ... Training model for up to 1392.42s of the 1392.40s of remaining time.\n",
      "\t-0.1162\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.32s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_r36_BAG_L2 ... Training model for up to 1392.42s of the 1392.40s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.00%)\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.00%)\n",
      "\tWarning: Exception caused NeuralNetTorch_r36_BAG_L2 to fail during training... Skipping this model.\n",
      "\t\t\u001b[36mray::_ray_fit()\u001b[39m (pid=808929, ip=172.17.0.5)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 413, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 201, in _fit\n",
      "    train_dataset = self._generate_dataset(X, y, train_params=processor_kwargs, is_train=True)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 669, in _generate_dataset\n",
      "    dataset = self._process_train_data(\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 734, in _process_train_data\n",
      "    self.processor = create_preprocessor(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/utils/data_preprocessor.py\", line 40, in create_preprocessor\n",
      "    return ColumnTransformer(\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "TypeError: ColumnTransformer.__init__() got an unexpected keyword argument 'force_int_remainder_cols'\n",
      "\tWarning: Exception caused NeuralNetTorch_r36_BAG_L2 to fail during training... Skipping this model.\n",
      "\t\t\u001b[36mray::_ray_fit()\u001b[39m (pid=808929, ip=172.17.0.5)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 413, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 201, in _fit\n",
      "    train_dataset = self._generate_dataset(X, y, train_params=processor_kwargs, is_train=True)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 669, in _generate_dataset\n",
      "    dataset = self._process_train_data(\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 734, in _process_train_data\n",
      "    self.processor = create_preprocessor(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/utils/data_preprocessor.py\", line 40, in create_preprocessor\n",
      "    return ColumnTransformer(\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "TypeError: ColumnTransformer.__init__() got an unexpected keyword argument 'force_int_remainder_cols'\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 2753, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 904, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(TypeError): \u001b[36mray::_ray_fit()\u001b[39m (pid=808929, ip=172.17.0.5)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 413, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 201, in _fit\n",
      "    train_dataset = self._generate_dataset(X, y, train_params=processor_kwargs, is_train=True)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 669, in _generate_dataset\n",
      "    dataset = self._process_train_data(\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 734, in _process_train_data\n",
      "    self.processor = create_preprocessor(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/utils/data_preprocessor.py\", line 40, in create_preprocessor\n",
      "    return ColumnTransformer(\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "TypeError: ColumnTransformer.__init__() got an unexpected keyword argument 'force_int_remainder_cols'\n",
      "Fitting model: NeuralNetFastAI_r100_BAG_L2 ... Training model for up to 1388.60s of the 1388.57s of remaining time.\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 2753, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 904, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(TypeError): \u001b[36mray::_ray_fit()\u001b[39m (pid=808929, ip=172.17.0.5)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 413, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 201, in _fit\n",
      "    train_dataset = self._generate_dataset(X, y, train_params=processor_kwargs, is_train=True)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 669, in _generate_dataset\n",
      "    dataset = self._process_train_data(\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 734, in _process_train_data\n",
      "    self.processor = create_preprocessor(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/utils/data_preprocessor.py\", line 40, in create_preprocessor\n",
      "    return ColumnTransformer(\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "TypeError: ColumnTransformer.__init__() got an unexpected keyword argument 'force_int_remainder_cols'\n",
      "Fitting model: NeuralNetFastAI_r100_BAG_L2 ... Training model for up to 1388.60s of the 1388.57s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.00%)\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.00%)\n",
      "2025-09-30 14:39:49,553\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-09-30 14:39:49,555\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-09-30 14:39:49,553\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-09-30 14:39:49,555\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "\tWarning: Exception caused NeuralNetFastAI_r100_BAG_L2 to fail during training... Skipping this model.\n",
      "\t\tTask was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: 4f341012b98106e6e9f53bd02cd389e262e0442401000000, name=_ray_fit, pid=812641, memory used=0.40GB) was running was 975.48GB / 1007.45GB (0.968265), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 006ccea4a8f56e8de296a88143cb217326c696770a954713df9f6b5e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-006ccea4a8f56e8de296a88143cb217326c696770a954713df9f6b5e*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t0.98\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.81\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "812641\t0.40\tray::_ray_fit\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "412520\t0.10\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 2753, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: 4f341012b98106e6e9f53bd02cd389e262e0442401000000, name=_ray_fit, pid=812641, memory used=0.40GB) was running was 975.48GB / 1007.45GB (0.968265), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 006ccea4a8f56e8de296a88143cb217326c696770a954713df9f6b5e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-006ccea4a8f56e8de296a88143cb217326c696770a954713df9f6b5e*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t0.98\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.81\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "812641\t0.40\tray::_ray_fit\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "412520\t0.10\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\tWarning: Exception caused NeuralNetFastAI_r100_BAG_L2 to fail during training... Skipping this model.\n",
      "\t\tTask was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: 4f341012b98106e6e9f53bd02cd389e262e0442401000000, name=_ray_fit, pid=812641, memory used=0.40GB) was running was 975.48GB / 1007.45GB (0.968265), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 006ccea4a8f56e8de296a88143cb217326c696770a954713df9f6b5e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-006ccea4a8f56e8de296a88143cb217326c696770a954713df9f6b5e*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t0.98\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.81\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "812641\t0.40\tray::_ray_fit\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "412520\t0.10\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 2753, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: 4f341012b98106e6e9f53bd02cd389e262e0442401000000, name=_ray_fit, pid=812641, memory used=0.40GB) was running was 975.48GB / 1007.45GB (0.968265), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 006ccea4a8f56e8de296a88143cb217326c696770a954713df9f6b5e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-006ccea4a8f56e8de296a88143cb217326c696770a954713df9f6b5e*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t0.98\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.81\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "812641\t0.40\tray::_ray_fit\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "412520\t0.10\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Fitting model: CatBoost_r163_BAG_L2 ... Training model for up to 1373.30s of the 1373.28s of remaining time.\n",
      "Fitting model: CatBoost_r163_BAG_L2 ... Training model for up to 1373.30s of the 1373.28s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.02%)\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.02%)\n",
      "\tWarning: Exception caused CatBoost_r163_BAG_L2 to fail during training... Skipping this model.\n",
      "\t\tTask was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: 56d2c74efbbe1d88304d91b3dacd02f7800cae7901000000, name=_ray_fit, pid=813480, memory used=0.25GB) was running was 975.46GB / 1007.45GB (0.968241), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 2e6c449024d7f75cf19b067ec3a90b17d9e2357bec124be5e7f37204) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-2e6c449024d7f75cf19b067ec3a90b17d9e2357bec124be5e7f37204*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.12\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.81\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "813480\t0.25\tray::_ray_fit\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "412520\t0.10\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 2753, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: 56d2c74efbbe1d88304d91b3dacd02f7800cae7901000000, name=_ray_fit, pid=813480, memory used=0.25GB) was running was 975.46GB / 1007.45GB (0.968241), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 2e6c449024d7f75cf19b067ec3a90b17d9e2357bec124be5e7f37204) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-2e6c449024d7f75cf19b067ec3a90b17d9e2357bec124be5e7f37204*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.12\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.81\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "813480\t0.25\tray::_ray_fit\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "412520\t0.10\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\tWarning: Exception caused CatBoost_r163_BAG_L2 to fail during training... Skipping this model.\n",
      "\t\tTask was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: 56d2c74efbbe1d88304d91b3dacd02f7800cae7901000000, name=_ray_fit, pid=813480, memory used=0.25GB) was running was 975.46GB / 1007.45GB (0.968241), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 2e6c449024d7f75cf19b067ec3a90b17d9e2357bec124be5e7f37204) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-2e6c449024d7f75cf19b067ec3a90b17d9e2357bec124be5e7f37204*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.12\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.81\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "813480\t0.25\tray::_ray_fit\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "412520\t0.10\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 2753, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: 56d2c74efbbe1d88304d91b3dacd02f7800cae7901000000, name=_ray_fit, pid=813480, memory used=0.25GB) was running was 975.46GB / 1007.45GB (0.968241), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 2e6c449024d7f75cf19b067ec3a90b17d9e2357bec124be5e7f37204) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-2e6c449024d7f75cf19b067ec3a90b17d9e2357bec124be5e7f37204*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.12\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.81\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "813480\t0.25\tray::_ray_fit\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "412520\t0.10\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Fitting model: CatBoost_r198_BAG_L2 ... Training model for up to 1369.36s of the 1369.33s of remaining time.\n",
      "Fitting model: CatBoost_r198_BAG_L2 ... Training model for up to 1369.36s of the 1369.33s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.04%)\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.04%)\n",
      "\tWarning: Exception caused CatBoost_r198_BAG_L2 to fail during training... Skipping this model.\n",
      "\t\tTask was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: 0ebf8ae5eb9375cb39a2b3a81e9ae5cb2c99161c01000000, name=_ray_fit, pid=815694, memory used=0.24GB) was running was 975.78GB / 1007.45GB (0.968561), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 94c65cc7f2459885f63efa7c491cfce5d71706d55e11c639615b58b3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-94c65cc7f2459885f63efa7c491cfce5d71706d55e11c639615b58b3*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.12\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.81\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "815694\t0.24\tray::_ray_fit\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "412520\t0.10\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 2753, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: 0ebf8ae5eb9375cb39a2b3a81e9ae5cb2c99161c01000000, name=_ray_fit, pid=815694, memory used=0.24GB) was running was 975.78GB / 1007.45GB (0.968561), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 94c65cc7f2459885f63efa7c491cfce5d71706d55e11c639615b58b3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-94c65cc7f2459885f63efa7c491cfce5d71706d55e11c639615b58b3*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.12\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.81\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "815694\t0.24\tray::_ray_fit\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "412520\t0.10\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\tWarning: Exception caused CatBoost_r198_BAG_L2 to fail during training... Skipping this model.\n",
      "\t\tTask was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: 0ebf8ae5eb9375cb39a2b3a81e9ae5cb2c99161c01000000, name=_ray_fit, pid=815694, memory used=0.24GB) was running was 975.78GB / 1007.45GB (0.968561), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 94c65cc7f2459885f63efa7c491cfce5d71706d55e11c639615b58b3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-94c65cc7f2459885f63efa7c491cfce5d71706d55e11c639615b58b3*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.12\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.81\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "815694\t0.24\tray::_ray_fit\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "412520\t0.10\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 2753, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: 0ebf8ae5eb9375cb39a2b3a81e9ae5cb2c99161c01000000, name=_ray_fit, pid=815694, memory used=0.24GB) was running was 975.78GB / 1007.45GB (0.968561), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 94c65cc7f2459885f63efa7c491cfce5d71706d55e11c639615b58b3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-94c65cc7f2459885f63efa7c491cfce5d71706d55e11c639615b58b3*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.12\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.81\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "815694\t0.24\tray::_ray_fit\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "412520\t0.10\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Fitting model: NeuralNetFastAI_r187_BAG_L2 ... Training model for up to 1364.31s of the 1364.29s of remaining time.\n",
      "Fitting model: NeuralNetFastAI_r187_BAG_L2 ... Training model for up to 1364.31s of the 1364.29s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.00%)\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.00%)\n",
      "\tWarning: Exception caused NeuralNetFastAI_r187_BAG_L2 to fail during training... Skipping this model.\n",
      "\t\tTask was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: fb221e6c94695ae027fba6234aaf4d7e42ac399f01000000, name=_ray_fit, pid=819194, memory used=0.08GB) was running was 976.21GB / 1007.45GB (0.968994), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7219e1f5cb151699775681f3983f33f39360d7055faa22a243e21942) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-7219e1f5cb151699775681f3983f33f39360d7055faa22a243e21942*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t0.98\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.81\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "817727\t0.38\tray::IDLE\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "412520\t0.10\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 2753, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: fb221e6c94695ae027fba6234aaf4d7e42ac399f01000000, name=_ray_fit, pid=819194, memory used=0.08GB) was running was 976.21GB / 1007.45GB (0.968994), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7219e1f5cb151699775681f3983f33f39360d7055faa22a243e21942) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-7219e1f5cb151699775681f3983f33f39360d7055faa22a243e21942*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t0.98\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.81\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "817727\t0.38\tray::IDLE\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "412520\t0.10\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\tWarning: Exception caused NeuralNetFastAI_r187_BAG_L2 to fail during training... Skipping this model.\n",
      "\t\tTask was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: fb221e6c94695ae027fba6234aaf4d7e42ac399f01000000, name=_ray_fit, pid=819194, memory used=0.08GB) was running was 976.21GB / 1007.45GB (0.968994), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7219e1f5cb151699775681f3983f33f39360d7055faa22a243e21942) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-7219e1f5cb151699775681f3983f33f39360d7055faa22a243e21942*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t0.98\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.81\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "817727\t0.38\tray::IDLE\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "412520\t0.10\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 2753, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: fb221e6c94695ae027fba6234aaf4d7e42ac399f01000000, name=_ray_fit, pid=819194, memory used=0.08GB) was running was 976.21GB / 1007.45GB (0.968994), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7219e1f5cb151699775681f3983f33f39360d7055faa22a243e21942) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-7219e1f5cb151699775681f3983f33f39360d7055faa22a243e21942*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t0.98\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.81\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "817727\t0.38\tray::IDLE\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "412520\t0.10\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Fitting model: NeuralNetTorch_r19_BAG_L2 ... Training model for up to 1356.74s of the 1356.71s of remaining time.\n",
      "Fitting model: NeuralNetTorch_r19_BAG_L2 ... Training model for up to 1356.74s of the 1356.71s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.00%)\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.00%)\n",
      "\tWarning: Exception caused NeuralNetTorch_r19_BAG_L2 to fail during training... Skipping this model.\n",
      "\t\t\u001b[36mray::_ray_fit()\u001b[39m (pid=819548, ip=172.17.0.5)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 413, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 201, in _fit\n",
      "    train_dataset = self._generate_dataset(X, y, train_params=processor_kwargs, is_train=True)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 669, in _generate_dataset\n",
      "    dataset = self._process_train_data(\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 734, in _process_train_data\n",
      "    self.processor = create_preprocessor(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/utils/data_preprocessor.py\", line 40, in create_preprocessor\n",
      "    return ColumnTransformer(\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "TypeError: ColumnTransformer.__init__() got an unexpected keyword argument 'force_int_remainder_cols'\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 2753, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 904, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(TypeError): \u001b[36mray::_ray_fit()\u001b[39m (pid=819548, ip=172.17.0.5)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 413, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 201, in _fit\n",
      "    train_dataset = self._generate_dataset(X, y, train_params=processor_kwargs, is_train=True)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 669, in _generate_dataset\n",
      "    dataset = self._process_train_data(\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 734, in _process_train_data\n",
      "    self.processor = create_preprocessor(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/utils/data_preprocessor.py\", line 40, in create_preprocessor\n",
      "    return ColumnTransformer(\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "TypeError: ColumnTransformer.__init__() got an unexpected keyword argument 'force_int_remainder_cols'\n",
      "\tWarning: Exception caused NeuralNetTorch_r19_BAG_L2 to fail during training... Skipping this model.\n",
      "\t\t\u001b[36mray::_ray_fit()\u001b[39m (pid=819548, ip=172.17.0.5)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 413, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 201, in _fit\n",
      "    train_dataset = self._generate_dataset(X, y, train_params=processor_kwargs, is_train=True)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 669, in _generate_dataset\n",
      "    dataset = self._process_train_data(\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 734, in _process_train_data\n",
      "    self.processor = create_preprocessor(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/utils/data_preprocessor.py\", line 40, in create_preprocessor\n",
      "    return ColumnTransformer(\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "TypeError: ColumnTransformer.__init__() got an unexpected keyword argument 'force_int_remainder_cols'\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 2753, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 904, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(TypeError): \u001b[36mray::_ray_fit()\u001b[39m (pid=819548, ip=172.17.0.5)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 413, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 201, in _fit\n",
      "    train_dataset = self._generate_dataset(X, y, train_params=processor_kwargs, is_train=True)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 669, in _generate_dataset\n",
      "    dataset = self._process_train_data(\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 734, in _process_train_data\n",
      "    self.processor = create_preprocessor(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/utils/data_preprocessor.py\", line 40, in create_preprocessor\n",
      "    return ColumnTransformer(\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "TypeError: ColumnTransformer.__init__() got an unexpected keyword argument 'force_int_remainder_cols'\n",
      "Fitting model: XGBoost_r95_BAG_L2 ... Training model for up to 1353.10s of the 1353.08s of remaining time.\n",
      "Fitting model: XGBoost_r95_BAG_L2 ... Training model for up to 1353.10s of the 1353.08s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.04%)\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.04%)\n",
      "2025-09-30 14:40:24,562\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-09-30 14:40:24,563\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-09-30 14:40:24,562\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-09-30 14:40:24,563\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "\tWarning: Exception caused XGBoost_r95_BAG_L2 to fail during training... Skipping this model.\n",
      "\t\tTask was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: c20f1a783f9c949ed2edb9ed2e7f0ffeb22611d301000000, name=_ray_fit, pid=822419, memory used=0.08GB) was running was 976.67GB / 1007.45GB (0.969443), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ad2d49476e99d54b66ba8edaec878f3adef1d66c7518e606f1695ab5) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-ad2d49476e99d54b66ba8edaec878f3adef1d66c7518e606f1695ab5*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.12\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.81\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "820870\t0.17\t\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "412520\t0.10\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 2753, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: c20f1a783f9c949ed2edb9ed2e7f0ffeb22611d301000000, name=_ray_fit, pid=822419, memory used=0.08GB) was running was 976.67GB / 1007.45GB (0.969443), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ad2d49476e99d54b66ba8edaec878f3adef1d66c7518e606f1695ab5) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-ad2d49476e99d54b66ba8edaec878f3adef1d66c7518e606f1695ab5*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.12\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.81\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "820870\t0.17\t\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "412520\t0.10\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\tWarning: Exception caused XGBoost_r95_BAG_L2 to fail during training... Skipping this model.\n",
      "\t\tTask was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: c20f1a783f9c949ed2edb9ed2e7f0ffeb22611d301000000, name=_ray_fit, pid=822419, memory used=0.08GB) was running was 976.67GB / 1007.45GB (0.969443), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ad2d49476e99d54b66ba8edaec878f3adef1d66c7518e606f1695ab5) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-ad2d49476e99d54b66ba8edaec878f3adef1d66c7518e606f1695ab5*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.12\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.81\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "820870\t0.17\t\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "412520\t0.10\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 2753, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: c20f1a783f9c949ed2edb9ed2e7f0ffeb22611d301000000, name=_ray_fit, pid=822419, memory used=0.08GB) was running was 976.67GB / 1007.45GB (0.969443), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ad2d49476e99d54b66ba8edaec878f3adef1d66c7518e606f1695ab5) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-ad2d49476e99d54b66ba8edaec878f3adef1d66c7518e606f1695ab5*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.12\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.81\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "820870\t0.17\t\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "412520\t0.10\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Fitting model: XGBoost_r34_BAG_L2 ... Training model for up to 1346.65s of the 1346.62s of remaining time.\n",
      "Fitting model: XGBoost_r34_BAG_L2 ... Training model for up to 1346.65s of the 1346.62s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.33%)\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.33%)\n",
      "\tWarning: Exception caused XGBoost_r34_BAG_L2 to fail during training... Skipping this model.\n",
      "\t\tTask was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: 86dfb8222171ae1c474c76e6fef0b793c464bfcb01000000, name=_ray_fit, pid=825455, memory used=0.10GB) was running was 977.12GB / 1007.45GB (0.969895), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e2095fd2bb5b593b876b94809755e63a7ffbb8b9c7f90d61ae006cb2) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-e2095fd2bb5b593b876b94809755e63a7ffbb8b9c7f90d61ae006cb2*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.12\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.81\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "825455\t0.10\tray::IDLE\n",
      "412520\t0.10\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 2753, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: 86dfb8222171ae1c474c76e6fef0b793c464bfcb01000000, name=_ray_fit, pid=825455, memory used=0.10GB) was running was 977.12GB / 1007.45GB (0.969895), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e2095fd2bb5b593b876b94809755e63a7ffbb8b9c7f90d61ae006cb2) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-e2095fd2bb5b593b876b94809755e63a7ffbb8b9c7f90d61ae006cb2*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.12\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.81\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "825455\t0.10\tray::IDLE\n",
      "412520\t0.10\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\tWarning: Exception caused XGBoost_r34_BAG_L2 to fail during training... Skipping this model.\n",
      "\t\tTask was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: 86dfb8222171ae1c474c76e6fef0b793c464bfcb01000000, name=_ray_fit, pid=825455, memory used=0.10GB) was running was 977.12GB / 1007.45GB (0.969895), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e2095fd2bb5b593b876b94809755e63a7ffbb8b9c7f90d61ae006cb2) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-e2095fd2bb5b593b876b94809755e63a7ffbb8b9c7f90d61ae006cb2*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.12\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.81\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "825455\t0.10\tray::IDLE\n",
      "412520\t0.10\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 2753, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: 86dfb8222171ae1c474c76e6fef0b793c464bfcb01000000, name=_ray_fit, pid=825455, memory used=0.10GB) was running was 977.12GB / 1007.45GB (0.969895), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e2095fd2bb5b593b876b94809755e63a7ffbb8b9c7f90d61ae006cb2) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-e2095fd2bb5b593b876b94809755e63a7ffbb8b9c7f90d61ae006cb2*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t1.12\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.81\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "825455\t0.10\tray::IDLE\n",
      "412520\t0.10\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Fitting model: LightGBM_r42_BAG_L2 ... Training model for up to 1327.82s of the 1327.79s of remaining time.\n",
      "Fitting model: LightGBM_r42_BAG_L2 ... Training model for up to 1327.82s of the 1327.79s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.13%)\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.13%)\n",
      "\tWarning: Exception caused LightGBM_r42_BAG_L2 to fail during training... Skipping this model.\n",
      "\t\tTask was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: cd2e6440e6d404b58bf475d2754ead104ad0cbb001000000, name=_ray_fit, pid=827285, memory used=0.08GB) was running was 976.62GB / 1007.45GB (0.969397), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 02dc4bdd5fe5bf052312af52359208cb91dab0884ce0a29f5731b4e9) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-02dc4bdd5fe5bf052312af52359208cb91dab0884ce0a29f5731b4e9*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t0.97\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.81\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "443\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-lab --ip=* --port=8888 --no-browser --LabApp.token= --LabApp...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 2753, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: cd2e6440e6d404b58bf475d2754ead104ad0cbb001000000, name=_ray_fit, pid=827285, memory used=0.08GB) was running was 976.62GB / 1007.45GB (0.969397), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 02dc4bdd5fe5bf052312af52359208cb91dab0884ce0a29f5731b4e9) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-02dc4bdd5fe5bf052312af52359208cb91dab0884ce0a29f5731b4e9*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t0.97\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.81\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "443\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-lab --ip=* --port=8888 --no-browser --LabApp.token= --LabApp...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\tWarning: Exception caused LightGBM_r42_BAG_L2 to fail during training... Skipping this model.\n",
      "\t\tTask was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: cd2e6440e6d404b58bf475d2754ead104ad0cbb001000000, name=_ray_fit, pid=827285, memory used=0.08GB) was running was 976.62GB / 1007.45GB (0.969397), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 02dc4bdd5fe5bf052312af52359208cb91dab0884ce0a29f5731b4e9) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-02dc4bdd5fe5bf052312af52359208cb91dab0884ce0a29f5731b4e9*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t0.97\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.81\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "443\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-lab --ip=* --port=8888 --no-browser --LabApp.token= --LabApp...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 2753, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.0.5, ID: 3016365cd732022a3a0474a19fc435bddec7261ff5ea535ea40a6d92) where the task (task ID: cd2e6440e6d404b58bf475d2754ead104ad0cbb001000000, name=_ray_fit, pid=827285, memory used=0.08GB) was running was 976.62GB / 1007.45GB (0.969397), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 02dc4bdd5fe5bf052312af52359208cb91dab0884ce0a29f5731b4e9) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.5`. To see the logs of the worker, use `ray logs worker-02dc4bdd5fe5bf052312af52359208cb91dab0884ce0a29f5731b4e9*out -ip 172.17.0.5. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "464630\t1.91\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "463952\t1.59\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "521385\t0.97\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v3184a...\n",
      "469312\t0.93\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v31e3d...\n",
      "522871\t0.81\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "470996\t0.72\t/home/bt708583/.local/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ra...\n",
      "517682\t0.24\t/usr/bin/python3 -m ipykernel_launcher --f=/home/bt708583/.local/share/jupyter/runtime/kernel-v341ed...\n",
      "463963\t0.12\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "412520\t0.09\t/home/bt708583/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/nod...\n",
      "443\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-lab --ip=* --port=8888 --no-browser --LabApp.token= --LabApp...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Fitting model: NeuralNetTorch_r1_BAG_L2 ... Training model for up to 1320.39s of the 1320.36s of remaining time.\n",
      "Fitting model: NeuralNetTorch_r1_BAG_L2 ... Training model for up to 1320.39s of the 1320.36s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.00%)\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.00%)\n",
      "\tWarning: Exception caused NeuralNetTorch_r1_BAG_L2 to fail during training... Skipping this model.\n",
      "\t\t\u001b[36mray::_ray_fit()\u001b[39m (pid=827422, ip=172.17.0.5)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 413, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 201, in _fit\n",
      "    train_dataset = self._generate_dataset(X, y, train_params=processor_kwargs, is_train=True)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 669, in _generate_dataset\n",
      "    dataset = self._process_train_data(\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 734, in _process_train_data\n",
      "    self.processor = create_preprocessor(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/utils/data_preprocessor.py\", line 40, in create_preprocessor\n",
      "    return ColumnTransformer(\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "TypeError: ColumnTransformer.__init__() got an unexpected keyword argument 'force_int_remainder_cols'\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 2753, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 904, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(TypeError): \u001b[36mray::_ray_fit()\u001b[39m (pid=827422, ip=172.17.0.5)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 413, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 201, in _fit\n",
      "    train_dataset = self._generate_dataset(X, y, train_params=processor_kwargs, is_train=True)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 669, in _generate_dataset\n",
      "    dataset = self._process_train_data(\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 734, in _process_train_data\n",
      "    self.processor = create_preprocessor(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/utils/data_preprocessor.py\", line 40, in create_preprocessor\n",
      "    return ColumnTransformer(\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "TypeError: ColumnTransformer.__init__() got an unexpected keyword argument 'force_int_remainder_cols'\n",
      "\tWarning: Exception caused NeuralNetTorch_r1_BAG_L2 to fail during training... Skipping this model.\n",
      "\t\t\u001b[36mray::_ray_fit()\u001b[39m (pid=827422, ip=172.17.0.5)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 413, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 201, in _fit\n",
      "    train_dataset = self._generate_dataset(X, y, train_params=processor_kwargs, is_train=True)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 669, in _generate_dataset\n",
      "    dataset = self._process_train_data(\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 734, in _process_train_data\n",
      "    self.processor = create_preprocessor(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/utils/data_preprocessor.py\", line 40, in create_preprocessor\n",
      "    return ColumnTransformer(\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "TypeError: ColumnTransformer.__init__() got an unexpected keyword argument 'force_int_remainder_cols'\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 2753, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 904, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(TypeError): \u001b[36mray::_ray_fit()\u001b[39m (pid=827422, ip=172.17.0.5)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 413, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 201, in _fit\n",
      "    train_dataset = self._generate_dataset(X, y, train_params=processor_kwargs, is_train=True)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 669, in _generate_dataset\n",
      "    dataset = self._process_train_data(\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 734, in _process_train_data\n",
      "    self.processor = create_preprocessor(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/utils/data_preprocessor.py\", line 40, in create_preprocessor\n",
      "    return ColumnTransformer(\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "TypeError: ColumnTransformer.__init__() got an unexpected keyword argument 'force_int_remainder_cols'\n",
      "2025-09-30 14:40:56,355\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-09-30 14:40:56,355\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "Fitting model: NeuralNetTorch_r89_BAG_L2 ... Training model for up to 1315.93s of the 1315.91s of remaining time.\n",
      "Fitting model: NeuralNetTorch_r89_BAG_L2 ... Training model for up to 1315.93s of the 1315.91s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.00%)\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.00%)\n",
      "\tWarning: Exception caused NeuralNetTorch_r89_BAG_L2 to fail during training... Skipping this model.\n",
      "\t\t\u001b[36mray::_ray_fit()\u001b[39m (pid=828998, ip=172.17.0.5)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 413, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 201, in _fit\n",
      "    train_dataset = self._generate_dataset(X, y, train_params=processor_kwargs, is_train=True)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 669, in _generate_dataset\n",
      "    dataset = self._process_train_data(\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 734, in _process_train_data\n",
      "    self.processor = create_preprocessor(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/utils/data_preprocessor.py\", line 40, in create_preprocessor\n",
      "    return ColumnTransformer(\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "TypeError: ColumnTransformer.__init__() got an unexpected keyword argument 'force_int_remainder_cols'\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 2753, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 904, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(TypeError): \u001b[36mray::_ray_fit()\u001b[39m (pid=828998, ip=172.17.0.5)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 413, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 201, in _fit\n",
      "    train_dataset = self._generate_dataset(X, y, train_params=processor_kwargs, is_train=True)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 669, in _generate_dataset\n",
      "    dataset = self._process_train_data(\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 734, in _process_train_data\n",
      "    self.processor = create_preprocessor(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/utils/data_preprocessor.py\", line 40, in create_preprocessor\n",
      "    return ColumnTransformer(\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "TypeError: ColumnTransformer.__init__() got an unexpected keyword argument 'force_int_remainder_cols'\n",
      "\tWarning: Exception caused NeuralNetTorch_r89_BAG_L2 to fail during training... Skipping this model.\n",
      "\t\t\u001b[36mray::_ray_fit()\u001b[39m (pid=828998, ip=172.17.0.5)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 413, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 201, in _fit\n",
      "    train_dataset = self._generate_dataset(X, y, train_params=processor_kwargs, is_train=True)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 669, in _generate_dataset\n",
      "    dataset = self._process_train_data(\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 734, in _process_train_data\n",
      "    self.processor = create_preprocessor(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/utils/data_preprocessor.py\", line 40, in create_preprocessor\n",
      "    return ColumnTransformer(\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "TypeError: ColumnTransformer.__init__() got an unexpected keyword argument 'force_int_remainder_cols'\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 2753, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/ray/_private/worker.py\", line 904, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(TypeError): \u001b[36mray::_ray_fit()\u001b[39m (pid=828998, ip=172.17.0.5)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 413, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 201, in _fit\n",
      "    train_dataset = self._generate_dataset(X, y, train_params=processor_kwargs, is_train=True)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 669, in _generate_dataset\n",
      "    dataset = self._process_train_data(\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 734, in _process_train_data\n",
      "    self.processor = create_preprocessor(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/utils/data_preprocessor.py\", line 40, in create_preprocessor\n",
      "    return ColumnTransformer(\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "TypeError: ColumnTransformer.__init__() got an unexpected keyword argument 'force_int_remainder_cols'\n",
      "2025-09-30 14:41:00,389\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-09-30 14:41:00,389\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "Fitting model: WeightedEnsemble_L3 ... Training model for up to 360.00s of the 1311.81s of remaining time.\n",
      "Fitting model: WeightedEnsemble_L3 ... Training model for up to 360.00s of the 1311.81s of remaining time.\n",
      "\tEnsemble Weights: {'ExtraTrees_r49_BAG_L2': 0.762, 'XGBoost_BAG_L1': 0.143, 'LightGBM_BAG_L1': 0.095}\n",
      "\t-0.1098\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "\tEnsemble Weights: {'ExtraTrees_r49_BAG_L2': 0.762, 'XGBoost_BAG_L1': 0.143, 'LightGBM_BAG_L1': 0.095}\n",
      "\t-0.1098\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 4086.48s ... Best model: WeightedEnsemble_L3 | Estimated inference throughput: 10.3 rows/s (16 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"/home/bt708583/ml_in_ms_wt24/AdvancedModule/AutogluonModels/ag-20250930_130252\")\n",
      "AutoGluon training complete, total runtime = 4086.48s ... Best model: WeightedEnsemble_L3 | Estimated inference throughput: 10.3 rows/s (16 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"/home/bt708583/ml_in_ms_wt24/AdvancedModule/AutogluonModels/ag-20250930_130252\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ AutoGluon Training abgeschlossen!\n",
      "\n",
      "📋 Fit Summary:\n",
      "<autogluon.tabular.predictor.predictor.TabularPredictor object at 0x7f4099733bf0>\n"
     ]
    }
   ],
   "source": [
    "# ===== AUTOGLUON TRAINING =====\n",
    "print(\"🚀 Starte AutoGluon AutoML Training...\")\n",
    "\n",
    "# Erstelle Predictor mit erweiterten Einstellungen\n",
    "predictor = TabularPredictor(\n",
    "    label=target_column,\n",
    "    problem_type='regression',\n",
    "    # eval_metric='r2',  # Gleiche Metrik wie NN (R²)\n",
    ")\n",
    "\n",
    "# Training mit verschiedenen Modellen und Zeit-Budget\n",
    "print(\"⏰ Starte Training mit 15-Minuten Zeit-Budget...\")\n",
    "fit_summary = predictor.fit(\n",
    "    train_data,\n",
    "    time_limit=7200,  \n",
    "    presets='best_quality',  # Beste Qualität\n",
    "\n",
    ")\n",
    "\n",
    "print(\"✅ AutoGluon Training abgeschlossen!\")\n",
    "print(\"\\n📋 Fit Summary:\")\n",
    "print(fit_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f635259b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TBB Warning: The number of workers is currently limited to 40. The request for 191 workers is ignored. Further requests for more workers will be silently ignored until the limit changes.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Evaluiere AutoGluon Modelle auf dem GLEICHEN Test-Set wie das Neural Network...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🎯 FINALE TEST-SET PERFORMANCE (identisch mit NN Test-Set):\n",
      "  R² Score:  0.780060\n",
      "  MSE:       0.007919\n",
      "  RMSE:      0.088990\n",
      "  MAE:       0.066381\n",
      "\n",
      "🏆 Model Leaderboard:\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "model",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "score_test",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "score_val",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "eval_metric",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "pred_time_test",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "pred_time_val",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "fit_time",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "pred_time_test_marginal",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "pred_time_val_marginal",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "fit_time_marginal",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "stack_level",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "can_infer",
         "rawType": "bool",
         "type": "boolean"
        },
        {
         "name": "fit_order",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "510056e9-50fc-4755-9b86-69a1fce39b60",
       "rows": [
        [
         "0",
         "RandomForest_r16_BAG_L2",
         "-0.07951427569009825",
         "-0.1295871682095476",
         "root_mean_squared_error",
         "0.7601807117462158",
         "1.7957584857940674",
         "1052.100370168686",
         "0.0366671085357666",
         "0.06833767890930176",
         "0.47274112701416016",
         "2",
         "True",
         "31"
        ],
        [
         "1",
         "RandomForestMSE_BAG_L2",
         "-0.07951427569009825",
         "-0.1295871682095476",
         "root_mean_squared_error",
         "0.7642080783843994",
         "1.791137456893921",
         "1051.9650073051453",
         "0.040694475173950195",
         "0.06371665000915527",
         "0.33737826347351074",
         "2",
         "True",
         "17"
        ],
        [
         "2",
         "RandomForest_r195_BAG_L2",
         "-0.07956440413260778",
         "-0.13002331846227008",
         "root_mean_squared_error",
         "0.7639651298522949",
         "1.7924211025238037",
         "1052.067429304123",
         "0.0404515266418457",
         "0.06500029563903809",
         "0.4398002624511719",
         "2",
         "True",
         "20"
        ],
        [
         "3",
         "RandomForest_r127_BAG_L2",
         "-0.0796408970067167",
         "-0.12577246404422066",
         "root_mean_squared_error",
         "0.7600276470184326",
         "1.82891845703125",
         "1052.0201025009155",
         "0.0365140438079834",
         "0.10149765014648438",
         "0.3924734592437744",
         "2",
         "True",
         "24"
        ],
        [
         "4",
         "RandomForest_r39_BAG_L2",
         "-0.07971973732991759",
         "-0.12644570535791708",
         "root_mean_squared_error",
         "0.759047269821167",
         "1.8149397373199463",
         "1052.0344026088715",
         "0.03553366661071777",
         "0.08751893043518066",
         "0.40677356719970703",
         "2",
         "True",
         "22"
        ],
        [
         "5",
         "RandomForest_r15_BAG_L2",
         "-0.08009717123932611",
         "-0.12464806550340356",
         "root_mean_squared_error",
         "0.7739677429199219",
         "1.7892608642578125",
         "1051.9642267227173",
         "0.050454139709472656",
         "0.061840057373046875",
         "0.3365976810455322",
         "2",
         "True",
         "29"
        ],
        [
         "6",
         "RandomForest_r166_BAG_L2",
         "-0.08472952456586351",
         "-0.11690815089004747",
         "root_mean_squared_error",
         "0.7599561214447021",
         "1.8255631923675537",
         "1052.02539396286",
         "0.03644251823425293",
         "0.09814238548278809",
         "0.3977649211883545",
         "2",
         "True",
         "28"
        ],
        [
         "7",
         "ExtraTrees_r172_BAG_L2",
         "-0.085047073824771",
         "-0.1199067750171864",
         "root_mean_squared_error",
         "0.7593443393707275",
         "1.8019466400146484",
         "1051.8959124088287",
         "0.03583073616027832",
         "0.07452583312988281",
         "0.2682833671569824",
         "2",
         "True",
         "21"
        ],
        [
         "8",
         "LightGBMLarge_BAG_L1",
         "-0.08519814177588103",
         "-0.12962257758392443",
         "root_mean_squared_error",
         "0.0564725399017334",
         "0.21379756927490234",
         "1450.1024317741394",
         "0.0564725399017334",
         "0.21379756927490234",
         "1450.1024317741394",
         "1",
         "True",
         "10"
        ],
        [
         "9",
         "ExtraTrees_r42_BAG_L1",
         "-0.08601505837518374",
         "-0.12499411666949957",
         "root_mean_squared_error",
         "0.03980517387390137",
         "0.06444931030273438",
         "0.3736882209777832",
         "0.03980517387390137",
         "0.06444931030273438",
         "0.3736882209777832",
         "1",
         "True",
         "12"
        ],
        [
         "10",
         "ExtraTreesMSE_BAG_L1",
         "-0.08648253281725927",
         "-0.1262783150667517",
         "root_mean_squared_error",
         "0.036897897720336914",
         "0.07382988929748535",
         "0.29592442512512207",
         "0.036897897720336914",
         "0.07382988929748535",
         "0.29592442512512207",
         "1",
         "True",
         "7"
        ],
        [
         "11",
         "ExtraTrees_r178_BAG_L2",
         "-0.08663003213278492",
         "-0.11713922064671049",
         "root_mean_squared_error",
         "0.7580704689025879",
         "1.7892670631408691",
         "1051.9626064300537",
         "0.03455686569213867",
         "0.061846256256103516",
         "0.334977388381958",
         "2",
         "True",
         "27"
        ],
        [
         "12",
         "WeightedEnsemble_L3",
         "-0.08898962072420487",
         "-0.10983798555707235",
         "root_mean_squared_error",
         "0.7623801231384277",
         "1.8207097053527832",
         "1051.9756433963776",
         "0.0016379356384277344",
         "0.00026607513427734375",
         "0.008049726486206055",
         "3",
         "True",
         "33"
        ],
        [
         "13",
         "ExtraTrees_r42_BAG_L2",
         "-0.08906885058916439",
         "-0.11499435916310417",
         "root_mean_squared_error",
         "0.7629401683807373",
         "1.7965447902679443",
         "1051.9227442741394",
         "0.039426565170288086",
         "0.06912398338317871",
         "0.29511523246765137",
         "2",
         "True",
         "19"
        ],
        [
         "14",
         "ExtraTrees_r197_BAG_L2",
         "-0.0891550699377225",
         "-0.11608713329131858",
         "root_mean_squared_error",
         "0.7657589912414551",
         "1.7918987274169922",
         "1052.048798084259",
         "0.04224538803100586",
         "0.06447792053222656",
         "0.4211690425872803",
         "2",
         "True",
         "30"
        ],
        [
         "15",
         "ExtraTreesMSE_BAG_L2",
         "-0.0891550699377225",
         "-0.11608713329131858",
         "root_mean_squared_error",
         "0.7815759181976318",
         "1.8180129528045654",
         "1051.9598786830902",
         "0.05806231498718262",
         "0.0905921459197998",
         "0.33224964141845703",
         "2",
         "True",
         "18"
        ],
        [
         "16",
         "ExtraTrees_r49_BAG_L2",
         "-0.08973446665463403",
         "-0.11049907153936261",
         "root_mean_squared_error",
         "0.7607421875",
         "1.8204436302185059",
         "1051.9675936698914",
         "0.03722858428955078",
         "0.09302282333374023",
         "0.3399646282196045",
         "2",
         "True",
         "23"
        ],
        [
         "17",
         "WeightedEnsemble_L2",
         "-0.08990211070602984",
         "-0.11544208819753707",
         "root_mean_squared_error",
         "0.26073479652404785",
         "0.5362622737884521",
         "881.1573176383972",
         "0.0021207332611083984",
         "0.00024962425231933594",
         "0.0074961185455322266",
         "2",
         "True",
         "16"
        ],
        [
         "18",
         "ExtraTrees_r126_BAG_L2",
         "-0.09048841346864381",
         "-0.11618075017481695",
         "root_mean_squared_error",
         "0.7587137222290039",
         "1.789536476135254",
         "1051.9443745613098",
         "0.03520011901855469",
         "0.06211566925048828",
         "0.3167455196380615",
         "2",
         "True",
         "32"
        ],
        [
         "19",
         "CatBoost_r177_BAG_L1",
         "-0.09071229681541133",
         "-0.12096026177405364",
         "root_mean_squared_error",
         "0.05323600769042969",
         "0.012926340103149414",
         "40.4083731174469",
         "0.05323600769042969",
         "0.012926340103149414",
         "40.4083731174469",
         "1",
         "True",
         "11"
        ],
        [
         "20",
         "CatBoost_BAG_L1",
         "-0.09153816877102357",
         "-0.12083098388194222",
         "root_mean_squared_error",
         "0.04677844047546387",
         "0.016295909881591797",
         "40.16321325302124",
         "0.04677844047546387",
         "0.016295909881591797",
         "40.16321325302124",
         "1",
         "True",
         "6"
        ],
        [
         "21",
         "RandomForest_r195_BAG_L1",
         "-0.09399987776374205",
         "-0.12244273671674077",
         "root_mean_squared_error",
         "0.04011106491088867",
         "0.0640878677368164",
         "0.5164597034454346",
         "0.04011106491088867",
         "0.0640878677368164",
         "0.5164597034454346",
         "1",
         "True",
         "13"
        ],
        [
         "22",
         "RandomForestMSE_BAG_L1",
         "-0.09403520406198294",
         "-0.12181529735909014",
         "root_mean_squared_error",
         "0.0358431339263916",
         "0.07961678504943848",
         "0.34259629249572754",
         "0.0358431339263916",
         "0.07961678504943848",
         "0.34259629249572754",
         "1",
         "True",
         "5"
        ],
        [
         "23",
         "LightGBM_BAG_L1",
         "-0.09602020071666006",
         "-0.1216681194352808",
         "root_mean_squared_error",
         "0.11212635040283203",
         "0.1881701946258545",
         "418.7485032081604",
         "0.11212635040283203",
         "0.1881701946258545",
         "418.7485032081604",
         "1",
         "True",
         "4"
        ],
        [
         "24",
         "RandomForest_r39_BAG_L1",
         "-0.10007153502767113",
         "-0.1275391495443412",
         "root_mean_squared_error",
         "0.03933572769165039",
         "0.06716632843017578",
         "0.35524415969848633",
         "0.03933572769165039",
         "0.06716632843017578",
         "0.35524415969848633",
         "1",
         "True",
         "15"
        ],
        [
         "25",
         "XGBoost_BAG_L1",
         "-0.10009492970597128",
         "-0.1218578212481911",
         "root_mean_squared_error",
         "0.06386613845825195",
         "0.25192975997924805",
         "421.8955087661743",
         "0.06386613845825195",
         "0.25192975997924805",
         "421.8955087661743",
         "1",
         "True",
         "9"
        ],
        [
         "26",
         "LightGBMXT_BAG_L1",
         "-0.10090617959394135",
         "-0.12404908958476696",
         "root_mean_squared_error",
         "0.22156333923339844",
         "0.22371506690979004",
         "722.3284151554108",
         "0.22156333923339844",
         "0.22371506690979004",
         "722.3284151554108",
         "1",
         "True",
         "3"
        ],
        [
         "27",
         "NeuralNetFastAI_BAG_L1",
         "-0.1194383788031507",
         "-0.13637591773814553",
         "root_mean_squared_error",
         "0.3380293846130371",
         "1.0647594928741455",
         "170.10184454917908",
         "0.3380293846130371",
         "1.0647594928741455",
         "170.10184454917908",
         "1",
         "True",
         "8"
        ],
        [
         "28",
         "ExtraTrees_r172_BAG_L1",
         "-0.11991092450260411",
         "-0.15094211712313887",
         "root_mean_squared_error",
         "0.0351712703704834",
         "0.07983279228210449",
         "0.2840909957885742",
         "0.0351712703704834",
         "0.07983279228210449",
         "0.2840909957885742",
         "1",
         "True",
         "14"
        ],
        [
         "29",
         "ExtraTrees_r4_BAG_L2",
         "-0.12333249472709777",
         "-0.1376151202431158",
         "root_mean_squared_error",
         "0.7830841541290283",
         "1.7909414768218994",
         "1051.9252831935883",
         "0.0595705509185791",
         "0.06352066993713379",
         "0.2976541519165039",
         "2",
         "True",
         "26"
        ],
        [
         "30",
         "KNeighborsDist_BAG_L1",
         "-0.1253240328590794",
         "-0.13953951630262806",
         "root_mean_squared_error",
         "0.08706498146057129",
         "0.06219935417175293",
         "0.002274751663208008",
         "0.08706498146057129",
         "0.06219935417175293",
         "0.002274751663208008",
         "1",
         "True",
         "2"
        ],
        [
         "31",
         "KNeighborsUnif_BAG_L1",
         "-0.1374214961199031",
         "-0.15738245275014742",
         "root_mean_squared_error",
         "0.059476375579833984",
         "0.06176590919494629",
         "0.002630472183227539",
         "0.059476375579833984",
         "0.06176590919494629",
         "0.002630472183227539",
         "1",
         "True",
         "1"
        ],
        [
         "32",
         "RandomForest_r34_BAG_L2",
         "-0.17995692326569993",
         "-0.17310509256910545",
         "root_mean_squared_error",
         "0.7574996948242188",
         "1.7925701141357422",
         "1051.8830351829529",
         "0.03398609161376953",
         "0.06514930725097656",
         "0.25540614128112793",
         "2",
         "True",
         "25"
        ]
       ],
       "shape": {
        "columns": 13,
        "rows": 33
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>score_test</th>\n",
       "      <th>score_val</th>\n",
       "      <th>eval_metric</th>\n",
       "      <th>pred_time_test</th>\n",
       "      <th>pred_time_val</th>\n",
       "      <th>fit_time</th>\n",
       "      <th>pred_time_test_marginal</th>\n",
       "      <th>pred_time_val_marginal</th>\n",
       "      <th>fit_time_marginal</th>\n",
       "      <th>stack_level</th>\n",
       "      <th>can_infer</th>\n",
       "      <th>fit_order</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RandomForest_r16_BAG_L2</td>\n",
       "      <td>-0.079514</td>\n",
       "      <td>-0.129587</td>\n",
       "      <td>root_mean_squared_error</td>\n",
       "      <td>0.760181</td>\n",
       "      <td>1.795758</td>\n",
       "      <td>1052.100370</td>\n",
       "      <td>0.036667</td>\n",
       "      <td>0.068338</td>\n",
       "      <td>0.472741</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RandomForestMSE_BAG_L2</td>\n",
       "      <td>-0.079514</td>\n",
       "      <td>-0.129587</td>\n",
       "      <td>root_mean_squared_error</td>\n",
       "      <td>0.764208</td>\n",
       "      <td>1.791137</td>\n",
       "      <td>1051.965007</td>\n",
       "      <td>0.040694</td>\n",
       "      <td>0.063717</td>\n",
       "      <td>0.337378</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RandomForest_r195_BAG_L2</td>\n",
       "      <td>-0.079564</td>\n",
       "      <td>-0.130023</td>\n",
       "      <td>root_mean_squared_error</td>\n",
       "      <td>0.763965</td>\n",
       "      <td>1.792421</td>\n",
       "      <td>1052.067429</td>\n",
       "      <td>0.040452</td>\n",
       "      <td>0.065000</td>\n",
       "      <td>0.439800</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RandomForest_r127_BAG_L2</td>\n",
       "      <td>-0.079641</td>\n",
       "      <td>-0.125772</td>\n",
       "      <td>root_mean_squared_error</td>\n",
       "      <td>0.760028</td>\n",
       "      <td>1.828918</td>\n",
       "      <td>1052.020103</td>\n",
       "      <td>0.036514</td>\n",
       "      <td>0.101498</td>\n",
       "      <td>0.392473</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RandomForest_r39_BAG_L2</td>\n",
       "      <td>-0.079720</td>\n",
       "      <td>-0.126446</td>\n",
       "      <td>root_mean_squared_error</td>\n",
       "      <td>0.759047</td>\n",
       "      <td>1.814940</td>\n",
       "      <td>1052.034403</td>\n",
       "      <td>0.035534</td>\n",
       "      <td>0.087519</td>\n",
       "      <td>0.406774</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>RandomForest_r15_BAG_L2</td>\n",
       "      <td>-0.080097</td>\n",
       "      <td>-0.124648</td>\n",
       "      <td>root_mean_squared_error</td>\n",
       "      <td>0.773968</td>\n",
       "      <td>1.789261</td>\n",
       "      <td>1051.964227</td>\n",
       "      <td>0.050454</td>\n",
       "      <td>0.061840</td>\n",
       "      <td>0.336598</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>RandomForest_r166_BAG_L2</td>\n",
       "      <td>-0.084730</td>\n",
       "      <td>-0.116908</td>\n",
       "      <td>root_mean_squared_error</td>\n",
       "      <td>0.759956</td>\n",
       "      <td>1.825563</td>\n",
       "      <td>1052.025394</td>\n",
       "      <td>0.036443</td>\n",
       "      <td>0.098142</td>\n",
       "      <td>0.397765</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ExtraTrees_r172_BAG_L2</td>\n",
       "      <td>-0.085047</td>\n",
       "      <td>-0.119907</td>\n",
       "      <td>root_mean_squared_error</td>\n",
       "      <td>0.759344</td>\n",
       "      <td>1.801947</td>\n",
       "      <td>1051.895912</td>\n",
       "      <td>0.035831</td>\n",
       "      <td>0.074526</td>\n",
       "      <td>0.268283</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>LightGBMLarge_BAG_L1</td>\n",
       "      <td>-0.085198</td>\n",
       "      <td>-0.129623</td>\n",
       "      <td>root_mean_squared_error</td>\n",
       "      <td>0.056473</td>\n",
       "      <td>0.213798</td>\n",
       "      <td>1450.102432</td>\n",
       "      <td>0.056473</td>\n",
       "      <td>0.213798</td>\n",
       "      <td>1450.102432</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ExtraTrees_r42_BAG_L1</td>\n",
       "      <td>-0.086015</td>\n",
       "      <td>-0.124994</td>\n",
       "      <td>root_mean_squared_error</td>\n",
       "      <td>0.039805</td>\n",
       "      <td>0.064449</td>\n",
       "      <td>0.373688</td>\n",
       "      <td>0.039805</td>\n",
       "      <td>0.064449</td>\n",
       "      <td>0.373688</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>ExtraTreesMSE_BAG_L1</td>\n",
       "      <td>-0.086483</td>\n",
       "      <td>-0.126278</td>\n",
       "      <td>root_mean_squared_error</td>\n",
       "      <td>0.036898</td>\n",
       "      <td>0.073830</td>\n",
       "      <td>0.295924</td>\n",
       "      <td>0.036898</td>\n",
       "      <td>0.073830</td>\n",
       "      <td>0.295924</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>ExtraTrees_r178_BAG_L2</td>\n",
       "      <td>-0.086630</td>\n",
       "      <td>-0.117139</td>\n",
       "      <td>root_mean_squared_error</td>\n",
       "      <td>0.758070</td>\n",
       "      <td>1.789267</td>\n",
       "      <td>1051.962606</td>\n",
       "      <td>0.034557</td>\n",
       "      <td>0.061846</td>\n",
       "      <td>0.334977</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>WeightedEnsemble_L3</td>\n",
       "      <td>-0.088990</td>\n",
       "      <td>-0.109838</td>\n",
       "      <td>root_mean_squared_error</td>\n",
       "      <td>0.762380</td>\n",
       "      <td>1.820710</td>\n",
       "      <td>1051.975643</td>\n",
       "      <td>0.001638</td>\n",
       "      <td>0.000266</td>\n",
       "      <td>0.008050</td>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>ExtraTrees_r42_BAG_L2</td>\n",
       "      <td>-0.089069</td>\n",
       "      <td>-0.114994</td>\n",
       "      <td>root_mean_squared_error</td>\n",
       "      <td>0.762940</td>\n",
       "      <td>1.796545</td>\n",
       "      <td>1051.922744</td>\n",
       "      <td>0.039427</td>\n",
       "      <td>0.069124</td>\n",
       "      <td>0.295115</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>ExtraTrees_r197_BAG_L2</td>\n",
       "      <td>-0.089155</td>\n",
       "      <td>-0.116087</td>\n",
       "      <td>root_mean_squared_error</td>\n",
       "      <td>0.765759</td>\n",
       "      <td>1.791899</td>\n",
       "      <td>1052.048798</td>\n",
       "      <td>0.042245</td>\n",
       "      <td>0.064478</td>\n",
       "      <td>0.421169</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>ExtraTreesMSE_BAG_L2</td>\n",
       "      <td>-0.089155</td>\n",
       "      <td>-0.116087</td>\n",
       "      <td>root_mean_squared_error</td>\n",
       "      <td>0.781576</td>\n",
       "      <td>1.818013</td>\n",
       "      <td>1051.959879</td>\n",
       "      <td>0.058062</td>\n",
       "      <td>0.090592</td>\n",
       "      <td>0.332250</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>ExtraTrees_r49_BAG_L2</td>\n",
       "      <td>-0.089734</td>\n",
       "      <td>-0.110499</td>\n",
       "      <td>root_mean_squared_error</td>\n",
       "      <td>0.760742</td>\n",
       "      <td>1.820444</td>\n",
       "      <td>1051.967594</td>\n",
       "      <td>0.037229</td>\n",
       "      <td>0.093023</td>\n",
       "      <td>0.339965</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>WeightedEnsemble_L2</td>\n",
       "      <td>-0.089902</td>\n",
       "      <td>-0.115442</td>\n",
       "      <td>root_mean_squared_error</td>\n",
       "      <td>0.260735</td>\n",
       "      <td>0.536262</td>\n",
       "      <td>881.157318</td>\n",
       "      <td>0.002121</td>\n",
       "      <td>0.000250</td>\n",
       "      <td>0.007496</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>ExtraTrees_r126_BAG_L2</td>\n",
       "      <td>-0.090488</td>\n",
       "      <td>-0.116181</td>\n",
       "      <td>root_mean_squared_error</td>\n",
       "      <td>0.758714</td>\n",
       "      <td>1.789536</td>\n",
       "      <td>1051.944375</td>\n",
       "      <td>0.035200</td>\n",
       "      <td>0.062116</td>\n",
       "      <td>0.316746</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>CatBoost_r177_BAG_L1</td>\n",
       "      <td>-0.090712</td>\n",
       "      <td>-0.120960</td>\n",
       "      <td>root_mean_squared_error</td>\n",
       "      <td>0.053236</td>\n",
       "      <td>0.012926</td>\n",
       "      <td>40.408373</td>\n",
       "      <td>0.053236</td>\n",
       "      <td>0.012926</td>\n",
       "      <td>40.408373</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>CatBoost_BAG_L1</td>\n",
       "      <td>-0.091538</td>\n",
       "      <td>-0.120831</td>\n",
       "      <td>root_mean_squared_error</td>\n",
       "      <td>0.046778</td>\n",
       "      <td>0.016296</td>\n",
       "      <td>40.163213</td>\n",
       "      <td>0.046778</td>\n",
       "      <td>0.016296</td>\n",
       "      <td>40.163213</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>RandomForest_r195_BAG_L1</td>\n",
       "      <td>-0.094000</td>\n",
       "      <td>-0.122443</td>\n",
       "      <td>root_mean_squared_error</td>\n",
       "      <td>0.040111</td>\n",
       "      <td>0.064088</td>\n",
       "      <td>0.516460</td>\n",
       "      <td>0.040111</td>\n",
       "      <td>0.064088</td>\n",
       "      <td>0.516460</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>RandomForestMSE_BAG_L1</td>\n",
       "      <td>-0.094035</td>\n",
       "      <td>-0.121815</td>\n",
       "      <td>root_mean_squared_error</td>\n",
       "      <td>0.035843</td>\n",
       "      <td>0.079617</td>\n",
       "      <td>0.342596</td>\n",
       "      <td>0.035843</td>\n",
       "      <td>0.079617</td>\n",
       "      <td>0.342596</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>LightGBM_BAG_L1</td>\n",
       "      <td>-0.096020</td>\n",
       "      <td>-0.121668</td>\n",
       "      <td>root_mean_squared_error</td>\n",
       "      <td>0.112126</td>\n",
       "      <td>0.188170</td>\n",
       "      <td>418.748503</td>\n",
       "      <td>0.112126</td>\n",
       "      <td>0.188170</td>\n",
       "      <td>418.748503</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>RandomForest_r39_BAG_L1</td>\n",
       "      <td>-0.100072</td>\n",
       "      <td>-0.127539</td>\n",
       "      <td>root_mean_squared_error</td>\n",
       "      <td>0.039336</td>\n",
       "      <td>0.067166</td>\n",
       "      <td>0.355244</td>\n",
       "      <td>0.039336</td>\n",
       "      <td>0.067166</td>\n",
       "      <td>0.355244</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>XGBoost_BAG_L1</td>\n",
       "      <td>-0.100095</td>\n",
       "      <td>-0.121858</td>\n",
       "      <td>root_mean_squared_error</td>\n",
       "      <td>0.063866</td>\n",
       "      <td>0.251930</td>\n",
       "      <td>421.895509</td>\n",
       "      <td>0.063866</td>\n",
       "      <td>0.251930</td>\n",
       "      <td>421.895509</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>LightGBMXT_BAG_L1</td>\n",
       "      <td>-0.100906</td>\n",
       "      <td>-0.124049</td>\n",
       "      <td>root_mean_squared_error</td>\n",
       "      <td>0.221563</td>\n",
       "      <td>0.223715</td>\n",
       "      <td>722.328415</td>\n",
       "      <td>0.221563</td>\n",
       "      <td>0.223715</td>\n",
       "      <td>722.328415</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>NeuralNetFastAI_BAG_L1</td>\n",
       "      <td>-0.119438</td>\n",
       "      <td>-0.136376</td>\n",
       "      <td>root_mean_squared_error</td>\n",
       "      <td>0.338029</td>\n",
       "      <td>1.064759</td>\n",
       "      <td>170.101845</td>\n",
       "      <td>0.338029</td>\n",
       "      <td>1.064759</td>\n",
       "      <td>170.101845</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>ExtraTrees_r172_BAG_L1</td>\n",
       "      <td>-0.119911</td>\n",
       "      <td>-0.150942</td>\n",
       "      <td>root_mean_squared_error</td>\n",
       "      <td>0.035171</td>\n",
       "      <td>0.079833</td>\n",
       "      <td>0.284091</td>\n",
       "      <td>0.035171</td>\n",
       "      <td>0.079833</td>\n",
       "      <td>0.284091</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>ExtraTrees_r4_BAG_L2</td>\n",
       "      <td>-0.123332</td>\n",
       "      <td>-0.137615</td>\n",
       "      <td>root_mean_squared_error</td>\n",
       "      <td>0.783084</td>\n",
       "      <td>1.790941</td>\n",
       "      <td>1051.925283</td>\n",
       "      <td>0.059571</td>\n",
       "      <td>0.063521</td>\n",
       "      <td>0.297654</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>KNeighborsDist_BAG_L1</td>\n",
       "      <td>-0.125324</td>\n",
       "      <td>-0.139540</td>\n",
       "      <td>root_mean_squared_error</td>\n",
       "      <td>0.087065</td>\n",
       "      <td>0.062199</td>\n",
       "      <td>0.002275</td>\n",
       "      <td>0.087065</td>\n",
       "      <td>0.062199</td>\n",
       "      <td>0.002275</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>KNeighborsUnif_BAG_L1</td>\n",
       "      <td>-0.137421</td>\n",
       "      <td>-0.157382</td>\n",
       "      <td>root_mean_squared_error</td>\n",
       "      <td>0.059476</td>\n",
       "      <td>0.061766</td>\n",
       "      <td>0.002630</td>\n",
       "      <td>0.059476</td>\n",
       "      <td>0.061766</td>\n",
       "      <td>0.002630</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>RandomForest_r34_BAG_L2</td>\n",
       "      <td>-0.179957</td>\n",
       "      <td>-0.173105</td>\n",
       "      <td>root_mean_squared_error</td>\n",
       "      <td>0.757500</td>\n",
       "      <td>1.792570</td>\n",
       "      <td>1051.883035</td>\n",
       "      <td>0.033986</td>\n",
       "      <td>0.065149</td>\n",
       "      <td>0.255406</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       model  score_test  score_val              eval_metric  \\\n",
       "0    RandomForest_r16_BAG_L2   -0.079514  -0.129587  root_mean_squared_error   \n",
       "1     RandomForestMSE_BAG_L2   -0.079514  -0.129587  root_mean_squared_error   \n",
       "2   RandomForest_r195_BAG_L2   -0.079564  -0.130023  root_mean_squared_error   \n",
       "3   RandomForest_r127_BAG_L2   -0.079641  -0.125772  root_mean_squared_error   \n",
       "4    RandomForest_r39_BAG_L2   -0.079720  -0.126446  root_mean_squared_error   \n",
       "5    RandomForest_r15_BAG_L2   -0.080097  -0.124648  root_mean_squared_error   \n",
       "6   RandomForest_r166_BAG_L2   -0.084730  -0.116908  root_mean_squared_error   \n",
       "7     ExtraTrees_r172_BAG_L2   -0.085047  -0.119907  root_mean_squared_error   \n",
       "8       LightGBMLarge_BAG_L1   -0.085198  -0.129623  root_mean_squared_error   \n",
       "9      ExtraTrees_r42_BAG_L1   -0.086015  -0.124994  root_mean_squared_error   \n",
       "10      ExtraTreesMSE_BAG_L1   -0.086483  -0.126278  root_mean_squared_error   \n",
       "11    ExtraTrees_r178_BAG_L2   -0.086630  -0.117139  root_mean_squared_error   \n",
       "12       WeightedEnsemble_L3   -0.088990  -0.109838  root_mean_squared_error   \n",
       "13     ExtraTrees_r42_BAG_L2   -0.089069  -0.114994  root_mean_squared_error   \n",
       "14    ExtraTrees_r197_BAG_L2   -0.089155  -0.116087  root_mean_squared_error   \n",
       "15      ExtraTreesMSE_BAG_L2   -0.089155  -0.116087  root_mean_squared_error   \n",
       "16     ExtraTrees_r49_BAG_L2   -0.089734  -0.110499  root_mean_squared_error   \n",
       "17       WeightedEnsemble_L2   -0.089902  -0.115442  root_mean_squared_error   \n",
       "18    ExtraTrees_r126_BAG_L2   -0.090488  -0.116181  root_mean_squared_error   \n",
       "19      CatBoost_r177_BAG_L1   -0.090712  -0.120960  root_mean_squared_error   \n",
       "20           CatBoost_BAG_L1   -0.091538  -0.120831  root_mean_squared_error   \n",
       "21  RandomForest_r195_BAG_L1   -0.094000  -0.122443  root_mean_squared_error   \n",
       "22    RandomForestMSE_BAG_L1   -0.094035  -0.121815  root_mean_squared_error   \n",
       "23           LightGBM_BAG_L1   -0.096020  -0.121668  root_mean_squared_error   \n",
       "24   RandomForest_r39_BAG_L1   -0.100072  -0.127539  root_mean_squared_error   \n",
       "25            XGBoost_BAG_L1   -0.100095  -0.121858  root_mean_squared_error   \n",
       "26         LightGBMXT_BAG_L1   -0.100906  -0.124049  root_mean_squared_error   \n",
       "27    NeuralNetFastAI_BAG_L1   -0.119438  -0.136376  root_mean_squared_error   \n",
       "28    ExtraTrees_r172_BAG_L1   -0.119911  -0.150942  root_mean_squared_error   \n",
       "29      ExtraTrees_r4_BAG_L2   -0.123332  -0.137615  root_mean_squared_error   \n",
       "30     KNeighborsDist_BAG_L1   -0.125324  -0.139540  root_mean_squared_error   \n",
       "31     KNeighborsUnif_BAG_L1   -0.137421  -0.157382  root_mean_squared_error   \n",
       "32   RandomForest_r34_BAG_L2   -0.179957  -0.173105  root_mean_squared_error   \n",
       "\n",
       "    pred_time_test  pred_time_val     fit_time  pred_time_test_marginal  \\\n",
       "0         0.760181       1.795758  1052.100370                 0.036667   \n",
       "1         0.764208       1.791137  1051.965007                 0.040694   \n",
       "2         0.763965       1.792421  1052.067429                 0.040452   \n",
       "3         0.760028       1.828918  1052.020103                 0.036514   \n",
       "4         0.759047       1.814940  1052.034403                 0.035534   \n",
       "5         0.773968       1.789261  1051.964227                 0.050454   \n",
       "6         0.759956       1.825563  1052.025394                 0.036443   \n",
       "7         0.759344       1.801947  1051.895912                 0.035831   \n",
       "8         0.056473       0.213798  1450.102432                 0.056473   \n",
       "9         0.039805       0.064449     0.373688                 0.039805   \n",
       "10        0.036898       0.073830     0.295924                 0.036898   \n",
       "11        0.758070       1.789267  1051.962606                 0.034557   \n",
       "12        0.762380       1.820710  1051.975643                 0.001638   \n",
       "13        0.762940       1.796545  1051.922744                 0.039427   \n",
       "14        0.765759       1.791899  1052.048798                 0.042245   \n",
       "15        0.781576       1.818013  1051.959879                 0.058062   \n",
       "16        0.760742       1.820444  1051.967594                 0.037229   \n",
       "17        0.260735       0.536262   881.157318                 0.002121   \n",
       "18        0.758714       1.789536  1051.944375                 0.035200   \n",
       "19        0.053236       0.012926    40.408373                 0.053236   \n",
       "20        0.046778       0.016296    40.163213                 0.046778   \n",
       "21        0.040111       0.064088     0.516460                 0.040111   \n",
       "22        0.035843       0.079617     0.342596                 0.035843   \n",
       "23        0.112126       0.188170   418.748503                 0.112126   \n",
       "24        0.039336       0.067166     0.355244                 0.039336   \n",
       "25        0.063866       0.251930   421.895509                 0.063866   \n",
       "26        0.221563       0.223715   722.328415                 0.221563   \n",
       "27        0.338029       1.064759   170.101845                 0.338029   \n",
       "28        0.035171       0.079833     0.284091                 0.035171   \n",
       "29        0.783084       1.790941  1051.925283                 0.059571   \n",
       "30        0.087065       0.062199     0.002275                 0.087065   \n",
       "31        0.059476       0.061766     0.002630                 0.059476   \n",
       "32        0.757500       1.792570  1051.883035                 0.033986   \n",
       "\n",
       "    pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  \\\n",
       "0                 0.068338           0.472741            2       True   \n",
       "1                 0.063717           0.337378            2       True   \n",
       "2                 0.065000           0.439800            2       True   \n",
       "3                 0.101498           0.392473            2       True   \n",
       "4                 0.087519           0.406774            2       True   \n",
       "5                 0.061840           0.336598            2       True   \n",
       "6                 0.098142           0.397765            2       True   \n",
       "7                 0.074526           0.268283            2       True   \n",
       "8                 0.213798        1450.102432            1       True   \n",
       "9                 0.064449           0.373688            1       True   \n",
       "10                0.073830           0.295924            1       True   \n",
       "11                0.061846           0.334977            2       True   \n",
       "12                0.000266           0.008050            3       True   \n",
       "13                0.069124           0.295115            2       True   \n",
       "14                0.064478           0.421169            2       True   \n",
       "15                0.090592           0.332250            2       True   \n",
       "16                0.093023           0.339965            2       True   \n",
       "17                0.000250           0.007496            2       True   \n",
       "18                0.062116           0.316746            2       True   \n",
       "19                0.012926          40.408373            1       True   \n",
       "20                0.016296          40.163213            1       True   \n",
       "21                0.064088           0.516460            1       True   \n",
       "22                0.079617           0.342596            1       True   \n",
       "23                0.188170         418.748503            1       True   \n",
       "24                0.067166           0.355244            1       True   \n",
       "25                0.251930         421.895509            1       True   \n",
       "26                0.223715         722.328415            1       True   \n",
       "27                1.064759         170.101845            1       True   \n",
       "28                0.079833           0.284091            1       True   \n",
       "29                0.063521           0.297654            2       True   \n",
       "30                0.062199           0.002275            1       True   \n",
       "31                0.061766           0.002630            1       True   \n",
       "32                0.065149           0.255406            2       True   \n",
       "\n",
       "    fit_order  \n",
       "0          31  \n",
       "1          17  \n",
       "2          20  \n",
       "3          24  \n",
       "4          22  \n",
       "5          29  \n",
       "6          28  \n",
       "7          21  \n",
       "8          10  \n",
       "9          12  \n",
       "10          7  \n",
       "11         27  \n",
       "12         33  \n",
       "13         19  \n",
       "14         30  \n",
       "15         18  \n",
       "16         23  \n",
       "17         16  \n",
       "18         32  \n",
       "19         11  \n",
       "20          6  \n",
       "21         13  \n",
       "22          5  \n",
       "23          4  \n",
       "24         15  \n",
       "25          9  \n",
       "26          3  \n",
       "27          8  \n",
       "28         14  \n",
       "29         26  \n",
       "30          2  \n",
       "31          1  \n",
       "32         25  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing feature importance via permutation shuffling for 20 features using 32 rows with 5 shuffle sets...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📈 Feature Importance:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t118.56s\t= Expected runtime (23.71s per shuffle set)\n",
      "2025-09-30 14:41:05,575\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::_ray_fit()\u001b[39m (pid=828435, ip=172.17.0.5)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 413, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 201, in _fit\n",
      "    train_dataset = self._generate_dataset(X, y, train_params=processor_kwargs, is_train=True)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 669, in _generate_dataset\n",
      "    dataset = self._process_train_data(\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 734, in _process_train_data\n",
      "    self.processor = create_preprocessor(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/utils/data_preprocessor.py\", line 40, in create_preprocessor\n",
      "    return ColumnTransformer(\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "TypeError: ColumnTransformer.__init__() got an unexpected keyword argument 'force_int_remainder_cols'\n",
      "2025-09-30 14:41:05,575\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::_ray_fit()\u001b[39m (pid=828435, ip=172.17.0.5)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 413, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 201, in _fit\n",
      "    train_dataset = self._generate_dataset(X, y, train_params=processor_kwargs, is_train=True)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 669, in _generate_dataset\n",
      "    dataset = self._process_train_data(\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 734, in _process_train_data\n",
      "    self.processor = create_preprocessor(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bt708583/.local/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/utils/data_preprocessor.py\", line 40, in create_preprocessor\n",
      "    return ColumnTransformer(\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "TypeError: ColumnTransformer.__init__() got an unexpected keyword argument 'force_int_remainder_cols'\n",
      "\t21.11s\t= Actual runtime (Completed 5 of 5 shuffle sets)\n",
      "\t21.11s\t= Actual runtime (Completed 5 of 5 shuffle sets)\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "importance",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "stddev",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "p_value",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "n",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "p99_high",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "p99_low",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "03dd7a0c-354c-482b-bd3c-c7d45c9045ae",
       "rows": [
        [
         "Test temperature (℃)",
         "0.07407983255423868",
         "0.013361883773092295",
         "0.00012168695792955297",
         "5",
         "0.10159213592389565",
         "0.04656752918458171"
        ],
        [
         "Ti",
         "0.053065061554055534",
         "0.00951262919210431",
         "0.00011878619413363579",
         "5",
         "0.07265169653910608",
         "0.03347842656900499"
        ],
        [
         "solution treatment temperature",
         "0.03033542430649825",
         "0.00929905358812186",
         "0.0009388407382749988",
         "5",
         "0.04948230417102038",
         "0.01118854444197612"
        ],
        [
         "Stable aging temperature (℃)",
         "0.012894756753931997",
         "0.006683426184183162",
         "0.006252312738072509",
         "5",
         "0.02665602364193174",
         "-0.0008665101340677474"
        ],
        [
         "Nb",
         "0.009078403415727224",
         "0.003986279665432422",
         "0.0035097260560425264",
         "5",
         "0.017286208343092684",
         "0.0008705984883617644"
        ],
        [
         "solution treatment time",
         "0.0067313811538238285",
         "0.003627251208564815",
         "0.00713216198751499",
         "5",
         "0.014199941526198263",
         "-0.0007371792185506048"
        ],
        [
         "Mo",
         "0.00626834721322235",
         "0.0018655463938070192",
         "0.0008397856849004819",
         "5",
         "0.010109533022662317",
         "0.002427161403782384"
        ],
        [
         "Fe",
         "0.004467054494961625",
         "0.0030075855848594544",
         "0.014673523233047934",
         "5",
         "0.010659714783437624",
         "-0.0017256057935143747"
        ],
        [
         "Aging temperature (℃)",
         "0.00429770532724473",
         "0.0027089762824475012",
         "0.011926561422734648",
         "5",
         "0.009875524934835355",
         "-0.0012801142803458955"
        ],
        [
         "Ni",
         "0.0033521473489273534",
         "0.002048278990660919",
         "0.010795130023794779",
         "5",
         "0.0075695821009223885",
         "-0.0008652874030676814"
        ]
       ],
       "shape": {
        "columns": 6,
        "rows": 10
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>importance</th>\n",
       "      <th>stddev</th>\n",
       "      <th>p_value</th>\n",
       "      <th>n</th>\n",
       "      <th>p99_high</th>\n",
       "      <th>p99_low</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Test temperature (℃)</th>\n",
       "      <td>0.074080</td>\n",
       "      <td>0.013362</td>\n",
       "      <td>0.000122</td>\n",
       "      <td>5</td>\n",
       "      <td>0.101592</td>\n",
       "      <td>0.046568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ti</th>\n",
       "      <td>0.053065</td>\n",
       "      <td>0.009513</td>\n",
       "      <td>0.000119</td>\n",
       "      <td>5</td>\n",
       "      <td>0.072652</td>\n",
       "      <td>0.033478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>solution treatment temperature</th>\n",
       "      <td>0.030335</td>\n",
       "      <td>0.009299</td>\n",
       "      <td>0.000939</td>\n",
       "      <td>5</td>\n",
       "      <td>0.049482</td>\n",
       "      <td>0.011189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Stable aging temperature (℃)</th>\n",
       "      <td>0.012895</td>\n",
       "      <td>0.006683</td>\n",
       "      <td>0.006252</td>\n",
       "      <td>5</td>\n",
       "      <td>0.026656</td>\n",
       "      <td>-0.000867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Nb</th>\n",
       "      <td>0.009078</td>\n",
       "      <td>0.003986</td>\n",
       "      <td>0.003510</td>\n",
       "      <td>5</td>\n",
       "      <td>0.017286</td>\n",
       "      <td>0.000871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>solution treatment time</th>\n",
       "      <td>0.006731</td>\n",
       "      <td>0.003627</td>\n",
       "      <td>0.007132</td>\n",
       "      <td>5</td>\n",
       "      <td>0.014200</td>\n",
       "      <td>-0.000737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mo</th>\n",
       "      <td>0.006268</td>\n",
       "      <td>0.001866</td>\n",
       "      <td>0.000840</td>\n",
       "      <td>5</td>\n",
       "      <td>0.010110</td>\n",
       "      <td>0.002427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fe</th>\n",
       "      <td>0.004467</td>\n",
       "      <td>0.003008</td>\n",
       "      <td>0.014674</td>\n",
       "      <td>5</td>\n",
       "      <td>0.010660</td>\n",
       "      <td>-0.001726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Aging temperature (℃)</th>\n",
       "      <td>0.004298</td>\n",
       "      <td>0.002709</td>\n",
       "      <td>0.011927</td>\n",
       "      <td>5</td>\n",
       "      <td>0.009876</td>\n",
       "      <td>-0.001280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ni</th>\n",
       "      <td>0.003352</td>\n",
       "      <td>0.002048</td>\n",
       "      <td>0.010795</td>\n",
       "      <td>5</td>\n",
       "      <td>0.007570</td>\n",
       "      <td>-0.000865</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                importance    stddev   p_value  n  p99_high  \\\n",
       "Test temperature (℃)              0.074080  0.013362  0.000122  5  0.101592   \n",
       "Ti                                0.053065  0.009513  0.000119  5  0.072652   \n",
       "solution treatment temperature    0.030335  0.009299  0.000939  5  0.049482   \n",
       "Stable aging temperature (℃)      0.012895  0.006683  0.006252  5  0.026656   \n",
       "Nb                                0.009078  0.003986  0.003510  5  0.017286   \n",
       "solution treatment time           0.006731  0.003627  0.007132  5  0.014200   \n",
       "Mo                                0.006268  0.001866  0.000840  5  0.010110   \n",
       "Fe                                0.004467  0.003008  0.014674  5  0.010660   \n",
       "Aging temperature (℃)             0.004298  0.002709  0.011927  5  0.009876   \n",
       "Ni                                0.003352  0.002048  0.010795  5  0.007570   \n",
       "\n",
       "                                 p99_low  \n",
       "Test temperature (℃)            0.046568  \n",
       "Ti                              0.033478  \n",
       "solution treatment temperature  0.011189  \n",
       "Stable aging temperature (℃)   -0.000867  \n",
       "Nb                              0.000871  \n",
       "solution treatment time        -0.000737  \n",
       "Mo                              0.002427  \n",
       "Fe                             -0.001726  \n",
       "Aging temperature (℃)          -0.001280  \n",
       "Ni                             -0.000865  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ===== MODEL EVALUATION AUF TEST SET =====\n",
    "print(\"📊 Evaluiere AutoGluon Modelle auf dem GLEICHEN Test-Set wie das Neural Network...\")\n",
    "\n",
    "# Predictions auf Test Set\n",
    "test_predictions = predictor.predict(test_data)\n",
    "test_true = test_df[target_column].values\n",
    "\n",
    "# Berechne Metriken\n",
    "test_r2 = r2_score(test_true, test_predictions)\n",
    "test_mse = mean_squared_error(test_true, test_predictions)\n",
    "test_rmse = np.sqrt(test_mse)\n",
    "test_mae = mean_absolute_error(test_true, test_predictions)\n",
    "\n",
    "print(f\"\\n🎯 FINALE TEST-SET PERFORMANCE (identisch mit NN Test-Set):\")\n",
    "print(f\"  R² Score:  {test_r2:.6f}\")\n",
    "print(f\"  MSE:       {test_mse:.6f}\")\n",
    "print(f\"  RMSE:      {test_rmse:.6f}\")\n",
    "print(f\"  MAE:       {test_mae:.6f}\")\n",
    "\n",
    "# Model Leaderboard\n",
    "print(\"\\n🏆 Model Leaderboard:\")\n",
    "leaderboard = predictor.leaderboard(test_data, silent=True)\n",
    "display(leaderboard)\n",
    "\n",
    "# Feature Importance\n",
    "print(\"\\n📈 Feature Importance:\")\n",
    "feature_importance = predictor.feature_importance(test_data)\n",
    "display(feature_importance.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6453ae0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Erstelle Visualisierungen für Vergleich mit Neural Network...\n",
      "💾 Jeder Plot wird einzeln als PNG gespeichert...\n",
      "📊 Parity Plot gespeichert: /home/bt708583/ml_in_ms_wt24/AdvancedModule/AutoGluon_Models/automl_20250930_130252/individual_plots/01_parity_plot.png\n",
      "📊 Parity Plot gespeichert: /home/bt708583/ml_in_ms_wt24/AdvancedModule/AutoGluon_Models/automl_20250930_130252/individual_plots/01_parity_plot.png\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Residuals Plot gespeichert: /home/bt708583/ml_in_ms_wt24/AdvancedModule/AutoGluon_Models/automl_20250930_130252/individual_plots/02_residuals_plot.png\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA90AAAMVCAYAAABqdZdfAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuNSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/xnp5ZAAAACXBIWXMAAA9hAAAPYQGoP6dpAAC20ElEQVR4nOzdeXhU1f3H8c9NyEaSCQkEEgQEWVVAFASriAlBCQSRyqJWBAS1FcRdFH9qRcV9aaWopaBsWkVUqiwqRaIiCkVA3FhEKAgTA4QwIWQjc35/pBkz2ZhMMtnm/XqePDr3nnvme+fMXOY759xzLGOMEQAAAAAAqHEBdR0AAAAAAACNFUk3AAAAAAA+QtINAAAAAICPkHQDAAAAAOAjJN0AAAAAAPgISTcAAAAAAD5C0g0AAAAAgI+QdAMAAAAA4CMk3QAAAAAA+AhJNwDUEcuyyv0LDAxUVFSUevTooZtuukmbNm2q61BdEhIS3GLdu3dvlY5v37692/H1xfz5893ievjhh+s6pFq1d+/eCt+PlmUpPDxcnTp10rXXXqt///vfdR1upSZMmOAWe2pqqtv+kvvat29fJzH6Quk2TEhI8Oi4pUuXuh3XsWPHSss7nU61adPG7Zjly5dXK/bqXlcAoL4j6QaAesbpdMrhcOi7777TP/7xD/Xt21d/+ctf6jos+LETJ05o9+7deuONN3TppZfquuuuU2FhYV2HVW805KRx+PDhiomJcT3++eef9cUXX1RY/pNPPtGBAwdcj+Pi4pScnOzTGAGgoWtS1wEAAIoMGTJETZs2VUZGhjZu3Kjs7GxJkjFG06ZN0+9//3udfvrpdRrjJZdcohYtWrgeh4eH12E08KWRI0dKknJycrR582alpaW59i1evFgdOnTQI488Ulfhea34vCSpZcuWdRhJ/RAcHKxrrrlGs2fPdm1btGiRLrroonLLL1q0yO3xtddeqyZN+DoJAJXhKgkA9cRLL73kGu66f/9+9ezZU5mZmZKkgoICrV69WjfccEPdBShpxowZdfr8qD1Lly51/f+JEyd0xRVXuA0t/+tf/6qHHnqowSVcJc8LRa6//nq3pHvJkiV68cUXFRwc7FbuxIkTevfdd922TZgwoTZCBIAGjeHlAFAPtW3bVgMGDHDbdvjw4XLL5uTk6JVXXtHgwYMVFxen4OBgRUVFqU+fPpoxY4aOHDlS7nH79+/X3XffrXPPPVfNmjVTkyZNFB0drU6dOmnIkCF66KGHtGXLFrdjPBlGu379eg0dOlTR0dEKDw9X7969NWfOHBljKj1nT+6rrux+3CNHjujRRx/VyJEjdfbZZysuLk4hISFq2rSp2rVrp+HDh+v111+X0+msNI7yZGdn69lnn9WAAQPUsmVLBQcHKyIiQqeffrouvvhi3XHHHR7f12q32xUUFOQ6j759+5Zbbvr06W7n+89//tO1b8OGDRo/fry6du2q8PBwBQUFKTY2VmeddZbGjBmjZ555xq1nurqaNm2q+++/322bw+HQ9u3bXY9Lt01+fr6efvpp9ezZU+Hh4WXu4T958qTeeOMNDR8+XG3atFFoaKgiIyPVo0cP3XPPPfrll18qjGf//v2aNGmSWrdurdDQUHXq1EnTp0/X8ePHT3kuntzTnZGRoWeeeUYDBw5Uq1atFBwcrGbNmunMM8/UxIkTtXHjRkm/fR4+/fRTt+M7dOhQ6edk9+7dbp+94OBgxcXFadiwYVq6dGmln5UFCxaob9++Cg8PV3R0tAYPHqxPPvnklOddmd69e6tHjx6ux0ePHi33/fzee++5vca9e/dW9+7dJUnr1q3THXfcocTERHXs2FHR0dFq0qSJa36Km2++Wd98802VY0tNTXV7LctL8j2ZKyIzM1PPPPOMa7ROUFCQYmJi1L9/f73wwguukUWl/fjjj7r55pt19tlnKzIyUk2aNFHz5s3VtWtXjRgxQo899ph++umnKp8XAD9jAAB1QpLb3549e9z2X3755W77FyxYUKaOH374wXTp0qVMXSX/4uLizPr1692O27Fjh4mJian0OEnmrrvucjvukksuqTTmRYsWmcDAwHLruuqqq0ybNm3ctpX02muvue3785//XOlrdvrpp7vt+89//nPK85FkBg8ebPLz8z1+7tzcXNO7d+9T1tu7d+8y8VZkxIgRbsf++OOPbvudTqdp27ata3/z5s1Nbm6uMcaYt956ywQEBJwyng8++MDjePbs2VPm+NK+//77MmVKvq9Kbo+PjzdJSUkV1nnw4EHTt2/fSuOPjIw0//rXv8rEsW3bNtOiRYtyjznrrLPM0KFD3batXbvW7fjK3kPGGLNy5coK6y/9/ij9eajor+TnZPbs2SY4OLjS8kOGDDHZ2dllYrvpppvKLW9Zlrnrrrvctl1yySWVN3opzz77rNvxI0aMKFNm8ODBbmVmzZrl2jdlypRTvg6BgYFm3rx5Zeqt7Lqydu1at33jx48vc/zpp59e6Xv3888/N3FxcZXG1rlzZ7Njx44yx4WGhp7yvEq+DgBQnoY1JgwA/MR///tft96zsLCwMpMVHT16VJdddplbj2CnTp3UtWtX/frrr65Zz9PS0nT55Zdr27Ztat26tSTpueeeU0ZGhuu4bt26qXPnzsrOztaBAwe0Z88e5efnVynmn376STfeeKPbBFstW7ZUr169tGvXLr311ltVqs9bcXFxOv300xUdHa3g4GAdPnxYW7ZsUU5OjiTpo48+0uzZs3X77bd7VN+7776rr7/+2vW4VatWOu+88yTJ9VplZWVVKcabbrpJy5Ytcz1etGiRZs6c6Xqcmpqq/fv3ux6PHz9eISEhkqQHH3zQ1VsfEBCg888/X61atdKRI0d04MAB/fe//z3lqAJvbN68ucy2+Pj4csva7XbZ7XaFh4frvPPOU2hoqP7zn/9IKrpVYujQodq6daurfJs2bdSzZ08dO3ZMX375pZxOp7KysnTVVVfpq6++0jnnnCOpqHd8zJgxbqM+mjZtqn79+unYsWPavHmzfvjhB6/PcePGjfr973+vvLw817bQ0FD17NlTrVq10s8//6zvv//eta+41/TTTz91i6l4foZixXMfvP3225oyZYpre2BgoPr166fo6Ght3brVNUHZqlWrNHHiRL355puusq+//rrmzJnjFm/nzp3Vvn17ff3113ruuee8Pm9JGjt2rO677z6dPHlSkrRy5UplZGS4JllLS0tzu70gODhYf/jDH9zqCAgIUJcuXRQbG6vo6GgVFBRo7969+vHHHyVJhYWFmjJlioYMGVLhe6em7d69WykpKXI4HK5t3bt3V/v27bVnzx5Xe+7atUtDhgzRt99+62q7Rx99VLm5ua7jzj33XLVt21aZmZk6ePCg9uzZw4SCADxT11k/APgrldO7NXLkSDNw4EDTtGlTt96h+fPnlzn+gQcecDv+ySefdNv/xhtvuO2/5ZZbXPsuvfRS1/akpKQydR8/ftwsX77cfPTRR27bK+uRuuWWW9z2XXjhhcbhcBhjjDl58qQZO3Zspb2p1e3pzszMNDt37iz3tU5LSzPh4eGuY/v16+fxc8+cOdOt97V0D+TJkyfNF198YV577bVyn7s8hYWFbr1z7dq1M06n07X/+uuvd4tn+/btrn1BQUGu7Y888ki557pw4cIyveeVqayn+8SJE+bjjz82p512mtv+bt26udVR+vhevXqZX375xbW/uKd+7ty5buUmT55sCgsLXeW++OILY1mWa/+wYcNc+5YuXep2bPPmzd16J1955ZUycVSlp3vAgAFl3sP79u1zK/Pjjz+aNWvWuG071QgQY4ravF27dq4y0dHR5ocffnDtLygoMCkpKW71bNq0ybW/e/fubvumTp3qes8cPny4zP6q9nQbU3Z0zUsvveTa99xzz7ntGzlypNuxu3btMpmZmeXW+7e//c3t2Jdfftltvy97uktfd/75z3+67X/88cfd9j/77LOufZ07d3ZtnzhxYpnnPXr0qHn77bfNl19+We55A0AxeroBoJ5YtWpVmW2dOnXSkiVLdO6555bZ995777k9/vLLLzVq1CjX49I9MB988IFmzZolSW6zoP/nP//RI488oh49eqhTp07q1KmTwsPDlZKSUqX4V69e7fb4oYceUmRkpKSiHr0nn3xSixcvrlKdVREVFaVffvlFt956qz7//HPt3btXx48fd/XclVTyXuRTKflaZWVl6a677tLFF1+sTp06qXPnzoqOjtaFF16oCy+80OM6AwICNGnSJD300EOSpH379unTTz9VQkKCcnJy9M4777jKDhgwQF27dnWLp/ge0tdff102m01du3ZVp06d1KFDB7Vq1UrXXXedx7FUpLJ11AMCAk7Zszpr1iyddtpprsfFPfWl37e7du3SmDFj3LYFBwe7eptXr16tvLw8hYSElHmP3XjjjerSpYvr8U033aTnnntOu3btqjS28hw+fFiff/6567FlWVq8eLHatm3rVq5bt27q1q1blevfvHmz9u3b53rctGlTPfjgg25lDh486Pb4gw8+UO/evZWWlqbvvvvOtT0kJESPPfaYq42aN2+u++67T2PHjq1yXCVNmDBBH3zwgevxwoULdfPNN7v+v3TZks444wwtXbpUb731lrZu3aq0tDTl5OSUO+qiKp+/6nA6nXr//fddj4ODg7V06VK3yfRKj1L54IMPdNddd0kq+qwVv5c+/PBDPf300zrrrLPUsWNHdezYUc2aNXO75gJAheo66wcAf6VT3CdY/Ne3b1+TkZFR5viSveGe/AUEBJiTJ08aY4p665o1a1ZuucDAQHPuueeamTNnmqysLLfnrKxHqvS9jwcOHCgTc+nnLKm6Pd1vvfWWadKkicevh6fPnZOTY3r16lVhPR06dDBTpkwpt3ezMgcOHHC7//366683xhjzz3/+063+xYsXux33xhtvuPUEl/wLCwszAwcONAsXLnTrOT+V8nq6K/pr2bKleeedd8rUUbJMcHCwW+91SWeddVaV3reSzM8//2yMMSY5Odlt++uvv16m/tL3y3va071x48ZK31+V8aSne8mSJVU+73HjxhljjPnqq6/ctnfu3LlM/Vu3bnUr401Pd15enmnevLlbPbt27TLbtm1z2xYXF2cKCgpcxzmdzjKve2V/EyZM8Pj1q05Pd3p6epVf83bt2rmOT01NNSEhIeWWCw4ONr/73e/MrFmzTF5eXpVfawD+hdnLAaCe2LNnj3Jzc/XZZ5+5zaq8cePGGlmWx+l0uu5r7tatm7777jvdf//96t27t0JDQ13lCgsLtWXLFv3f//2fBg4cWGf3LJbuof71118rLJufn6+bb77Z7ZjY2FgNHjxYI0eO1MiRI93usa2K0NBQrV+/Xi+++KIGDhyoqKgot/179uzR7Nmzdd555+m///2vx/W2bt3abTTBO++8o5ycHLd1kGNiYsr0pF1zzTXauHGjbrzxRnXu3FkBAb/9U56Tk6NPPvlE48aNc/XWeav4dRs1apTGjRune++9V++++6727dunK6+8stJjW7Vq5RZXdVU0s3RjV9vnXd592osWLTrl2tzvvPOO2xwFktSjRw8NHz5cI0eOLLMSg6nGnAPljVxJT0/3ur7SSr7ml1xyibZt26bbbrtN3bt3V1BQkGtffn6+vvzyS02dOlVXX311jT0/gEaqrrN+APBXKtVzUrJ3Z/PmzWVmqC59f/XZZ5/t2mdZljl48KDXsRQWFpoDBw6Y1atXm4svvrjCnsLKeqRKz6JeOt4DBw6UOeeSXn/9dbd9f/rTn9z2v/322xX2RG7evNltX69evVz3EBtTdL9s6ZEBJXnSy17SkSNHzIYNG8rMJn2q40pbvny52/F/+ctf3Hrrb7/99lPWkZeXZ3bv3m3efvtt07p1a9exISEhJicnx6M4PJm9/FQqapvSSt+3/NVXX3n8HKVf7+nTp5cpU/p96GlPd3p6utsIAsuyXD3sp5KQkFDh56JY6dn1k5OTPT7vgwcPuh0bEhLimi+hWOnPjzc93caU/SydccYZZe7n//bbb92OufXWW932P/XUU277S88vUbq3urLryhdffFHp61beqgXFCgsLTWRkpGu7zWarVq90QUGB2bdvn/nggw/crr8VtTkAFKOnGwDqoXPPPbfMfbml7/8cPny46/+NMZoyZYrbDL3Ftm3bpgcffFCvvPKKa9t7772nd955x7XmbkBAgFq3bq1Bgwbp4osvdjve0/WeBw0a5Pb40UcfddVfWFio6dOnV3p88czqxZYvX+6amX3Xrl267777Kjy2oKDA7XFwcLCrV8rpdGr69Ok6ceKER+dR2tatW/X3v//d7X7bmJgY9e3bt0wvdFXXxh4yZIjbPcMlZ4+Wiu5RLu3FF19Uamqqq1xwcLDOOOMMXXnllerYsaOrXF5enjIzM6sUT20o+b6VpDvuuKPcnsqffvpJTz31lB555BHXttLvsX/84x9uayTPnTtXO3fu9Cqu2NhYXXTRRa7HxhiNHTvWbRZ5qWg27NLrYoeFhbk9Lp6FvKTzzjvP7R73jz/+uMx90pKUm5urlStXasyYMa73f3x8vM466yxXmby8PD300EOuHuOMjAw99dRTnp5qpc4991z17NnT9fjnn392O5+Sa3MXK/35KzmqJC0tTY899pjX8ZS+Lqxbt851f3taWpomT55c4bEBAQEaNmyY67HD4dCdd97pNju9VNTWGzZs0O233+4258D8+fO1cuVKV/kmTZqobdu2GjZsmGtG/WJV/ewD8DN1m/MDgP9SJT3dxhjz008/lblHueTay4cPHy6z9mxERIQZMGCAGT58uBkwYIDbesMle2Fvu+02132JPXv2NEOGDDFXXHGFOe+888rEtXXrVtdxlfVI7dy5s8z9j61atTKDBw82Z5xxRpl6S/8TlJ2dbWw2m9v+oKAg065du3LvYS7ZS5mdnW0iIiLc9nfs2NGkpKSYDh06uHouK3ruynq633vvPdfxnTp1MpdeeqkZMWKEGTBgQJnz/ctf/lLl98HDDz9c7mvTv3//csufc845rl67fv36mcsvv9wMGzbMdZ7Ffy1atHDdw38qtdnTnZeXV6aXMCQkxFx44YXmiiuuMImJiW499iV7RQsKCsr0ZIeHh5uBAwdWuJZ6VWYvX79+fZk1tENDQ12v8znnnGMsyyozouGOO+5wOyY2NtYMGzbMjBw50kybNs1VrnRvtCTTvn17k5ycbIYOHWp69erl9p4q+flauHBhmWO7dOliLrvssjL3YUve93QbY8zzzz9f7msplb8m9YIFC9zKBAQEmIsvvtgMGjTIRERElPnsVaWn2xhjOnXqVKb+du3auc2JUNF7d8eOHWWuDTExMSYxMdEMHz7cXHjhhSYqKsq1r+QqBFdccYWRZJo2bWp69+5tUlJSzPDhw8vMS9CkSRNz+PBhr19vAI0fSTcA1JHSXxTLG55Yeumo3r17u+3/9ttv3Za1qezv0UcfdR1XnHSf6u+Pf/yj2/Od6svx/PnzywyLL/5LTk428fHxlSZ2L7zwQoWxlB7CWjphevHFFys89pZbbql0WSFPku5T/Z133nnm+PHjlbR4+fbv319u8rBw4cJyyxcn3ZX9BQYGmkWLFnkcQ20m3cYUnXOfPn08el0nTZrkduzWrVtNTExMuWXbt29vEhMT3bZVJek2xpgPPvigwvrLe38Ux1TRJH6lP7MvvvhimcS+or/Sy5VNmjSpwrITJ050e1ydpPvXX38t93yCg4PNkSNHypTPz883/fr1KzeusLAw8+ijj7ptq2rS/c4771Q4eeDIkSPdfqQp772bmppa5gfKiv5Kfm6Kk+5T/T3xxBNev9YA/ANJNwDUkdJf3MpLunfv3l3my++yZcvcypw4ccL84x//MEOHDjWtW7c2ISEhJigoyLRq1cpcdNFF5q677jJr1qxxm036xx9/NE8//bT5/e9/b7p162ZatGhhmjRpYsLCwkyHDh3MyJEjyzyPMZ7N0vz555+b5ORkExUVZcLCwsw555xjXnjhBVNQUFBp4lts0aJFpnfv3iY0NNRERkaaSy65xLz33ntlXrPyEqalS5eaCy64wISFhZmIiAjTt29fV8+Vt0n3oUOHzCuvvGLGjx9vevbsaeLj401wcLAJCgoy8fHxZtCgQWbWrFke3z9dnmHDhrk9f3R0dIX1rV271jzwwANm0KBB5owzzjBRUVEmICDAREREmLPPPtvceOONZsuWLVV6/tpOuo0pWt/8zTffNL///e9Nu3btTGhoqAkKCjItWrQwffv2NVOmTDHvv/++yc/PL3Ps3r17zYQJE0yrVq1McHCwad++vbn99tvNkSNHzPjx46uVdBtT1OZPPvmkueSSS0yLFi1MUFCQiYqKMl27djUTJkwwGzZsKHPMv//9b5OUlGSaNWvmliCWTrqNKfpc33vvveb888830dHRJjAw0DRt2tR07NjRDB8+3Dz77LPl3k/udDrNq6++avr06WPCwsKMzWYzCQkJ5oMPPijThtVJuo0pP+EsvTZ3ScePHzfTpk0z7du3N0FBQSY2NtaMGjXKfPfdd6ecgdyT68rKlStN//79TdOmTU3Tpk3N+eefb+bNm2ecTqdH15XMzEzzwgsvmKSkJNOyZUsTFBRkQkJCzGmnnWYSExPN//3f/5WZX+A///mPefTRR83QoUNN586dTUxMjKutunTpYsaOHWtSU1Or/NoC8D+WMdWYQhIAAAAAAFSIidQAAAAAAPARkm4AAAAAAHyEpBsAAAAAAB8h6QYAAAAAwEdIugEAAAAA8BGSbgAAAAAAfISkGwAAAAAAHyHpBgAAAADAR0i6AQAAAADwEZJuAAAAAAB8hKQbAAAAAAAfIekGAAAAAMBHSLoBAAAAAPARkm4AAAAAAHyEpBsAAAAAAB8h6QYAAAAAwEdIugEAAAAA8BGSbgAAAAAAfISkGwAAAAAAHyHpBgAAAADAR0i6AQAAAADwEZJuAAAAAAB8hKQbAAAAAAAfIekGAAAAAMBHSLoBAAAAAPCRJnUdgL9yOp06ePCgIiMjZVlWXYcDAAAAAKgCY4yysrLUunVrBQRU3J9N0l1HDh48qLZt29Z1GAAAAACAati/f7/atGlT4X6S7joSGRkpqaiBbDZbHUdTPzidTh06dEixsbGV/lKExoV291+0vX+i3f0T7e6faHf/5E/t7nA41LZtW1duVxGS7jpSPKTcZrORdP+P0+lUbm6ubDZbo/+A4je0u/+i7f0T7e6faHf/RLv7J39s91PdLuwfrwIAAAAAAHWApBsAAAAAAB8h6QYAAAAAwEdIugEAAAAA8BEmUgMAAAD8VGFhoQoKCnxSt9PpVEFBgXJzc/1mQi00jnZv0qSJAgMDTzlBmsf11UgtAAAAABoMY4zS0tKUmZnp0+dwOp3KysqqseQF9V9jaffAwEC1bNlSUVFR1T4Pkm4AAADAzxQn3C1btlTTpk19khwZY3Ty5Ek1adKkQSdfqJqG3u7F8TscDtntduXk5Cg+Pr5adZJ0AwAAAH6ksLDQlXA3b97cZ8/T0JMveKextHtkZKRCQkJ0+PBhtWzZUoGBgV7X1TAH2QMAAADwSvE93E2bNq3jSID6LTw8XMaYas97QNINAAAA+KGG3AsJ1Iaa+oyQdAMAAAAA4CMk3QAAAAAA+AhJNwAAAAAAPkLSDQAAAABeeOaZZ3TGGWcoMDBQvXr1qutw6kz79u01YcKEU5abP3++LMvS3r17fRbL3r17ZVmW5s+f77PnqCqSbgAAAACNQnFSV/wXGhqqLl266JZbbtGvv/5ao8/18ccfa9q0abrooov02muv6fHHH6/R+iVp5cqVevjhhz0un5CQ4Hb+YWFh6tmzp/7yl7/I6XTWeHzwDOt0AwAAAGhUHnnkEXXo0EG5ublat26dXn75Za1cuVLfffddjS2V9sknnyggIEDz5s1TcHBwjdRZ2sqVKzV79uwqJd5t2rTRE088IUk6fPiw3njjDd1xxx06dOiQZs6c6ZM4d+zYoYAA+nMrQtINAAAAoFEZMmSI+vTpI0m64YYb1Lx5cz3//PP617/+pWuuuaZadZ84cUJNmzZVenq6wsLCfJZweysqKkpjx451Pf7Tn/6kbt26adasWXrkkUcUGBhY488ZEhJS43U2JvwcAQAAAOA3R496/5eXV3G9mZme11PDBg4cKEnas2ePa9vixYvVu3dvhYWFKSYmRldffbX279/vdlxCQoK6d++ur7/+WgMGDFDTpk11//33y7Isvfbaa8rOznYN5S55D7EndUvShg0bNHToUEVHRys8PFw9e/bUX//6V0nShAkTNHv2bElyGzJeVaGhoTr//POVlZWl9PR0t32exLlr1y6NHDlScXFxCg0NVZs2bXT11Vfr2LFjrjLl3dP9/fffa+DAgQoLC1ObNm302GOPlTvE3bKscnvyS9eZkZGhu+++Wz169FBERIRsNpuGDBmib7755pSvQVpamq6//nq1adNGISEhio+P1xVXXOHTe8tLoqcbAAAAwG8uvdT7Y6dNk8aMKX/fqFFFibcnNm3yPoZy7N69W5LUvHlzSdLMmTP14IMPasyYMbrhhht06NAhzZo1SwMGDNCWLVvUrFkz17FHjhzRkCFDdPXVV2vs2LFq1aqV+vTpozlz5mjjxo2aO3euJOnCCy+sUt2rV6/WsGHDFB8fr9tuu01xcXH68ccftXz5ct1222364x//qIMHD2r16tVatGhRtc6/eHKxkuflSZz5+fkaPHiw8vLyNHXqVMXFxenAgQNavny5MjMzFRUVVe7zpaWlaeDAgTp58qTuu+8+hYeHa86cOQoLC/P6HH7++WctW7ZMo0ePVocOHfTrr7/q73//uy655BL98MMPat26dYXHjhw5Ut9//72mTp2q9u3bKz09XatXr9a+ffvUvn17r2PyFEk3AAAAgEbl2LFjOnz4sHJzc/XFF1/okUceUVhYmIYNG6b//ve/+vOf/6zHHntM999/v+uYK6+8Uueee65eeuklt+1paWl65ZVX9Mc//tHtOf79739r8+bNbkO5Pa27sLBQf/zjHxUfH6+tW7e6JcPGGEnS7373O3Xp0kWrV692e45TKSws1OHDhyUV/WAwb948bdq0SSkpKa6k19M4f/jhB+3Zs0dvv/22Ro0a5Sr30EMPVRrDs88+q0OHDmnDhg3q27evJGn8+PHq3Lmzx+dRWo8ePbRz5063e8evu+46devWTfPmzdODDz5Y7nGZmZlav369nnnmGd19992u7dOnT/c6lqpieDkAAACARmXQoEGKjY1V27ZtdfXVVysiIkLvvfeeTjvtNL377rtyOp0aM2aMDh8+7PqLi4tT586dtXbtWre6QkJCdP3113v0vJ7WvWXLFu3Zs0e33367W8Ityash5CVt375dsbGxio2NVbdu3fTMM89o+PDhbsPfPY2zuCf7o48+0okTJzyOYdWqVbrgggtcCbckxcbG6tprr/X6vEJCQlwJd2FhoY4cOaKIiAh17dpVmzdvrvC44vvuU1NTddQHty54gp5uAAAaKWOM0tLS5HA4ZLPZFBcXV+0vcwDQEMyePVtdunRRkyZN1KpVK3Xt2tWVsO3atUvGmAp7XYOCgtwen3baaR5PluZp3cXD3bt37+5RvVXRvn17/eMf/5DT6dTu3bs1c+ZMHTp0SKGhoVWOs0OHDrrzzjv1/PPP6/XXX9fFF1+s4cOHa+zYsRUOLZekffv26YILLiizvWvXrl6fl9Pp1F//+le99NJL2rNnjwoLC137im8bKE9ISIieeuop3XXXXWrVqpUuuOACDRs2TOPGjVNcXJzX8VQFSTcAAI2MMUapaz7U2uULFB92WDHhTmVkB8ie00KJw8YrISmZ5BtAxVav9v7YypbjWrpU+t/QaV/r27eva/by0pxOpyzL0qpVq8qdyTsiIsLtcVXuQ65q3b4QHh6uQYMGuR5fdNFFOu+883T//ffrxRdfrHKczz33nCZMmKB//etf+vjjj3XrrbfqiSee0FdffaU2bdr47DxKJtWS9Pjjj+vBBx/UxIkT9eijjyomJkYBAQG6/fbbT7kG+e23367LL79cy5Yt00cffaQHH3xQTzzxhD755BOde+65PjuHYiTdAAA0IsYYzZ39lKIyV+mBlAgFB/32ZTG/IEvLvnpS83Zs06TJ00i8AZQvOto39ZYaRl1XOnbsKGOMOnTooC5dutRJ3R07dpQkfffdd24Jcmk1cZ3u2bOnxo4dq7///e+6++671a5duyq/Bj169FCPHj30wAMPaP369brooov0yiuv6LHHHiu3fLt27bRr164y23fs2FFmW3R0tDJLTbCXn58vu93utm3p0qVKTEzUvHnz3LZnZmaqRYsWpzyHjh076q677tJdd92lXbt2qVevXnruuee0ePHiUx5bXdzTDQBAI5K65kNFZa7SmP42BQe5/zMfHBSgMf1tsh1dqdQ1H9VRhABQt6688koFBgZqxowZrknLihljdOTIEZ/Xfd5556lDhw76y1/+UibhLHlceHi4JJUpU1XTpk1TQUGBnn/++SrF6XA4dPLkSbf9PXr0UEBAgPIqWR5uyJAh+uqrr7Rx40bXtkOHDun1118vU7Zjx4767LPP3LbNmTOnTE93YGBgmVjffvttHThwoMI4pKJ11XNzc8s8Z2RkZKXnUJPo6QYAoJEwxmjt8gV6IKXy4Ysj+kVo5oqFShyUXEuRAUD90bFjRz322GOaPn269u7dqxEjRigyMlJ79uzRe++9p5tuusltlmtf1B0QEKCXX35Zl19+uXr16qXrr79e8fHx2r59u77//nt99FHRD6O9e/eWJN16660aPHiwAgMDdfXVV1c5rrPOOktDhw7V3Llz9eCDD3oc5yeffKJbbrlFo0ePVpcuXXTy5EktWrRIgYGBGjlyZIXPd9ddd+n1119XcnKybrvtNteSYaeffrq2bdvmVvaGG27Qn/70J40cOVKXXnqpvvnmG3300Udleq+HDRumRx55RNdff70uvPBCffvtt3r99dd1xhlnVHruO3fuVFJSksaMGaOzzjpLTZo00Xvvvadff/3Vq9fSGyTdAAA0EmlpaYoPO+w2pLw8wUEBigtNl91uV3x8fC1FBwD1x3333acuXbrohRde0IwZMyRJbdu21WWXXabhw4fXSt2DBw/W2rVrNWPGDD333HNyOp3q2LGjbrzxRleZK6+8UlOnTtWbb76pxYsXyxjjdaJ4zz33aMWKFZo1a5Yefvhhj+I855xzNHjwYH3wwQc6cOCAmjZtqnPOOcc1O3lF4uPj9cknn+jWW2/Vk08+qebNm+tPf/qTWrdurUmTJrmVvfHGG7Vnzx7NmzdPH374oS6++GKtXr1aSUlJbuXuv/9+ZWdn64033tBbb72l8847TytWrNB9991X6Xm3bdtW11xzjdasWaNFixapSZMm6tatm5YsWVLpDwc1yTKl++hRKxwOh6KionTs2DHZbLa6DqdecDqdSk9PV8uWLd3W30PjRrv7L9q+5u3YsUNb375ZV/UPP2XZN9ed0HljXq7x+xlPhXb3T7R7/ZKbm6s9e/aoQ4cObjNa1zRjjE6ePKkmTZowh4QfaUztfqrPiqc5HVc9AAAaCZvNpoxsz/5pP5ptKTIy0scRAQAAkm4AABqJuLg42XNaKL+g8qVT8gucSsttydByAABqAUk3AACNhGVZShw2Xss2HK+03LIN2UpIGVdLUQEA4N9IugEAaEQSkpJ1rNkQLVnnKNPjnV/g1JJ1WXJED1FC0uA6ihAAAP/C7OUAADQilmXphin3KnVNL81csVBxoemKDjc6mm0pLbelElKmanTS4AY/uQ0AAA0FSTcAAI2MZVlKHJSsxEHJstvtysrKUmRkJPdwA3DDIkZA5WrqM0LSDQBAIxYfH0+yDcBNUFCQJOnEiRMKCwur42iA+is7O1uWZbk+M94i6QYAADXGGKO0tDQ5HA7ZbDbFxcUxlB2oZwIDA9WsWTOlp6dLkpo2beqTz2ljWq8Znmvo7V4cv8PhkMPhULNmzRQYGFitOkm6AQBAtRljlLrmQ61dvkDxYYcVE+5URnaA7DktlDhsvBKSkhvkly+gsYqLi5MkV+LtC8YYOZ1OBQQE8Pn3I42l3QMDAxUfH6+oqKhq10XSDQAAqsUYo7mzn1JU5io9kBKh4KDfhqvmF2Rp2VdPat6ObZo0eVodRgmgJMuyFB8fr5YtW6qgoMAnz+F0OnXkyBE1b95cAQEsmuQvGkO7N2nSRIGBgTX2owFJNwAAqJbUNR8qKnOVxvS3ldkXHBSgMf1tWrJupVLXnKNLBl5WBxECqEhgYGC1h85WxOl0KigoSKGhoQ02+ULV0e5l8SoAAACvGWO0dvkCjegXUWm5Ef0ilLpiYS1FBQBA/UHSDQAAvJaWlqb4sMMKDqr8K0VwUIDiQtOVlpZWS5EBAFA/kHQDAACvORwOxYQ7PSobHW6UlZXl44gAAKhfSLoBAIDXbDabMrI9+zpxNNtSZGSkjyMCAKB+IekGAABei4uLkz2nhfILKu/tzi9wKi23pWuZIgAA/AVJNwAA8JplWUocNl7LNhyvtNyyDdlKSBlXS1EBAFB/kHQDAIBqSUhK1rFmQ7RknaNMj3d+gVNL1mXJET1ECUmD6yhCAADqDut0AwCAarEsSzdMuVepa3pp5oqFigtNV3S40dFsS2m5LZWQMlWjkwbLsiwZY+o6XAAAahVJNwAAqDbLspQ4KFmJg5Jlt9uVlZWlyMhIxcfH13VoAADUKZJuAABQo+Lj40m2AQD4H+7pBgAAAADAR0i6AQAAAADwEZJuAAAAAAB8hKQbAAAAAAAfIekGAAAAAMBHSLoBAAAAAPARkm4AAAAAAHyEpBsAAAAAAB8h6QYAAAAAwEdIugEAAAAA8BGSbgAAAAAAfISkGwAAAAAAH2lUSXdeXp7uvfdetW7dWmFhYerXr59Wr159yuN27NihO+64QxdeeKFCQ0NlWZb27t1bYfn3339f5513nkJDQ9WuXTv9+c9/1smTJ2vwTAAAAAAAjUGjSronTJig559/Xtdee63++te/KjAwUEOHDtW6desqPe7LL7/Uiy++qKysLJ155pmVll21apVGjBihZs2aadasWRoxYoQee+wxTZ06tSZPBQAAAADQCDSp6wBqysaNG/Xmm2/qmWee0d133y1JGjdunLp3765p06Zp/fr1FR47fPhwZWZmKjIyUs8++6y2bt1aYdm7775bPXv21Mcff6wmTYpePpvNpscff1y33XabunXrVqPnBQAAAABouBpNT/fSpUsVGBiom266ybUtNDRUkyZN0pdffqn9+/dXeGxMTIwiIyNP+Rw//PCDfvjhB910002uhFuSJk+eLGOMli5dWr2TAAAAAAA0Ko2mp3vLli3q0qWLbDab2/a+fftKkrZu3aq2bdtW+zkkqU+fPm7bW7durTZt2rj2lycvL095eXmuxw6HQ5LkdDrldDqrFVdj4XQ6ZYzh9fAztLv/ou39E+3un2h3/0S7+yd/andPz7HRJN12u13x8fFlthdvO3jwYI08R8k6Sz9PZc/xxBNPaMaMGWW2Hzp0SLm5udWOrTFwOp06duyYjDEKCGg0gzBwCrS7/6Lt/RPt7p9od/9Eu/snf2r3rKwsj8o1mqQ7JydHISEhZbaHhoa69tfEc0iq8HmKe6/LM336dN15552uxw6HQ23btlVsbGyZ3nl/5XQ6ZVmWYmNjG/0HFL+h3f0Xbe+faHf/RLv7J9rdP/lTuxfnmqfSaJLusLAwt+HbxYp7kcPCwmrkOSRV+DyVPUdISEi5yXpAQECjfzNWhWVZvCZ+iHb3X7S9f6Ld/RPt7p9od//kL+3u6fk1mlchPj7eNfy7pOJtrVu3rpHnKFln6eepiecAAAAAADQejSbp7tWrl3bu3FlmiPeGDRtc+2viOSRp06ZNbtsPHjyoX375pUaeAwAAAADQeDSapHvUqFEqLCzUnDlzXNvy8vL02muvqV+/fq6Zy/ft26ft27d79Rxnn322unXrpjlz5qiwsNC1/eWXX5ZlWRo1alT1TgIAAAAA0Kg0mnu6+/Xrp9GjR2v69OlKT09Xp06dtGDBAu3du1fz5s1zlRs3bpw+/fRTGWNc244dO6ZZs2ZJkr744gtJ0t/+9jc1a9ZMzZo10y233OIq+8wzz2j48OG67LLLdPXVV+u7777T3/72N91www0688wza+lsAQAAAAANQaNJuiVp4cKFevDBB7Vo0SIdPXpUPXv21PLlyzVgwIBKjzt69KgefPBBt23PPfecJOn00093S7qHDRumd999VzNmzNDUqVMVGxur+++/Xw899FDNnxAAAAAAoEGzTMkuX9Qah8OhqKgoHTt2jCXD/sfpdCo9PV0tW7Zs9DMd4je0u/+i7f0T7e6faHf/RLv7J39qd09zusb9KgAAAAAAUIdIugEAAAAA8BGSbgAAAAAAfISkGwAAAAAAHyHpBgAAAADAR0i6AQAAAADwEZJuAAAAAAB8hKQbAAAAAAAfIekGAAAAAMBHSLoBAAAAAPARkm4AAAAAAHyEpBsAAAAAAB8h6QYAAAAAwEdIugEAAAAA8BGSbgAAAAAAfISkGwAAAAAAHyHpBgAAAADAR0i6AQAAAADwEZJuAAAAAAB8hKQbAAAAAAAfIekGAAAAAMBHSLoBAAAAAPARkm4AAAAAAHyEpBsAAAAAAB8h6QYAAAAAwEdIugEAAAAA8BGSbgAAAAAAfISkGwAAAAAAHyHpBgAAAADAR0i6AQAAAADwEZJuAAAAAAB8hKQbAAAAAAAfIekGAAAAAMBHSLoBAAAAAPARkm4AAAAAAHyEpBsAAAAAAB8h6QYAAAAAwEdIugEAAAAA8BGSbgAAAAAAfISkGwAAAAAAHyHpBgAAAADAR0i6AQAAAADwEZJuAAAAAAB8hKQbAAAAAAAfIekGAAAAAMBHSLoBAAAAAPARkm4AAAAAAHyEpBsAAAAAAB8h6QYAAAAAwEdIugEAAAAA8BGSbgAAAAAAfISkGwAAAAAAHyHpBgAAAADAR0i6AQAAAADwEZJuAAAAAAB8hKQbAAAAAAAfIekGAAAAAMBHSLoBAAAAAPARkm4AAAAAAHyEpBsAAAAAAB8h6QYAAAAAwEdIugEAAAAA8BGSbgAAAAAAfISkGwAAAAAAHyHpBgAAAADAR0i6AQAAAADwEZJuAAAAAAB8hKQbAAAAAAAfIekGAAAAAMBHSLoBAAAAAPARkm4AAAAAAHyEpBsAAAAAAB8h6QYAAAAAwEdIugEAAAAA8JFGlXTn5eXp3nvvVevWrRUWFqZ+/fpp9erVHh174MABjRkzRs2aNZPNZtMVV1yhn3/+uUw5y7LK/XvyySdr+nQAAAAAAA1ck7oOoCZNmDBBS5cu1e23367OnTtr/vz5Gjp0qNauXav+/ftXeNzx48eVmJioY8eO6f7771dQUJBeeOEFXXLJJdq6dauaN2/uVv7SSy/VuHHj3Lade+65PjknAAAAAEDD1WiS7o0bN+rNN9/UM888o7vvvluSNG7cOHXv3l3Tpk3T+vXrKzz2pZde0q5du7Rx40adf/75kqQhQ4aoe/fueu655/T444+7le/SpYvGjh3ru5MBAAAAADQKjWZ4+dKlSxUYGKibbrrJtS00NFSTJk3Sl19+qf3791d67Pnnn+9KuCWpW7duSkpK0pIlS8o9JicnR7m5uTV3AgAAAACARqfR9HRv2bJFXbp0kc1mc9vet29fSdLWrVvVtm3bMsc5nU5t27ZNEydOLLOvb9+++vjjj5WVlaXIyEjX9vnz5+ull16SMUZnnnmmHnjgAf3hD3+oNL68vDzl5eW5HjscDtfzO51Oz0+0EXM6nTLG8Hr4Gdrdf9H2/ol290+0u3+i3f2TP7W7p+fYaJJuu92u+Pj4MtuLtx08eLDc4zIyMpSXl3fKY7t27SpJuvDCCzVmzBh16NBBBw8e1OzZs3Xttdfq2LFjuvnmmyuM74knntCMGTPKbD906BA95v/jdDp17NgxGWMUENBoBmHgFGh3/0Xb+yfa3T/R7v6JdvdP/tTuWVlZHpVrNEl3Tk6OQkJCymwPDQ117a/oOEkeH/vFF1+4lZk4caJ69+6t+++/XxMmTFBYWFi5zzN9+nTdeeedrscOh0Nt27ZVbGxsmd55f+V0OmVZlmJjYxv9BxS/od39F23vn2h3/0S7+yfa3T/5U7sX54un0miS7rCwMLfh28WKe5ErSoaLt3tzrCQFBwfrlltu0Z/+9Cd9/fXXFc6SHhISUm5iHxAQ0OjfjFVhWRaviR+i3f0Xbe+faHf/RLv7J9rdP/lLu3t6fo3mVYiPj5fdbi+zvXhb69atyz0uJiZGISEhXh1brPhe8YyMjCrFDAAAAABo3BpN0t2rVy/t3LnTNUFZsQ0bNrj2lycgIEA9evTQpk2byuzbsGGDzjjjDLdJ1Mrz888/S5JiY2O9iBwAAAAA0Fg1mqR71KhRKiws1Jw5c1zb8vLy9Nprr6lfv36u3uh9+/Zp+/btZY79z3/+45Z479ixQ5988olGjx7t2nbo0KEyz5uVlaW//OUvatGihXr37l3TpwUAAAAAaMAazT3d/fr10+jRozV9+nSlp6erU6dOWrBggfbu3at58+a5yo0bN06ffvqpjDGubZMnT9Y//vEPpaSk6O6771ZQUJCef/55tWrVSnfddZer3OzZs7Vs2TJdfvnlateunex2u1599VXt27dPixYtUnBwcK2eMwAAAACgfms0SbckLVy4UA8++KAWLVqko0ePqmfPnlq+fLkGDBhQ6XGRkZFKTU3VHXfcoccee0xOp1MJCQl64YUX3IaMX3TRRVq/fr3mzp2rI0eOKDw8XH379tWrr76qgQMH+vr0AAAAAAANjGVKdvmi1jgcDkVFRenYsWMsGfY/TqdT6enpatmyZaOf6RC/od39F23vn2h3/0S7+yfa3T/5U7t7mtM17lcBAAAAAIA6RNINAAAAAICPkHQDAAAAAOAjJN0AAAAAAPgISTcAAAAAAD5C0g0AAAAAgI+QdAMAAAAA4CMk3QAAAAAA+AhJNwAAAAAAPkLSDQAAAACAj5B0AwAAAADgIyTdAAAAAAD4CEk3AAAAAAA+QtINAAAAAICPkHQDAAAAAOAjJN0AAAAAAPgISTcAAAAAAD5C0g0AAAAAgI+QdAMAAAAA4CMk3QAAAAAA+AhJNwAAAAAAPtLEm4MOHDigNWvW6PPPP9fu3bt16NAhSVJsbKzOOOMMXXzxxUpKSlKbNm1qNFgAAAAAABqSKiXdy5Yt09///nf9+9//ltPpLLfMp59+qtdee02WZWnQoEH605/+pBEjRtRErAAAAAAANCgeDS//+OOP1adPH40cOVIff/yxCgsLZYyp9M/pdGr16tUaOXKk+vTpo9WrV/v6XAAAAAAAqFc86ulOTk6WZVkyxkiSgoOD1aNHD51//vlq06aNmjdvLmOMMjIytH//fm3atEnffvut8vPzJUmbN2/WkCFDdPLkSd+dCWqMMUZpaWlyOByy2WyKi4uTZVl1HRYAAAAANDgeDy8PCAhQSkqKrr76al1++eUKDw+vtHx2drY++OADvfnmm1qxYkWFw9FRfxhjlLrmQ61dvkDxYYcVE+5URnaA7DktlDhsvBKSkkm+AQAAAKAKPEq6J0+erDvvvFNnnHGGxxWHh4fr6quv1tVXX63du3frhRde8DpI+J4xRnNnP6WozFV6ICVCwUFhrn35BVla9tWTmrdjmyZNnkbiDQAAAAAe8uie7r/97W9VSrhL69ixo/72t795fTx8L3XNh4rKXKUx/W0KDnJ/WwQHBWhMf5tsR1cqdc1HdRQhAAAAADQ8rNMNGWO0dvkCjegXUWm5Ef0ilLpiYS1FBQAAAAANn1frdBczxmjFihVav369Dh06pNGjR6tfv346duyYJKldu3Y1EiR8Ky0tTfFhh92GlJcnOChAcaHpstvtio+Pr6XoAAAAAKDh8jrp3rFjh0aOHKkff/zRte3MM8/UiRMndOWVVyogIEDr1q3TBRdcUCOBwnccDodiwj2b6C463CgrK4ukGwAAAAA84NXw8iNHjmjQoEGuhLt4KTFJuvzyyxUVFSVjjJYtW1YjQcK3bDabMrI9eysczbYUGRnp44gAAAAAoHHwKul+9tlndeDAgaIKAtyrCAwMVGJioowxWrduXfUjhM/FxcXJntNC+QWV93bnFziVltuSXm4AAAAA8JBXSff7778vSTr99NO1f//+MvvPOussSdLOnTurERpqi2VZShw2Xss2HK+03LIN2UpIGVdLUQEAAABAw+dV0r1nzx5ZlqVrr71WcXFxZfZHRBTNgp2ZmVmt4FB7EpKSdazZEC1Z5yjT451f4NSSdVlyRA9RQtLgOooQAOovY4zsdrt27Nghu93udtsVAADwb15NpFY8pDwwMLDc/cW932Fhlc+GjfrDsizdMOVepa7ppZkrFiouNF3R4UZHsy2l5bZUQspUjU4aLMuy6jpUAKg3jDFKXfOh1i5foPiww4oJdyojO0D2nBZKHDZeCUnJXDcBAPBzXiXd7dq10/bt2/Xee+/p/vvvd9tnt9v19ttvy7IsdejQoUaCRO2wLEuJg5KVOChZdrtdWVlZioyM5B5uACiHMUZzZz+lqMxVeiAlwm3ZxfyCLC376knN27FNkyZPI/EGAMCPeTW8fNCgQZKk7777Tuecc45r+/z589WzZ08dPnxYknTppZfWQIioC/Hx8erSpQsJNwBUIHXNh4rKXKUx/W0KDnL/5zQ4KEBj+ttkO7pSqWs+qqMIAQBAfeBV0n3HHXeoadOmkoomSyv+Bf/777/XkSNHJEnh4eGaOnVqDYUJAED9YYzR2uULNKJfRKXlRvSLUOqKhbUUFQAAqI+8Sro7dOig119/XaGhoTLGuCaMKf5vaGioFi9erHbt2tVcpAAA1BNpaWmKDztcpoe7tOCgAMWFpstut9dSZAAAoL7x6p5uSbriiiv0/fffa9asWfriiy+UkZGhmJgYXXjhhZo6dSr3cwMAGi2Hw6GYcOepC0qKDjfKysridh0AAPyU10m3JLVv317PPfdcTcUCAECDYLPZlJHt2WCxo9mWIiMjfRwRAACor6qVdAMA4I/i4uJkz2mh/IKsSoeY5xc4lZbbkl5uAPCAMUZpaWlyOByy2WyKi4tj9Qc0Cl4l3RMnTvSonGVZmjdvnjdPAQBAvWVZlhKHjdeyr57UmP62Csst25CthBQmFQWAyhhjlLrmQ61dvkDxYYcVE+5URnaA7DktlDhsvBKSkkm+0aB5lXTPnz//lG98YwxJNwCg0UpIStbc7d9oybpVGtEvwq3HO7/AqWUbsuWIHqLRSYPrMEoAqN+MMZo7+ylFZa7SAykRCg4Kc+3LL8jSsq+e1Lwd2zRp8jQSbzRY1RpeXjxbebHiD0Lp7QAANDaWZemGKfcqdU0vzVyxUHGh6YoONzqabSktt6USUqZqdNJgviQCQCVS13yoqMxV5Y4aCg4K0Jj+Ni1Zt1Kpa85R4qDkOogQqD6vku4BAwaU+RKRl5en3bt369ChQ7IsS127dlWrVq1qJEgAAOojy7KUOChZiYOSZbfblZWVpcjISO7hBgAPGGO0dvkCPZASUWm5Ef0iNHPFQpJuNFheJd2pqanlbjfGaM6cOZo8ebIKCgr07rvvVic2AAAajPj4eJJtAKiCtLQ0xYcddhtSXp7goADFhabLbrdznUWD5Nl6Jx6yLEt//OMfNXDgQP3888966KGHarJ6AAAAAI2Ew+FQTLjTo7LR4UZZWVk+jgjwjRpNuouFhYXJGENPNwAAAIBy2Ww2ZWR7lo4czbYUGRnp44gA3/BqePlnn31WZpsxRjk5Ofrqq6+0cuVKSVJGRkb1ogMAAADQKMXFxcme00L5BVluK0CUll/gVFpuS4aWo8HyKulOSEiodDbW4uXCOnbs6HVgAAAAABovy7KUOGy8ln31ZLmzlxdbtiFbCSlTazEyoGZVa3i5MabMX8l9d955Z7UDBAAAANA4JSQl61izIVqyzqH8Avf7u/MLnFqyLkuO6CFKSBpcRxEC1ef1Ot0VrcVtjFGXLl00bdo0TZw40evAAAAAADRulmXphin3KnVNL81csVBxoemKDjc6mm0pLbelElKmanTS4EpH2QL1nVdJ9549e8rdHhAQoGbNmjHJAQAAAACPWJalxEHJShyULLvdrqysLEVGRnIPNxqNKifdOTk5+uabbyRJUVFRuuSSS2o8KAAAAAD+Jz4+nmQbjU6Vk+6QkBD9/ve/lyRNnDiRpBsAAAAAgApUeSK1gIAAxcbGSpJOP/30Gg8IAAAAAIDGwqvZyy+99FIZY7Rt27aajgcAAAAAgEbDq6T78ccfV1xcnN555x397W9/k9PpPPVBAAAAAAD4Ga9mLx8/frxsNpvS0tJ022236eGHH1bHjh0VHh7uVs6yLK1Zs6ZGAgUAAAAAoKHxKulOTU2VZVmyLEvGGGVkZOjo0aNuZYwxrKcHACUYY5SWliaHwyGbzaa4uDiukwAAAI2cV0m3VPTlsbLHAIAixhilrvlQa5cvUHzYYcWEO5WRHSB7TgslDhuvhKRkkm8AAIBGyuOke+DAgbIsS3/60580fvx4X8YEAI2GMUZzZz+lqMxVeiAlQsFBYa59+QVZWvbVk5q3Y5smTZ5G4g0AANAIeZx0Fw8pT0lJ0WuvvebLmACg0Uhd86GiMldpTH9bmX3BQQEa09+mJetWKnXNOUoclFwHEQIAAMCXvJq9HABwasYYrV2+QCP6RVRabkS/CKWuWFhLUQEAAKA2kXQDgI+kpaUpPuywgoMqv9QGBwUoLjRddru9liIDAACo34wxstvt2rFjh+x2e4OeQ8zridQAAJVzOByKCXd6VDY63CgrK0vx8fE+jgoAAKD+aowT0FY56b7nnnt0zz33eFTWsiydPHmyykEBQGNgs9mUke3ZgKKj2ZYiIyN9HBEAAED91VgnoK3y8HJjTJX+AMBfxcXFyZ7TQvkFlfd25xc4lZbbkl5uAADg10pOQFv69rziCWhtR1cqdc1HdRShd7inGwB8xLIsJQ4br2UbjldabtmGbCWkjKulqAAAAOqfxjwBbZWHl/fp00dnn322L2IBgEYnISlZc7d/oyXrVmlEvwi3X23zC5xatiFbjughGp00uA6jBAAAqFu/TUAbVmm5khPQNpRRglVOuq+++mrdeeedvogFABody7J0w5R7lbqml2auWKi40HRFhxsdzbaUlttSCSlTNTppcIO6LwkAAKCmNeYJaJm9HAB8zLIsJQ5KVuKgZNntdmVlZSkyMrLB/EMBAADga415AtpGdU93Xl6e7r33XrVu3VphYWHq16+fVq9e7dGxBw4c0JgxY9SsWTPZbDZdccUV+vnnn8stO2/ePJ155pkKDQ1V586dNWvWrJo8DQCNWHx8vLp06ULCDQAAUEJjnoC2USXdEyZM0PPPP69rr71Wf/3rXxUYGKihQ4dq3bp1lR53/PhxJSYm6tNPP9X999+vGTNmaMuWLbrkkkt05MgRt7J///vfdcMNN+jss8/WrFmz9Lvf/U633nqrnnrqKV+eGgAAAAA0Wo15AlrLeLiu14IFCyRJ559/vs466yyfBuWNjRs3ql+/fnrmmWd09913S5Jyc3PVvXt3tWzZUuvXr6/w2Kefflr33nuvNm7cqPPPP1+StH37dnXv3l3Tpk3T448/LknKyclR27ZtdcEFF2j58uWu48eOHatly5Zp//79io6O9iheh8OhqKgoHdu7Vzabreon3LSpFBJS/r7MTMnb5dpCQ6WwCiYvcDikwkLv6g0JKYq5PFlZ0smTcjqdOnTokGJjYxUQ4OHvQUFBUkQFMxwePy4VFHgXb5MmUkVDVk6ckPLyvKs3MFCqqL1zcqTcXO/qtSypWbPy9+XlFcXsrYre0/n5Una29/VGRUkBAXI6nUpPT1fLli2L2r2goKjtvGWzFb3OpRUWFr2HvRURUfR+K83plI4d877e8HApOLj8fUePel9vA7hGuH3mw8JOeY3wCteIIvXoGuHxtf5/14gyuEb8pgFdI5xOpw7t3q3YmBjP/40vyYPvEV7hGlHER9cIp9Op9IKC3/6NL6mGvkeUUVAgk5WlX3/91XU7V6tWrTyfO4VrRJFqXCMqvc6f4nuEOXlSC+f+VbZjq5XSO0LBQb+1W36B0Yqvs5XVbJCum3Sbe5vW0TXCceSIotq317FjxyrN6TxOuuu7adOm6fnnn1dGRobbCT/xxBO6//77tW/fPrVt27bcY/v27SupKHEvafDgwdq9e7d++uknSdLKlSuVkpKiFStWaOjQoa5yX375pS688EItWrRIY8eO9SheV9Ldq5ds5X2wT2XaNGnMmPL3DRpU9GHwxk03Ff2VZ8wYqYIh96c0erR0770VP+fmzTKSThYUqElQkDyeUiopSapolMG990pr1ngRrKTzzpPmzCl/31NPSW+/7V29Z5whLVlS/r45cyp+zlNp1kz697/L37dkifT0097VK0mbNpW//d//lu67z/t6V6+WoqPLJt1ffy398Y/e17tkSdHrXNrPP1f8mfHE3/8u9e5ddvvRo9Kll3pf75NPFn1my9Onj/f1NoBrhNtn3oNrhFe4RhSpR9cIj6/1/7tGlME14jcN6BrhdDqVN2KEQg8e9Pzf+JK4RhRpYNcII+nX5cvLT7pr6HuE2/MZo00vz1LrGQ8rKOCkmgRIJ51SgbOJIqOaK9IWder3H9eIItW4RlR6nffge4SRlOVwKOvYEQUFFJRox6D/taOtbL11dI1wfPyxorZuPWXS3WgmUtuyZYu6dOlS5mSLE+qtW7eWm3Q7nU5t27ZNEydOLLOvb9+++vjjj12/km3ZskVS0bJpJfXu3VsBAQHasmVLhUl3Xl6e8kr8qun43y9l5n9/VWWczqJfxspRnTmQK623Gr/PGGNOXa8xRa+FMTKe/hppTFHMldXrjUrqlTHev8aV1et0Vr/tGlq9TqecTqeMMXIWP08N1VtGQ6tXPvws+6reqn7mSnzmnZ5cI7zBNeK3qutLvR5e6xvNZ5lrhCT9do2vyr/xJevlGnHqeuvjNcIY93/ja6pelX2vGWP06svPqP33b6t3lFOWFVBin1OZ2Wk6nJujFi3jqlSvr+L1eb2qw2tEJdd5T79HRNpsirTZVFBQIGdhoQICAxVUYqRA6U9nXV0jPK250STdFa3TVrzt4MGD5R6XkZGhvLy8Ux7btWtX2e12BQYGqmXLlm7lgoOD1bx58wqfQyrqcZ8xY0aZ7ScLCnSyootcJbIdDuWmp5e7Lzo/XwFeDoc6kZWlnArqjcrLUxMv6809flzZFdRry8lRUEGBjKTC/w1N9fQikXfihI5XUG/EiRMK8TLegpwcOSqoN/z4cYV6We/JvDwdq6DesKwsNfWyXmd+vo5WUG+ow6Fwb4fHSTpSQb3BmZmKrEa9GYcOyRQUyOl06tixYzLGKCAgQE0yMhRVjXozjxxRYTnDgAKPHFGzatR7LCNDJ8t5LazMTMVUo96szEzlV/AaN69GvQ3hGlHyM5/nwTXCG1wjitSna4Sn1/ria0RpXCN+05CuEU6nUxH5+Qo8edKrRMCT7xHe4BpRxFfXCCMpMzPT9W98STX1PaLYtq1fK7pwh87tfoYK1+wvk5VFhktZublyOI6paVgFw5DFNaJYda4RlV3nq/o9wpIU+L9RwScrec66ukZUFlNJjSbpzsnJUUg59x2Ehoa69ld0nCSPjs3JyVFwBfdMhIaGVvgckjR9+nS39c0dDofatm2rJkFBauLF8HKbzSZbqeS/mBUcXP49Ix6IjIxUZEX1hoR4XW94RITCK6o3LKyo3v/9ChXUpEnRvUUeaNK0qZpWVG/Tpl7H2yQsTKEV1KuICFne1hsSopCK6o2M9LpeBQeX+THIxWbzvl6p4nqbNatWvbGxsa7h5ZZl/XbfT0xMtept3ry5VF7Mx49Xq96YmJjy6w0Kqla9zZo1K79eqVr1NohrRInPfBNPrhFe4BrxP/XpGuHhtb74GlEG1wiXhnSNcDqdKggOrtK/8SV59D3CC1wj/sdX1whj1KxZs/Lv7a2h7xFFT2O07qPFun9IloL3OGRZ+eUe0yxUsh/LkM0WVWG9XCOKVOsaUcl1vk5zDS+c6hpR6GG9jeae7u7du6tVq1ZaU+remx9++EFnn322XnnlFf2xnHvADh8+rNjYWD3yyCN68MEH3fa99NJLmjJlirZv366uXbvqlltu0SuvvKKT5dyI37JlSyUlJemf//ynR/EykVoJTKRWpB5OgCKJidSK+dEEKJViIrUiXCN+w0RqRbhGFGEitSJcIyTV3kRqdrtdy168TjcPDiu6+Ten4vfDvE9ylXLTK4qLq2CYOdeIInU0kZovcw2v1NBEao2mpzs+Pl4HDhwos91ut0uSWrduXe5xMTExCgkJcZWr7Nj4+HgVFha6EoRi+fn5OnLkSIXPUano6Iovnt6q6KJZXTUdZ7Hif5SczqJhQtHR5X/RqqqKPiDV1bRpxR/q6ggLq/giVB0hIRVfNKsjOLjii3x1BAVV/CW+OgIDfVNvQIBv6pV8V299uUZ4+pmv6ItrdXGNKFLb14jqXuu5RvymgV0jTGRkzf0bXxLXiCL18RrhdEoVDM2tye8RDodDMeH/u12zSYAUWXG94c1PyhEYqLiqfn64RvzmVNcIb6/zvs41alpERIX3kZfWaNbp7tWrl3bu3OmaoKzYhg0bXPvLExAQoB49emhTObOvbtiwQWeccYYi/9dQxXWULrtp0yY5nc4KnwMAKmKMkd1u144dO2S329VIBh8BAFBrbDabMrI9S2uOZluu7/ZAbamRpHvJkiUaOHCgkpKSaqI6r4waNUqFhYWaU2KphLy8PL322mvq16+fa+byffv2afv27WWO/c9//uOWTO/YsUOffPKJRo8e7do2cOBAxcTE6OWXX3Y7/uWXX1bTpk2VkpLii1MD0AgZY7T236v05zuu0bIXr9PWt2/Wshev05/vuEZr/72K5BsAAA/FxcXJntNC+QWV9zrmFziVltuy3AmUAV+qkeHl+/fvV2pqqueLzvtAv379NHr0aE2fPl3p6enq1KmTFixYoL1792revHmucuPGjdOnn37q9oV28uTJ+sc//qGUlBTdfffdCgoK0vPPP69WrVrprrvucpULCwvTo48+qilTpmj06NEaPHiwPv/8cy1evFgzZ84smiABAE7BGKO5s59SVOYqPZASoeCg34YD5hdkadlXT2rejm2aNHlanV5XAQBoCCzLUuKw8Vr21ZMa07/iIcrLNmQrIWVqLUYGFGk0w8slaeHChbr99tu1aNEi3XrrrSooKNDy5cs1YMCASo+LjIxUamqqBgwYoMcee0wPPvigzjnnHH366adFMyOWMHnyZM2ZM0fffvutpkyZoi+++EIvvPCCpk+f7stTA9CIpK75UFGZqzSmv03BQaWWUAkK0Jj+NtmOrlTqmo/qKEIAABqWhKRkHWs2REvWOcr0eOcXOLVkXZYc0UOUkDS4jiKEP6uR2cufe+453XPPPbIsy7UmGyrnmr38FDPd+ZMys1jDL/hbuxtj9Oc7rtEDKVllEu6S8gucmrkySjNeeKMWo6td/tb2KEK7+6f60O7GGKWlpcnhcMhmsykuLo7RRD5W2+1ujFHqmo+UumKh4kLTFR1udDTbUlpuSyWkjFNC0mDavBbUh897bfE0p2s0s5cDQEOQlpam+LDDbkPKyxMcFKC40HTZ7XbuPQOAaihKxD7U2uULFB92WDHhTmVkB8ie00KJw8YrISmZRKyRsCxLiYOSlTgoWXa7XVlZWYqMjOTfUdQ5km4AqEVuy5qcQnS4UVZWFl8WAMBLzKHhv+Lj4/n3E/VGjfT3X3XVVVq7dq0++eSTmqgOABotljUBgNrDHBoA6oMaSbrbtGmjSy65RJdccklNVAcAjVZDX9aEdcUBNBTGGK1dvkAj+kVUWm5EvwilrlhYS1EB8EcMLweAWtRQlzXhnkgADQ1zaACoL0i6AaCWJSQla+72b7Rk3SqN6BfhNuQxv8CpZRuy5YgeotH1ZFkT7okE0BAxhwaA+oKkGwBqmWVZumHKvUpd00szy13WZKpG16NlTUreE1la8T2RS9atVOqac5Q4KLkOIgSAsphDA0B9QdINAHWgoSxrUnxP5AMpp74ncuaKhSTdAOqN3+bQyCoziVpJ9XUODQCNR+NerRwAGoD4+Hh16dKlXn7h++2eyMr/uSh5TyQA1AeuOTQ2HK+0XNEcGuNqKSoA/oikGwBQIW/uiQSA+iIhKVnHmg3RknWOMqtG5Bc4tWRdlhzRQ5RQT+bQANA4MbwcAFChxnRPpDFGaWlpcjgcstlsiouLqzf3zQPwjYY2hwaAxsmjpHvhQu/XLhw3juE6ANBQNYZ7IlnuDPBvDWUODQCNl0dJ94QJE7z+QkLSDQANV0NdV7wYy50BKCk+Pp5kG0Ct8/qebmOM68+T7QCAhqkh3xNZcrmz0j31xcud2Y6uVOqaj+ooQs8ZY2S327Vjxw7Z7Xb+nQUAoIHw+J7u8v5xtyyr3AS7eDsAoOFrqPdENpblzhgeDwBAw+ZR0u10uvdsnDx5Ur///e+1atUqPfbYYxo7dqxatWqlX3/9VYsWLdJDDz2khIQErVq1yidBAwBqV0O8J/K35c7CKi1Xcrmz+nY+DI8HAKDh82p4+dNPP62VK1dq3Lhxmj59utq2bavg4GC1bdtW999/v8aOHavU1FTNnDmzpuMFANSx+ryueEmNYbmzxjQ8HgAAf+VV0v3qq69Kkk477bRy97dt21bGGC1evNj7yAAAqIaGvtxZ8fD4Ef1OPTw+dYX3q4wAaPiY8wGo37xap/uXX36RJC1ZskR33323oqKiXPsyMzP11ltvSZIOHDhQAyECAFB1DX25s8YwPB6AbzHnA9AweJV0t2vXTrt379ZPP/2kDh06KDk5WS1btlR6ero+/PBDHTt2TFJRjzcAAHWhoS935s3weJJuwH8w5wPQcHg1vHzixImuYSvFPduzZs3SW2+9pczMTBljZFmWJk2aVKPBAgBQFQ15ubOGPjwegG8x5wPQcHjV033PPffom2++cQ0jL8+oUaN0zz33eB0YAADV1VCXO5Ma/vB4AL7TWJZEBPyFV0l3YGCg/vnPf2rkyJGaN2+eNm3apMzMTDVr1kx9+vTRpEmTNGrUqJqOFQCAKmuIy51JDX94PADfYc4HoGHxKukuNmrUKJJrAECDER8f36C+eCYkJWvu9m+0ZN0qjegX4dbjnV/g1LIN2XJED9Hoejg8HoDvMOcD0LBUK+kulpubqyNHjigqKkoREZUPcwEAAJ5pyMPjAfgOcz4ADUu1ku4333xTzz77rLZu3SpjjJ555hl1795db775pizL0nPPPadmzZrVUKgAAPifhjo8HoDvMOcD0LB4NXu5VDSZ2rXXXqstW7bI6fxteEvXrl01f/58zZ8/X++8806NBAkAAIqGx3fp0oUv0ICfc835sOF4peWK5nwYV0tRAaiIV0n3qlWr9Nxzz0mSa+mwYqeffrrOPfdcSdLHH39czfAAAAAAlNaQl0QE/I1Xw8tnz54tqehXtptvvlkvvfSS2/4LLrhAW7Zs0ZYtW6ofIQAAAAA3zPkANBxeJd0bN26UZVkaPXq0/va3v5VJuk877TRJ0sGDB6sfIQAAAIAymPMBaBi8SrqPHTsmSerRo0e5+3NzcyVJBQUFXoYFAAAAwFMNbUlEwJ94dU938YzkP/30U7n7169fL0lq3ry5d1EBAAAAANAIeJV09+rVS8YY/fOf/9SCBQtc2w8ePKjp06frk08+kWVZ6t27d40FCgAAAABAQ+PV8PKxY8dq9erVys/P18SJEyUVzWL+wgsvlCkHAAAAAIC/8qqne+zYsUpKSnItF2ZZVpmZEQcNGqSrrrqq+hECAADUA8YY2e127dixQ3a7vcyyqQAAlMernm7LsvTBBx/o9ttv16uvvqqTJ0+69gUGBmrixIn6y1/+UlMxAgAA1BljjFLXfKi1yxcoPuywYsKdysgOkD2nhRKHjVdCUjLLMgEAKuRV0i1JoaGheuWVV/TEE09ow4YNysjIUExMjPr166fo6OiajBEAAKBOGGM0d/ZTispcpQdSIhQcFObal1+QpWVfPal5O7Zp0uRpJN4AgHJ5lXR/9tlnkqSOHTvqtNNOU3Jyco0GBQAAUB+krvlQUZmrNKa/rcy+4KAAjelv05J1K5W65hwlDuL7EACgLK/u6U5ISFBiYqLeeuutcvfPmjVLNptNUVFR1QoOAACgrhhjtHb5Ao3oF1FpuRH9IpS6YmEtRQUAaGi8SrpPJT8/X8ePH9fx48d9UT0AAIDPpaWlKT7ssIKDKv+6FBwUoLjQdNnt9lqKDADQkPgk6d6/f78vqgUAAKg1DodDMeFOj8pGhxtlZWX5OCIAQEPk8T3dAwcOLLPt5Zdf1vLly922nThxQl9//bWkosnWAAAAGiKbzaaMbM/6J45mW4qMjPRxRACAhsjjpDs1NdVtVk5jjH7++Wf9/PPPZcoaY2RZls4666yaiRIAAKCWxcXFyZ7TQvkFWZUOMc8vcCott6Xi4+NrMToAQENRpeHlxhgZY8o8Lv0nFa3lfe+999ZstAAAv2OMkd1u144dO2S3293+HQJ8ybIsJQ4br2UbKp+jZtmGbCWkjKulqAAADY3HPd3jxo1z9XQvWLBAlmWpd+/eOvvss93KBQUF6bTTTtOIESN0zjnn1Gy0AAC/YYxR6poPtXb5AsWHHVZMuFMZ2QGy57RQ4rDxSkhKZl1kuDHGKC0tTQ6HQzabTXFxcdV+jyQkJWvu9m+0ZN0qjegX4dbjnV/g1LIN2XJED9HopMHVDR8A0Eh5nHTPnz/f9f8LFiyQJF199dW68847azwoAIB/M8Zo7uynFJW5Sg+kRCg4KMy1L78gS8u+elLzdmzTpMnTSLwbgJLJcGRkZI23mS9/oLEsSzdMuVepa3pp5oqFigtNV3S40dFsS2m5LZWQMlWjkwbzPgQAVMjjpLuk1157TZJ0/vnn12gwAABIUuqaDxWVuUpj+tvK7AsOCtCY/jYtWbdSqWvOUeKg5DqIEJ4oPxkO1ImmPXTuBYOUOGhItZPV2viBxrIsJQ5KVuKgZNntdmVlZSkyMpJ7uAEAHvEq6b722mt14sQJSVJhYaECAwNd+woLC5WdnS1Jatq0qZo08eopAAB+yhijtcsX6IGUiErLjegXoZkrFpJ011MVJcNOY+lAbo6+3PC05u38ttqjFWr7B5r4+HiSbQBAlXi1Tve9996r6OhoxcXFKS0tzW1fenq64uLiFB0dzURqAIAqS0tLU3zY4Upni5aKEqq40HTZ7fZaigxVUTIZLt2WQU0CNOoim2xHVyp1zUdeP0fxDzQj+p36B5rUFQu9fh4AAKrDq6R77dq1MsYoJSVFp512mtu++Ph4jRgxQsYYrVmzpkaCBAD4D4fDoZhwp0dlo8ONsrKyfBwRqqq2kmF+oAEANAReJd3//e9/ZVlWmZnLi3Xt2tVVDgCAqrDZbMrI9uyfp6PZliIjI30cEaqqtpJhfqABADQEXiXdxfdsHzt2rNz9mZmZkqScnBzvogIA+K24uDjZc1oov6DyZCq/wKm03JbcX1sP1VYyzA80AICGwKuku0WLFjLG6L333nNNqFYsJydHy5YtkyQ1b9682gECAPyLZVlKHDZeyzYcr7Tcsg3ZSkgZV0tRoSpqKxnmBxoAQEPgVdLdu3dvSdL+/fs1YMAALV26VF9//bWWLl2qAQMGuIaf9+nTp0aDBQD4h4SkZB1rNkRL1jnKJFT5BU4tWZclR/QQJSQNrqMIUZnaSob5gQYA0BB4tZ7Xddddpw8++ECStHnzZl111VXllhs3jn/gAABVZ1mWbphyr1LX9NLMFQsVF5qu6HCjo9mW0nJbKiFlqkYnDa72Gs/wDVcy/NWT5S7lVawoGZ5aredKSErW3O3faMm6VRrRL8LtPvK8/ELNX3NUh0IuUv+zesoYw3sGAFDrLGOM8ebAYcOGaeXKlbIsSyWrKH6ckpLiSsxRlsPhUFRUlI4dOyabreIvJP7E6XQqPT1dLVu2VECAV4Mw0ADR7v6rKm1vt9uVlZWlyMhIhgg3ECXX6S6ZDBet032avvx6uxzRydVep7v4uVLXfKTU//1A06ypU5u2Z+jnX46qT7cYdWwbraPZAbLntFDisPFKSEom+a5lXOv9E+3un/yp3T3N6bxOuvPy8nTnnXfqH//4h06ePOna3qRJE91000167rnnFBIS4k3VfoGkuyx/+oDiN7S7/6LtG7/SyXDRaIUA5YSfo179kpQ4qOaT34MHD2r+359TW7NeV10c49bznV/g1LINx+WIHlojyT48x+fdP9Hu/smf2t3TnM6r4eWSFBISotmzZ2vmzJn66quvlJGRoZiYGF1wwQVq1qyZt9UCAIBGwrIsJQ5KVuKgZNdohYiICAUEBKhly5Y+SXp3/PCNOgVt0pj+LcrsCw4K0Jj+Ni1Zt1Kpa85R4qDkGn9+AABK8zrpLtasWTMlJ/OPFgAAqFh8fLzi4+NdPSC+YIzR2uUL9EBKRKXlRvSL0MwVC0m6AQC1wqOke9++fZKkmJgYRUREuB57ol27dt5FBgAAUAVpaWmKDzus4KCwSssFBwUoLjRddrudOQIAAD7nUdLdvn17WZalZ599VnfccYfr8alYluV2vzcAAICvOBwOxYRXvkxZsehwo6ysLJJuAIDPVWl4eek517ycgw0AAKDG2Ww2ZWR7NmnP0WxLkZGRPo4IAIAqJN0k3AAAoD6Li4uTPaeF8guy3GYtLy2/wKm03Jb0cvsJY4zS0tLkcDhks9kUFxfHzPUAapVHSffatWslSR07dnR7DAAAUF9YlqXEYeO17KsnNaZ/xUu3LNuQrYSUqbUYGepC0ZJ1H2rt8gWKDzusmHCnMlivHUAd8CjpvuSSSyp9DAAAUB8kJCVr7vZvtGTdKo3oF1HOOt3ZckQP0eikwXUYJXzNGKO5s59SVOYqPZAS4Ta5Xn5BlpZ99aTm7djGeu0AakW1lwwDAACoLyzL0g1T7lXqml6auWKh4kLTFR1udDTbUlpuSyWkTNXopMEkWo1c6poPFZW5qtwRD6zXDqC2eZR0P/LII14/wUMPPeT1sQAAAFVlWZYSByUrcVCy7Ha7srKyFBkZyT3cfoL12gHUNx4l3Q8//LDXvwiTdAMAgLoSHx9Psu1nWK8dQH3j2boa/2OMKfNXcl/pcgAAAEBt8ma9dgDwJY96utu1a1empzs3N1e//vqrjDEKCgpS8+bNdeTIERUUFMiyLDVv3lzh4eE+CRoAAAAoD+u1A6hvPLoi7d27V3v27HH9ffvtt2rbtq1CQ0P1+uuvKycnRwcPHlROTo4WL16s4OBgxcXFaevWrT4OHwAAAPjNb+u1V97bzXrtAGpLlYaXF3v44Yf19ddfa9KkSbrmmmsUEFBUTUBAgP7whz9o0qRJ+v7777mfGwAAALXKtV77huOVlitar31cLUUFwJ95lXS/9dZbkqSIiPJnhYyMjJQxRu+88473kQEAAABeSEhK1rFmQ7RknaNMj3d+gVNL1mXJET1ECazXDqAWeJV0Hz58WJK0cOFC7d69223fTz/9pAULFkiSjhw5Us3wPJeZmambbrpJsbGxCg8PV2JiojZv3uzx8T/++KOSk5MVERGhmJgYXXfddTp06JBbmb1798qyrHL/3nzzzZo+JQAAAHiheL322Auma+bKKL38UY7eXHdCL3+Uo5kroxR7wX2aNHka67UDqBUeTaRWWufOnfX999/LbrfrzDPPVJ8+fdSyZUulp6dr06ZNKiwslGVZ6tSpU03HWy6n06mUlBR98803uueee9SiRQu99NJLSkhI0Ndff63OnTtXevwvv/yiAQMGKCoqSo8//riOHz+uZ599Vt9++602btyo4OBgt/LXXHONhg4d6rbtd7/7XY2fFwAAALzDeu0A6guvku6pU6fqj3/8oyzL0smTJ7VhwwbXPmOM61fDW2+9tWaiPIWlS5dq/fr1evvttzVq1ChJ0pgxY9SlSxf9+c9/1htvvFHp8Y8//riys7P19ddfq127dpKkvn376tJLL9X8+fN10003uZU/77zzNHbsWN+cDAAAAGoU67UDqEteDS+/8cYbde+997oel7du9913360bb7yx+hF6YOnSpWrVqpWuvPJK17bY2FiNGTNG//rXv5SXl1fp8e+8846GDRvmSrgladCgQerSpYuWLFlS7jHZ2dnKz8+vmRMAAAAAADRKXvV0S9ITTzyhkSNHat68edq0aZMyMzPVrFkz9enTRxMnTtT5559fk3FWasuWLTrvvPNcs6gX69u3r+bMmaOdO3eqR48e5R574MABpaenq0+fPmX29e3bVytXriyzfcaMGbrnnntkWZZ69+6tmTNn6rLLLqs0xry8PLfk3+FwSCoaGu90Vr6khb9wOp0yxvB6+Bna3X/R9v6JdvdPtLt/ot39kz+1u6fn6HXSLUl9+vQpN1mtbXa7XQMGDCizvXgY0cGDBytMuu12u1vZ0sdnZGQoLy9PISEhCggI0GWXXabf//73Ou200/Tzzz/r+eef15AhQ/T+++8rJSWlwhifeOIJzZgxo8z2Q4cOKTc316PzbOycTqeOHTsmY0yZH1DQeNHu/ou290+0u3+i3f0T7e6f/Knds7KyPCpXraS72M6dO3Xo0CG1b99ep512WrXqcjqdHg/bDgkJkWVZysnJUUhISJn9oaGhkqScnJwK6yjed6rjQ0JC1K5dO3300UduZa677jqdddZZuuuuuypNuqdPn64777zT9djhcKht27aKjY2VzWar5Cz9h9PplGVZio2NbfQfUPyGdvdftL1/ot39E+3un2h3/+RP7V6cL56K10m3MUZPPfWUnn/+edfSYM8884zOOOMMvfjii7IsS2+88YZatWpVpXo/++wzJSYmelT2xx9/VLdu3RQWFlbufdvFPchhYWEV1lG8z9vjY2JidP311+vJJ5/UL7/8ojZt2pRbLiQkpNzEPiAgoNG/GavCsixeEz9Eu/sv2t4/0e7+iXb3T7S7f/KXdvf0/LxOuq+55hq9/fbbktxnLL/ooos0evRoOZ1OLVmyRFOnTq1Svd26ddNrr73mUdniIeHx8fGuYeIlFW9r3br1Keuo6PiYmJhyk+WS2rZtK0nKyMioMOkGAAAA/JUxRmlpaXI4HLLZbIqLi2OddPgNr5LuN954Q0uWLJFlWW4zlktFs4b369dPX375pdasWVPlpDsuLk4TJkyo0jG9evXS559/LqfT6fZrw4YNG9S0aVN16dKlwmNPO+00xcbGatOmTWX2bdy4Ub169Trl8//888+Sis4dAAAAQBFjjFLXfKjUFQsVH3ZYMeFOZWQHyJ7TQonDxishKZnkG42eV/398+bNkyQFBQXp6aefLrO/T58+MsZo27Zt1YvOQ6NGjdKvv/6qd99917Xt8OHDevvtt3X55Ze79VTv3r1bu3fvdjt+5MiRWr58ufbv3+/atmbNGu3cuVOjR492bTt06FCZ5z5w4IBeffVV9ezZk/UfAQAAgP8xxujjFe/o8Ian9UBKlm4eHKar+ofr5sFheiAlS4e+elLzXnq6TCce0Nh41dO9ZcsWWZal6667TnfffbemTZvmtj8uLk6SlJaWVv0IPTBq1ChdcMEFuv766/XDDz+oRYsWeumll1RYWFhmxvCkpCRJ0t69e13b7r//fr399ttKTEzUbbfdpuPHj+uZZ55Rjx49dP3117vKTZs2Tbt371ZSUpJat26tvXv36u9//7uys7P117/+tVbOFQAAAGgIPv3kI4Vnf60rL7IpwHJPrIODAjSmv01L1q1U6ppzlDgouY6iBHzPq57u7OxsSVKHDh3K3V88dXpt/WoVGBiolStX6qqrrtKLL76oe+65Ry1atNAnn3yirl27nvL4tm3b6tNPP1XHjh1133336emnn9bQoUO1evVqt17yyy67TJZlafbs2Zo8ebLmzJmjAQMG6Msvv1RCQoIPzxAAAABoOIwxSl25SP26VD6784h+EUpdsbCWogLqhlc93c2bN9evv/5a4fDx1atXS5JatmzpfWRVFB0drblz52ru3LmVlivZw13S2WefXWY5sNKuueYaXXPNNd6GCAAAAPiFtLQ0xYceVlCTmErLBQcFKC40XXa7nVs10Wh51dPdt29fGWO0dOlSt+Hb3333na655hpt2rRJlmWpX79+NRYoAAAAgIbB4XAoJtzpUdnocOMaKQs0Rl71dE+aNEnvv/++jDF65JFHJBUNIVmwYIFbuYkTJ1Y/QgAAAAANis1mU0a2Z/17R7MtRUZG+jgioO541dN9+eWXa+zYsa57ti3Lck31X7ztuuuuU3IyEyIAAAAA/iYuLk723BYqOFl5b3d+gVNpuS0ZWo5GzaukW5IWLFigxx9/XM2bN5cxxvXXvHlzzZw5U6+99lpNxgkAAACggbAsSwlDr9OGnbmVllu2IVsJKeNqKSqgbniddFuWpfvuu0+//vqrfvjhB61bt04//PCD0tPTNX36dBa5BwAAAPzYJQMHKzu8t5Z+4VB+gXuPd36BU0vWZckRPUQJSYPrKEKgdlT5nu6srCxdcsklkqR+/frp5ZdfVrdu3Wo8MAAAAAANl2VZuixlpH78rqNmrlykuNB0RYcbHc22lJbbUgkpUzU6aTCddWj0qpx0R0ZGavv27crLy9Pll1/ui5gAAAAANAKWZSkhabAGXjpEdrtdWVlZioyM5B5u+BWvhpcX92yfOHGiRoMBAAAA0DjFx8erS5cuJNzwO14l3VOmTJExRu+88w5r6gEAAAAAUAGv1unu3LmzLr74Yn3++ec699xzNWXKFHXr1k3h4eFlyg4YMKDaQQIAAAAA0BB5lXQnJCS4Jjz4+eefdffdd5dbzrIsnTx50vvoAAAAAABowLxKuouVnGnQGOO2veRjAABQNcYYpaWlyeFwyGazKS4ujhl+AQBogLxOuitLqkm4AQDwjjFGqWs+1NrlCxQfdlgx4U5lZAfIntNCicPGKyEpmeQbAIAGxKuke8+ePTUdBwAAfs8Yo7mzn1JU5io9kBKh4KAw1778giwt++pJzduxTZMmTyPxBgCggfAq6T799NNrOg4AAPxe6poPFZW5SmP628rsCw4K0Jj+Ni1Zt1Kpa85R4qDkOogQAABUVZWT7l9//VWrVq1SWlqa4uLiNGTIELVq1coXsQEA4DeMMVq7fIEeSImotNyIfhGauWIhSTcAAA1ElZLuxYsX609/+pNycnJc28LCwvTSSy9p3LhxNR4cAAD+Ii0tTfFhh92GlJcnOChAcaHpstvtio+Pr6XoAACAtwI8Lbht2zZNnDhRJ06ccE2UZozRiRMndMMNN+ibb77xWZAAADR2DodDMeFOj8pGhxtlZWX5OCIAAFATPE66//rXv+rkyZOuiVuKE2/LslRYWKgXX3zRNxECAOAHbDabMrI9+2f5aLalyMhIH0cEAABqgsdJ92effVZ0QECA/u///k/Lly/X/fff70rCi/cDAICqi4uLkz2nhfILKu/tzi9wKi23JUPLAQBoIDxOug8ePCjLsnTVVVfp0Ucf1dChQ/XYY4/p6quvljFGBw4c8GWcAAA0apZlKXHYeC3bcLzScss2ZCshhXlUAABoKDxOuosnTzvnnHPctvfs2VOSlJeXV4NhAQDgfxKSknWs2RAtWeco0+OdX+DUknVZckQPUULS4DqKEAAAVFWVlwwLDAys9DEAAPCOZVm6Ycq9Sl3TSzNXLFRcaLqiw42OZltKy22phJSpGp002HVrFwAAqP+qnHR//PHHOn78t6Fv69evd/3/I488Uqb8Qw895GVoAAD4H8uylDgoWYmDkmW325WVlaXIyEju4QYAoIGqctK9evVqrV69usx2Y4xmzJhRZjtJNwAA3omPjyfZBgCggfP4nu7KWJZVZqhb8ZJiAAAAAAD4qyr1dJNIAwAAAADgOY+T7j179vgyDgAAAAAAGh2Pk+7TTz/dl3EAAAAAANDo1Mg93QAAAAAAoCyPku533nmn2k/07rvvVrsOAAAAAAAaEo+S7tGjR6tr166aNWuW0tLSPK780KFDmj17ts4880yNGTPG6yABAAAAAGiIPL6n+6efftLtt9+uO++8UxdccIEuvPBC9enTR23btlVMTIwk6ejRo9q/f782b96sL7/8UuvWrZPT6ZQxpsySYgAAAAAANHYeJd3Tpk3TrFmzlJOTo8LCQq1fv17r168/5XHFS4yFhobqtttuq16kAAAAAAA0MB4NL3/yySf1008/6ZZbblFUVJSMMR79RUZGavLkydq1a5eeeOIJX58LAAAAAAD1isfDy+Pj4/Xiiy/q6aef1rvvvquPP/5Yn3/+ufbu3evq0bYsS+3atdNFF12kSy+9VKNHj1bTpk19FjwAAAAAAPWZx0l3sdDQUP3hD3/QH/7wB0mS0+nUkSNHJEkxMTEKDAys2QgBAAAAAGigqpx0lxYQEKDY2NiaiAUAAAAAgEbFo3u6AQAAAABA1ZF0AwAAAADgIyTdAAAAAAD4CEk3AAAAAAA+QtINAAAAAICPkHQDAAAAAOAjJN0AAAAAAPhItdfpLmnBggXavHmzmjVrphtuuEFt27atyeoBAAAAAGhQvEq6X375ZT366KOyLEtffPGF2rdvr6uvvlpvv/22W5lNmzapXbt2NRYsAAAAAAANiVfDyz/99FOlpaUpNDRU7du31+7du7VkyRJJkjFGxhgdOXJEzz77bI0GCwAAAABAQ+JV0r1161ZZlqX+/ftLktauXStJatKkiYYPH64mTZrIGKM1a9bUXKQAAAAAADQwXiXd6enpkuS6Z/v777+XJF1xxRVatmyZxo0bJ0nat29fTcQIAAAAAECD5FXSffz4cUlSeHi4JGnXrl2yLEs9evSQJHXq1EmSVFBQUBMxAgAAAADQIHmVdEdFRUkqurc7PT1dGzZskCR17txZkpSRkSFJat68eU3ECAAAAABAg+RV0t29e3dJ0urVqxUfH68jR45Ikvr27StJOnDggCTptNNOq4kYAQAAAABokLxKusePHy9jjCS5/nvBBRfojDPOkDFGa9eulWVZOv/882suUgAAAAAAGhiv1umeMGGC9u7dq1deeUW5ubm6+OKL9fLLL0uSvv32W9lsNkVFRemyyy6r0WABAAAAAGhIvEq6Jenhhx/Www8/XGZ7z549tX379urEBAAAAABAo+DV8HIAAAAAAHBqHvV0L1y40OsnKF6zGwAAAAAAf+NR0j1hwgRZluXVE5B0AwAAAAD8lc+GlxfPag4AAAAAgL/yeCI1kmgAAAAAAKrGo6Tb6XT6Og4AAAAAABodZi8HAAAAAMBHSLoBAAAAAPARj+/pLu3EiRN66aWX9NFHH+mXX35RXl5emTKWZWn37t3VChAAAAAAgIbKq6T7xIkTuvDCC/Xtt99KqniSNW+XGQMAAAAAoDHwanj5X/7yF23btk1SUcJtWZYrwS75/wAAAAAA+DOvku5//etfkqTw8HANGDDA1dN9zz33qGvXrpKkkSNH6qGHHqqhMAEAAAAAaHi8Srp37twpy7J01VVX6fLLL3dtf+qpp7R582Z169ZNH3/8sUaNGlVjgQIAAAAA0NB4lXRnZ2dLkjp06KCAgN+qOHnypEJDQzV69GhlZWVp+vTpNRMlAAAAAAANkFdJd2RkpKSi+7fDw8Nd27/55htJUlpamiRp3bp11Y0PAAAAAIAGy6vZy1u0aKHMzEwdPXpU5557rmv7iBEj1KdPHy1fvlySlJubWzNRAgAAAADQAHmVdJ911ln66aeftG/fPl144YUKDg5WQUGBDhw4oIMHD7pmNO/du3dNxwsAAAAAQIPh1fDyiy66SDExMdq5c6dsNptuvfVWV6JdLDAwUI8++miNBQoAAAAAQEPjVdJ9991369ChQ9q8ebOkolnLn3/+efXr10+dOnXSsGHDtHbtWiUkJNRkrJXKzMzUTTfdpNjYWIWHhysxMdEV36ls3LhRkydPVu/evRUUFHTKdcbnzZunM888U6GhoercubNmzZpVE6cAAAAAAGhkvEq6S7MsS7fffrvWr1+vHTt26P3339dFF11UE1V7xOl0KiUlRW+88YZuueUWPf3000pPT1dCQoJ27dp1yuNXrlypuXPnyrIsnXHGGZWW/fvf/64bbrhBZ599tmbNmqXf/e53uvXWW/XUU0/V1OkAAAAAABqJGkm669rSpUu1fv16zZ8/X3/+8581ZcoUpaamKjAwUH/+859PefzNN9+sY8eOadOmTbr00ksrLJeTk6P/+7//U0pKipYuXaobb7xRCxcu1LXXXqtHH31UR48ercnTAgAAAAA0cF5NpDZx4kSPylmWpXnz5nnzFFWydOlStWrVSldeeaVrW2xsrMaMGaPFixcrLy9PISEhFR7fqlUrj55n7dq1OnLkiCZPnuy2fcqUKXr99de1YsUKjR071ruTAAAAAAA0Ol4l3fPnzz/lfc/FE6vVRtK9ZcsWnXfeeQoIcO+479u3r+bMmaOdO3eqR48eNfI8ktSnTx+37b1791ZAQIC2bNlSYdKdl5envLw812OHwyGpaGi80+msdmyNgdPplDGG18PP0O7+i7b3T7S7f6Ld/RPt7p/8qd09PUevku5ixhi3x8WJeOntvma32zVgwIAy2+Pj4yVJBw8erJGk2263KzAwUC1btnTbHhwcrObNm+vgwYMVHvvEE09oxowZZbYfOnSI9cz/x+l06tixYzLGlPkBBY0X7e6/aHv/RLv7J9rdP9Hu/smf2j0rK8ujcl4l3QMGDCjT052Xl6fdu3fr0KFDsixLXbt29XjYdklOp1P5+fkelQ0JCZFlWcrJySl3+HhoaKikonuxa0JOTo6Cg4PL3RcaGlrp80yfPl133nmn67HD4VDbtm0VGxsrm81WI/E1dE6nU5ZlKTY2ttF/QPEb2t1/0fb+iXb3T7S7f6Ld/ZM/tXtxvnkqXiXdqamp5W43xmjOnDmaPHmyCgoK9O6771a57s8++0yJiYkelf3xxx/VrVs3hYWFuQ3dLlbcgxwWFlblOMoTFhZW4Q8Cubm5lT5PSEhIuT8MBAQENPo3Y1VYlsVr4odod/9F2/sn2t0/0e7+iXb3T/7S7p6eX7WGl5dmWZb++Mc/aunSpfrkk0/00EMPVXkN627duum1117zqGzx8PH4+HjZ7fYy+4u3tW7dukoxVPZ8hYWFSk9Pdxtinp+fryNHjtTY8wAAAAAAGocaTbqLhYWFyRijd999t8pJd1xcnCZMmFClY3r16qXPP/9cTqfT7deGDRs2qGnTpurSpUuV6qvseSRp06ZNGjp0qGv7pk2b5HQ6XfsBAAAAAJC8TLo/++yzMtuMMcrJydFXX32llStXSpIyMjKqF52HRo0apaVLl+rdd9/VqFGjJEmHDx/W22+/rcsvv9xtWPfu3bslSR07dqzy8wwcOFAxMTF6+eWX3ZLul19+WU2bNlVKSko1zwQAAAAA0Jh4lXQnJCRUumRY8XJh3iS23hg1apQuuOACXX/99frhhx/UokULvfTSSyosLCwzY3hSUpIkae/eva5t//3vf7Vo0SJJRb3WkvTYY49Jkk4//XRdd911kop68B999FFNmTJFo0eP1uDBg/X5559r8eLFmjlzpmJiYnx9qgAAeMwYo7S0NDkcDtlsNsXFxZ1yyU8AAFCzanTJMMl92bCSs3X7UmBgoFauXKl77rlHL774onJycnT++edr/vz56tq16ymP37Nnjx588EG3bcWPL7nkElfSLUmTJ09WUFCQnnvuOb3//vtq27atXnjhBd122201e1IAAHjJGKPUNR9q7fIFig87rJhwpzKyA2TPaaHEYeOVkJRM8g0AQC2xjBeLap9qlrYuXbpo2rRpmjhxoteBNXYOh0NRUVE6duwYS4b9j9PpdE1S19hnOsRvaHf/Rdv7hjFGc2c/pajMVRrRL0LBQb+9tvkFTi3bcFyO6KGaNHlanSTetLt/ot39E+3un/yp3T3N6bzq6d6zZ0+52wMCAtSsWTNFRkZ6Uy0AAKim1DUfKipzlcb0L/uPf3BQgMb0t2nJupVKXXOOEgcl10GEAAD4F6+S7tNPP72m4wAAANVkjNHa5Qv0QEpEpeVG9IvQzBULSbrRoDBHAYCGyidLhgEAgNqXlpam+LDDCg4Kq7RccFCA4kLTZbfbFR8fX0vRAd5hjgIADZ1HSffAgQO9qtyyLK1Zs8arYwEAQNU4HA7FhDs9KhsdbpSVlUXSjXqt5BwFD6REuP2glF+QpWVfPal5O7bV2RwFAOAJj5Lu1NTUKl/IipcNAwAAtcNmsykj27NJa45mW8zBgnqPOQoANAYeTydnjCn3r/Q+AABQN+Li4mTPaaH8gsp7u/MLnErLbUkvN+q14jkKRvQ79RwFqSsW1lJUAFB1HvV0jx8/vsy2nTt36ssvv1STJk3Uv39/tWrVSr/++qvWrVunkydPqnfv3urevXuNBwwAAMpnWZYSh43Xsq+eLLdnsNiyDdlKSJlai5EBVcccBQAaC4+S7tdee83t8c8//6y+ffuqbdu2+uyzz9xmM9+7d68GDBigXbt2afHixTUbLQAAqFRCUrLmbv9GS9ZVtE53thzRQzQ6aXAdRgmcGnMUAGgsvFqt/L777tPRo0c1duzYMsuHtW/fXmPHjpXD4dD9999fI0ECAADPWJalG6bcq9gLpmvmyii9/FGO3lx3Qi9/lKOZK6MUe8F9TDqFBoE5CgA0Fl4tGVY8I/nRo0fL3V+8PTU11buoAKAeYo1YNBSWZSlxULISByXLbrcrKytLkZGR9AKiQfltjoIstxEbpTFHAYD6zqukOy8vT5L06quvqm/fvrrmmmsUEhKivLw8vfHGG3r11VfdygFAQ8YasWjI4uPjSUbQIDFHAYDGwquku0+fPvr888+Vn5+vSZMmadKkSYqMjFRWVpak35YLO//882s0WACobawRCwB1hzkKADQGXiXdjzzyiAYNGqTCwkLXMmEOh0OSXF86AwMDNWPGjBoKEwDqBmvEAkDdKZ6jIHVNL81csVBxoemKDjc6mm0pLbelElKmanTSYH70BFCveZV0DxgwQO+++65uuOEGpaenu+0zxig2NlZz587VgAEDaiRIAKgLxWvEPpBy6jViZ65YSNINAD7AHAUAGjqvkm5JGjZsmPbs2aP3339fmzZtUmZmppo1a6Y+ffpo+PDhCgurfE1FAKjvWCMWAOoX5igA0BB5nXRLUlhYmK666ipdddVVNRUPANQbrBELAACA6vJqnW4A8AesEQsAAIDq8qin+4wzzpAk/d///Z8mTZrkenwqlmVp9+7d3kcHAHWINWIBAABQXR4l3Xv37pVlWTp27Jjb4+KZy0sr3sdMkgAaMtaIBQAAQHV5Pby8ooT7VPsAoCFJSErWsWZDtGSdQ/kF7vd35xc4tWRdlhzRQ5TAGrEAAAAoh0c93X/+858lSRdeeKHbYwBo7FgjFgAAANVRpaS7oscA0JixRiwAAAC8Va0lwwDA37BGLAAAAKrCq3u67Xa7Pv74Y3388ceuydV+/PFHJSYmKioqSqeffrpeeumlGg0UAAAAAICGxque7lmzZumpp55SYGCgDh06JKfTqaFDh2rfvn0yxigrK0tTp05V+/btNXTo0JqOGQAAAACABsGrnu4NGzbIGKMLLrhAUVFR+vzzz/Xf//7XrYwxRq+88kqNBAkAAAAAQEPkVdL9008/ybIsde/eXVJREi5JrVu31rvvvqv27dtLkjZv3lwzUQIAAABAA2WMkd1u144dO2S321li2c94Nbz80KFDkqQ2bdpIknbs2CFJGj58uEaMGKH//Oc/euKJJ1zlAAAAAMDfGGOUuuZDrV2+QPFhhxUT7lRGdoDsOS2UOGy8EpKSWXbUD3iVdDudTklSdna2JGn79u2yLEtdunSRJIWHh0uSgoODayJGAAAAAGhQjDGaO/spRWWu0gMpEQoOCnPtyy/I0rKvntS8Hds0afI0Eu9Gzqvh5cXL5SxevFhPP/20Nm7cKEnq1q2bpKLZzSWpZcuWNREjAAAAADQoqWs+VFTmKo3pb1NwkHvaFRwUoDH9bbIdXanUNR/VUYSoLV4l3RdddJGMMfrll180ffp0FRYWKiQkRBdeeKEkadeuXbIsS506darRYAEAAACgvjPGaO3yBRrRL6LSciP6RSh1xcJaigp1xauk+7777lPTpk1ljHFNAjB16lRFRkbq2LFjSk1NlSRXEg4AAAAA/iItLU3xYYfL9HCXFhwUoLjQdNdIYTROXt3T3b17d/3nP//RggULlJubq4svvlgjR46UJB09elQzZsyQJP3+97+vuUgBAAAAoAFwOByKCXd6VDY63CgrK8t1Cy8aH6+Sbkk688wz9eSTT5bZ3r59e917773VCgoAAAAAGiqbzaaMbM8GFR/NthQZGenjiFCXvE66i2VmZmrjxo06dOiQzjvvPJ155pk1ERcAAAAANDjFt+B+ezBY/007oXatwiqcnTy/wKm03Jb0cjdyXifdOTk5uuOOO/Taa6/p5MmTkqRnnnlG27Zt0/Tp02VZlv7973+rQ4cONRYsAAAAANRHpdfk7tHikF5btk/OgKZK7N1GCefFlkm+l23IVkLK1DqKGLXFq6T75MmTSk5O1rp161wTqRW/gYYMGaLx48eroKBAS5YsYag5AAAAgEatvDW5jdpp3958mZxf9dVPuzTvgEOTLu8oy7KUX+DUsg3ZckQP0eikwXUdPnzMq9nL58yZo88//7zcfTabTf379y/6ped/s5gDAAAAQGNV3prclqR27TspIrazzu0YpuPH7Hp48X69/FGOZq6MUuwF92nS5GkVDj1H4+FV0r148WLp/9u787io6v2P4+9h3xEFBYqyTNRM01RU7CrkLup1wS1NMf1VLq1Xs271o0V/7WXmUmapqf3ULC2X7JphmRr96FppcvW6VgruCCqLOuf3hzGXEUSWOWzzej4e83g4Z5vPmQ8jvOd8zzmS/Pz8tHz58kLzW7ZsKUn69ddfy1EaAAAAAFRtxd2T2yIpOLiuGt16h+7t30qZlgj1e2iRnnvzI8V26UHgdhJlCt2//vqrLBaLRo0apfj4+ELz69SpI0k6duxY+aoDAAAAgCqspPfkDvDzUWRITgVVhaqkTKE7NzdXklSvXr0i558+ffryxl3KtHkAAAAAqBbKck9uOJcypeL8sL1169ZC8y5duqTVq1dLksLDw8tRGgAAAABUbdyTG9dSptAdHR0twzC0fv16jR492jZ98+bNio2N1b/+9S9ZLBZ16NDBYYUCAAAAQFUTGhqqtOxg5V0o/mg39+R2XmUK3ePHj7f9+8MPP5TFYpFhGPr888+1ZcsW27xx48aVv0IAAAAAqKIsFotie4/SquSzxS53+Z7cIyuoKlQlZQrdf/nLX/Tkk0/a7tEt/ec+3fnTnnzySbVr184BJQIAAABA1RXTuYfO1Oqp5d9lFjrinXfBquXfZSkzqKdiuCe3U3Ir64rTpk1Ts2bN9Oqrr+qnn36yhe0WLVpo8uTJGjZsmMOKBAAAAICqymKxaOyEKdq0sYWmrf1QoV7HFORr6PQ5i9Jz6iom7kEN6tydW4Q5qTKHbkkaOnSohg4dquzsbJ0+fVq1atWSj4+Po2oDAAAAgGrBYrEotksPxXbpobS0NGVlZcnf359zuFG24eVX8vb2Vnh4eKHAvWrVKkdsHgAAAACqjbCwMEVGRhK4IclBobsgwzC0dOlSNW/eXAMHDnT05gEAAAAAqDZKNbz88OHDevPNN7Vr1y4FBwdryJAhiouLs81fsmSJnn/+ee3du1eGYXDOAgAAAADAqZU4dB89elRRUVFKT0+3TVuyZIlee+013X///erbt6+SkpLsrmgOAAAAAIAzK3HofvPNN5WWlmZ39NowDP33f/+3tm/frq+//lqSbPfs9vHx0dixYx1fMQAAAAAA1USJQ/eGDRskXQ7aHh4e8vPz06lTp3Tu3DktWbLEFrYDAgI0YcIEPfLIIwoODjatcAAAAAAAqroSX0ht//79slgsat68uY4dO6YTJ05o+vTpkmQbUn7//ffr4MGDmjp1KoEbAAAAAOD0Shy6s7KyJEn9+vVTQECAJGn06NGSLg8p7927t+bMmaPAwEATygQAAAAAoPopcei2Wq2SZAvckuTv72/7d3R0tAPLAgAAAACg+ivVLcMkKSUlRR9++GGh6T///HOR00eOHFm2ygAAAAAAqOZKHbqXLVumZcuW2U0zDKPI6RKhGwAAAADgvEoduiXZ3Yv7yluI5U8zDMNuHgAAAAAAzqZUobtg2C7tNAAAAAAAnE2JQ3dSUpKZdQAAAABAjWYYhtLT05WZmamAgACFhoYyOtgJlDh0d+rUycw6AAAAAKBGMgxDmzauV9KahQrzPqHavladOueitOxgxfYepZjOPQjfNViZzukGAAAAAFybYRiaN+tlBWZ8oafj/OTh7m2bl3chS6u+f0nv7/5FY8Y/TvCuoUp8n24AAAAAQOls2rhegRlfaPCdAfJwt49fHu4uGnxngAJOr9OmjV9WUoUwG6EbAAAAAExgGIaS1ixUv7Z+xS7Xr62fNq39sIKqQkUjdAMAAKDGMQxDaWlp2r17t9LS0ri7DipFenq6wrxPFDrCfSUPdxeFeh1TWlpaBVWGisQ53QAAAKgxuGAVqpLMzEzV9rWWaNkgX0NZWVkKCwszuSpUNEI3AAAAagQuWIWqJiAgQKfOlWxw8elzFvn7+5tcESoDw8sBAKXCkE0AVRUXrEJVExoaqrTsYOVdKP5od94Fq9Jz6nKUu4biSDcAoEQYsgmgKsu/YNXTcde+YNW0tR8qtkuPCqoMzsxisSi29yit+v4lDb4z4KrLrUo+p5i4ByuwMlQkQjcA4JoYsgmgqvvPBau8i12u4AWrOKqIihDTuYfm/etnLf/uC/Vr62c3CiPvglWrks8pM6inBnXuXolVwkyEbgDANRUcsnml/CGby79bp00bb+foEYBKwQWrUFVZLBaNnTBFmza20LS1HyrU65iCfA2dPmdRek5dxcQ9qEGdu/OldQ1G6AYAFIshmwCqAy5YharMYrEotksPxXbpobS0NGVlZcnf358vfpwEoRsAUCyGbAKoDv5zwaqsYu+JzAWrUNnCwsL4+XMyNebq5RkZGbrvvvsUEhIiX19fxcbG6p///GeJ1v3hhx80fvx4tWrVSu7u7sUO7bBYLEU+XnrpJUftCgBUKWUZsgkAFc12warks8Uud/mCVSMrqCoAqCFHuq1Wq+Li4vTzzz9r8uTJCg4O1uzZsxUTE6Mff/xRDRs2LHb9devWad68eWrevLluvvlm7dmzp9jlu3btqpEj7f+zbtmyZbn3AwCqIoZsAqguuGAVgKqoRoTuFStWaOvWrfr4448VHx8vSRo8eLAiIyOVmJiojz76qNj1x40bpylTpsjb21sTJ068ZuiOjIzUiBEjHFY/AFRlDNkEUF1wwSoAVVGNCd316tXTgAEDbNNCQkI0ePBgLV68WLm5ufL09Lzq+vXq1Sv1a2ZnZ8tiscjLy6tMNQNAdcE9RgFUJ1ywCkBVUyNC9/bt23XHHXfIxcX+CExUVJTmzp2rPXv2qFmzZg57vQULFmj27NkyDENNmjTR008/rbvvvrvYdXJzc5Wbm2t7npmZKeny0HirtWTnStZ0VqtVhmHwfjgZ+l49dIztpg/+9bOWf7defaMKD9n8/IfLQzYHxnYtcS/pvXOi786psvper14928EVfuYqHp935+RMfS/pPtaI0J2WlqaOHTsWmp7/jeaRI0ccFrqjo6M1ePBg3XTTTTpy5IhmzZql4cOH68yZMxo3btxV13vxxRf13HPPFZp+/Phx5eTkOKS26s5qterMmTMyDKPQFyioueh79dF74Cjt+LmZZv+QpFruGfLzks7mSBkXaum21rH6y+136Pjx4yXeHr13TvTdOdF350TfnZMz9b2kF4+tcqHbarUqLy+vRMt6enrKYrEoOzu7yOHj+UO/s7OzHVbfli1b7J7fe++9atWqlf7+978rISFB3t5F31LnySef1GOPPWZ7npmZqYiICIWEhCgg4OrDNZ2J1WqVxWJRSEhIjf+A4j/oe/VSr1tPdenWU+np6bYhm6GhoWXaFr13TvTdOdF350TfnZMz9b2kpxpXudD97bffKjY2tkTLpqamqnHjxvL29rYbup0v/wjy1YKwI3h4eGjixIl64IEH9OOPP+rOO+8scjlPT88ivxhwcXGp8T+MpWGxWHhPnBB9r37Cw8Mdsh1675zou3Oi786JvjsnZ+l7SfevyoXuxo0ba/78+SVaNn/4eFhYmNLS0grNz5/mqD8OryYiIkKSdOrUKVNfBwAAAABQvVS50B0aGqqEhIRSrdOiRQtt3rxZVqvV7tuG5ORk+fj4KDIy0sFV2tu/f7+ky1dMBwAAAAAgX4043h8fH6+jR4/q008/tU07ceKEPv74Y/Xp08duWPe+ffu0b9++Mr1OURcIysrK0vTp0xUcHKxWrVqVabsAAAAAgJqpyh3pLov4+Hi1a9dOo0eP1q5duxQcHKzZs2fr0qVLha4Y3rlzZ0nSwYMHbdMOHTqkRYsWSZJSUlIkSVOnTpUk3XjjjbrnnnskSbNmzdKqVavUp08f3XDDDUpLS9MHH3yg3377TYsWLZKHh4fZuwoAAAAAqEZqROh2dXXVunXrNHnyZM2YMUPZ2dlq06aNFixYoEaNGl1z/QMHDuiZZ56xm5b/vFOnTrbQ3aFDB23dulXz5s3TyZMn5evrq6ioKH3wwQe66667HL9jAAAAAIBqrUaEbkkKCgrSvHnzNG/evGKXK3iEO19MTIwMw7jma3Tt2lVdu3Yta4kAAAAAACdTI87pBgAAAACgKqoxR7oBAAAAwNEMw1B6eroyMzMVEBCg0NBQWSyWyi4L1QihGwAAAACuYBiGNm1cr6Q1CxXmfUK1fa06dc5FadnBiu09SjGdexC+USKEbgAAAAAowDAMzZv1sgIzvtDTcX7ycPe2zcu7kKVV37+k93f/ojHjHyd445o4pxsAAAAACti0cb0CM77Q4DsD5OFuH5k83F00+M4ABZxep00bv6ykClGdELoBAAAA4E+GYShpzUL1a+tX7HL92vpp09oPK6gqVGeEbgAAAAD4U3p6usK8TxQ6wn0lD3cXhXodU1paWgVVhuqK0A0AAAAAf8rMzFRtX2uJlg3yNZSVlWVyRajuCN0AAAAA8KeAgACdOleymHT6nEX+/v4mV4TqjtANAAAAAH8KDQ1VWnaw8i4Uf7Q774JV6Tl1FRYWVkGVoboidAMAAADAnywWi2J7j9Kq5LPFLrcq+Zxi4kZWUFWozgjdAAAAAFBATOceOlOrp5Z/l1noiHfeBauWf5elzKCeiuncvZIqRHXiVtkFAAAAAEBVYrFYNHbCFG3a2ELT1n6oUK9jCvI1dPqcRek5dRUT96AGde4ui8VS2aWiGiB0AwAAAMAVLBaLYrv0UGyXHkpLS1NWVpb8/f05hxulRugGAAAAgGKEhYURtlFmnNMNAAAAAIBJCN0AAAAAAJiE0A0AAAAAgEkI3QAAAAAAmITQDQAAAACASQjdAAAAAACYhNANAAAAAIBJCN0AAAAAAJiE0A0AAAAAgEkI3QAAAAAAmITQDQAAAACASQjdAAAAAACYhNANAAAAAIBJCN0AAAAAAJiE0A0AAAAAgEkI3QAAAAAAmITQDQAAAACASQjdAAAAAACYhNANAAAAAIBJCN0AAAAAAJiE0A0AAAAAgEkI3QAAAAAAmITQDQAAAACASQjdAAAAAACYhNANAAAAAIBJCN0AAAAAAJiE0A0AAAAAgEkI3QAAAAAAmITQDQAAAACASQjdAAAAAACYhNANAAAAAIBJCN0AAAAAAJiE0A0AAAAAgEkI3QAAAAAAmITQDQAAAACASQjdAAAAAACYhNANAAAAAIBJCN0AAAAAAJiE0A0AAAAAgEkI3QAAAAAAmITQDQAAAACASQjdAAAAAACYhNANAAAAAIBJCN0AAAAAAJiE0A0AAAAAgEkI3QAAAAAAmITQDQAAAACASQjdAAAAAACYhNANAAAAAIBJCN0AAAAAAJiE0A0AAAAAgEkI3QAAAAAAmITQDQAAAACASQjdAAAAAACYhNANAAAAAIBJCN0AAAAAAJiE0A0AAAAAgEkI3QAAAAAAmITQDQAAAACASWpM6M7IyNB9992nkJAQ+fr6KjY2Vv/85z+vuZ7VatWCBQvUt29fRUREyNfXV7fddpumTp2qnJycItd5//331aRJE3l5ealhw4Z6++23Hb07AAAAAIAaoEaEbqvVqri4OH300UeaOHGiXnnlFR07dkwxMTH697//Xey658+f1+jRo3X8+HE98MADmj59uqKiopSYmKiePXvKMAy75d99912NHTtWTZs21dtvv6327dvroYce0ssvv2zmLgIAAAAAqiG3yi7AEVasWKGtW7fq448/Vnx8vCRp8ODBioyMVGJioj766KOrruvh4aEtW7YoOjraNu2//uu/VL9+fSUmJmrjxo3q0qWLJCk7O1tPPfWU4uLitGLFCtuyVqtVL7zwgu677z4FBQWZuKcAAAAAgOqkRhzpXrFiherVq6cBAwbYpoWEhGjw4MH67LPPlJube9V1PTw87AJ3vv79+0uSUlNTbdOSkpJ08uRJjR8/3m7ZCRMm6Ny5c1q7dm15dwUAAAAAUIPUiCPd27dv1x133CEXF/vvEKKiojR37lzt2bNHzZo1K9U209PTJUnBwcF2ryNJrVu3tlu2VatWcnFx0fbt2zVixIgit5ebm2sX/jMzMyVdHhpvtVpLVVtNZbVaZRgG74eToe/Oi947J/runOi7c6LvzsmZ+l7SfawRoTstLU0dO3YsND0sLEySdOTIkVKH7ldeeUUBAQHq2bOn3eu4urqqbt26dst6eHioTp06OnLkyFW39+KLL+q5554rNP348eNXvWCbs7FarTpz5owMwyj0BQpqLvruvOi9c6Lvzom+Oyf67pycqe9ZWVklWq7KhW6r1aq8vLwSLevp6SmLxaLs7Gx5enoWmu/l5SXp8rnYpfE///M/+uqrrzR79mzVqlXLNj07O1seHh5FruPl5VXs6zz55JN67LHHbM8zMzMVERGhkJAQBQQElKq+mspqtcpisSgkJKTGf0DxH/TdedF750TfnRN9d0703Tk5U9/z8+a1VLnQ/e233yo2NrZEy6ampqpx48by9vYu8rzt/CPI3t7eJX79ZcuW6emnn9aYMWM0btw4u3ne3t5X/UIgJyen2Nfx9PQs8osBFxeXGv/DWBoWi4X3xAnRd+dF750TfXdO9N050Xfn5Cx9L+n+VbnQ3bhxY82fP79Ey+YPHw8LC1NaWlqh+fnTwsPDS7S9DRs2aOTIkYqLi9M777xT5OtdunRJx44dsxtinpeXp5MnT5b4dQAAAAAAzqHKhe7Q0FAlJCSUap0WLVpo8+bNslqtdt82JCcny8fHR5GRkdfcRnJysvr376/WrVtr+fLlcnMr/Na0aNFCkpSSkqJevXrZpqekpMhqtdrmAwAAAAAg1ZBbhsXHx+vo0aP69NNPbdNOnDihjz/+WH369LEb1r1v3z7t27fPbv3U1FTFxcWpfv36WrNmzVWHid91112qXbu25syZYzd9zpw58vHxUVxcnAP3CgAAAABQ3VW5I91lER8fr3bt2mn06NHatWuXgoODNXv2bF26dKnQFcM7d+4sSTp48KCky1ec6969u06fPq3JkycXutd2gwYN1L59e0mXz+l+4YUXNGHCBA0aNEjdu3fX5s2btXjxYk2bNk21a9c2f2cBAAAAANVGjQjdrq6uWrdunSZPnqwZM2YoOztbbdq00YIFC9SoUaNi1z158qR+//13SdITTzxRaP6oUaNsoVuSxo8fL3d3d73++uv6/PPPFRERoTfffFMPP/ywY3cKAAAAAFDtWQzDMCq7CGeUmZmpwMBAnTlzhluG/clqtdouUlfTr3SI/6DvzoveOyf67pzou3Oi787Jmfpe0kxXs98FAAAAAAAqEaEbAAAAAACTELoBAAAAADAJoRsAAAAAAJMQugEAAAAAMAmhGwAAAAAAkxC6AQAAAAAwCaEbAAAAAACTELoBAAAAADAJoRsAAAAAAJMQugEAAAAAMAmhGwAAAAAAkxC6AQAAAAAwCaEbAAAAAACTELoBAAAAADAJoRsAAAAAAJMQugEAAAAAMAmhGwAAAAAAkxC6AQAAAAAwCaEbAAAAAACTELoBAAAAADAJoRsAAAAAAJMQugEAAAAAMAmhGwAAAAAAkxC6AQAAAAAwCaEbAAAAAACTELoBAAAAADAJoRsAAAAAAJMQugEAAAAAMAmhGwAAAAAAkxC6AQAAAAAwCaEbAAAAAACTuFV2AQAAAKhaDMNQenq6MjMzFRAQoNDQUFkslsouCwCqJUI3AAAAJF0O25s2rlfSmoUK8z6h2r5WnTrnorTsYMX2HqWYzj0I3wBQSoRuAAAAyDAMzZv1sgIzvtDTcX7ycPe2zcu7kKVV37+k93f/ojHjHyd4A0ApcE43AAAAtGnjegVmfKHBdwbIw93+T0QPdxcNvjNAAafXadPGLyupQgCongjdAAAATs4wDCWtWah+bf2KXa5fWz9tWvthBVUFADUDoRsAAMDJpaenK8z7RKEj3FfycHdRqNcxpaWlVVBlAFD9EboBAACcXGZmpmr7Wku0bJCvoaysLJMrAoCag9ANAADg5AICAnTqXMn+LDx9ziJ/f3+TKwKAmoPQDQAA4ORCQ0OVlh2svAvFH+3Ou2BVek5dhYWFVVBlAFD9EboBAACcnMViUWzvUVqVfLbY5VYln1NM3MgKqgoAagZCNwAAABTTuYfO1Oqp5d9lFjrinXfBquXfZSkzqKdiOnevpAoBoHpyq+wCAAAAUPksFovGTpiiTRtbaNraDxXqdUxBvoZOn7MoPaeuYuIe1KDO3WWxWCq7VACoVgjdAAAAkPTnMPMuPRTbpYfS0tKUlZUlf39/zuEGgHIgdAMAAKCQsLAwwjYAOADndAMAAAAAYBJCNwAAAAAAJiF0AwAAAABgEkI3AAAAAAAmIXQDAAAAAGASQjcAAAAAACYhdAMAAAAAYBJCNwAAAAAAJiF0AwAAAABgEkI3AAAAAAAmIXQDAAAAAGASQjcAAAAAACYhdAMAAAAAYBJCNwAAAAAAJiF0AwAAAABgEkI3AAAAAAAmIXQDAAAAAGASQjcAAAAAACYhdAMAAAAAYBJCNwAAAAAAJiF0AwAAAABgEkI3AAAAAAAmcavsApyVYRiSpMzMzEqupOqwWq3KysqSl5eXXFz4PshZ0HfnRe+dE313TvTdOdF35+RMfc/PcvnZ7moI3ZUkKytLkhQREVHJlQAAAAAAyiorK0uBgYFXnW8xrhXLYQqr1aojR47I399fFoulssupEjIzMxUREaHff/9dAQEBlV0OKgh9d1703jnRd+dE350TfXdOztR3wzCUlZWl8PDwYo/qc6S7kri4uOj666+v7DKqpICAgBr/AUVh9N150XvnRN+dE313TvTdOTlL34s7wp2vZg+yBwAAAACgEhG6AQAAAAAwCaEbVYanp6cSExPl6elZ2aWgAtF350XvnRN9d0703TnRd+dE3wvjQmoAAAAAAJiEI90AAAAAAJiE0A0AAAAAgEkI3QAAAAAAmITQDQAAAACASQjdMF1ubq6mTJmi8PBweXt7q23bttqwYcM111u5cqW6d++u8PBweXp66vrrr1d8fLx27txZAVWjvMra9yt17dpVFotFEydONKFKmKGsvX/22WdlsVgKPby8vCqgapRXeT/zy5YtU/v27eXr66tatWopOjpaX3/9tYkVwxHK2vf69esX+Xm3WCxq2LBhBVSO8ijP5/2rr75SbGysgoODVatWLUVFRWnRokUmVwxHKE/fly5dqjvuuENeXl4KCQnRmDFjdOLECZMrrjrcKrsA1HwJCQlasWKFHnnkETVs2FALFixQr169lJSUpDvvvPOq6+3YsUNBQUF6+OGHFRwcrPT0dH3wwQeKiorStm3bdPvtt1fgXqC0ytr3gj799FNt27bN5ErhaOXt/Zw5c+Tn52d77urqama5cJDy9P3ZZ5/V888/r/j4eCUkJOjChQvauXOnDh8+XEHVo6zK2vfp06fr7NmzdtMOHTqkp59+Wt26dTO7bJRTWfv++eefq1+/fmrfvr3ti9bly5dr5MiROnHihB599NEK3AuUVln7PmfOHI0fP16dO3fWG2+8oT/++ENvvfWWUlJSlJyc7BxfrhuAiZKTkw1Jxquvvmqblp2dbTRo0MBo3759qbeXnp5uuLm5Gffff78jy4SDOaLv2dnZRv369Y3nn3/ekGRMmDDBrHLhQOXpfWJioiHJOH78uNllwsHK0/dt27YZFovFeOONN8wuEw7m6N/xL7zwgiHJ2LJliyPLhIOVp+9du3Y1wsPDjZycHNu0CxcuGA0aNDCaN29uWs0ov7L2PTc316hVq5bRsWNHw2q12qavXr3akGTMmDHD1LqrCoaXw1QrVqyQq6ur7rvvPts0Ly8vjRkzRtu2bdPvv/9equ3VrVtXPj4+ysjIcHClcCRH9P2VV16R1WrVpEmTzCwVDuaI3huGoczMTBmGYWapcKDy9H369OkKDQ3Vww8/LMMwCh39RNXl6N/xH330kW666SZFR0c7ulQ4UHn6npmZqaCgIHl6etqmubm5KTg4WN7e3qbWjfIpa9937typjIwMDRkyRBaLxTa9d+/e8vPz09KlS02vvSogdMNU27dvV2RkpAICAuymR0VFSZJ++umna24jIyNDx48f144dOzR27FhlZmaqc+fOZpQLBylv33/77Te99NJLevnll/klXM044jN/8803KzAwUP7+/hoxYoSOHj1qRqlwoPL0fePGjWrTpo1mzJihkJAQ+fv7KywsTDNnzjSzZDiAIz7vBbeVmpqqu+++25ElwgTl6XtMTIx+/fVXPfPMM9q7d6/27dunF154QSkpKXr88cfNLBvlVNa+5+bmSlKRf895e3tr+/btslqtji22CuKcbpgqLS1NYWFhhabnTzty5Mg1t9GuXTvt3r1bkuTn56enn35aY8aMcWyhcKjy9v1vf/ubWrZsqaFDh5pSH8xTnt4HBQVp4sSJat++vTw9PbV582bNmjVLP/zwg1JSUgr9okfVUda+nz59WidOnNCWLVv09ddfKzExUTfccIPmz5+vBx98UO7u7rr//vtNrR1l54jf8fmWLFkiSRo+fLhjioNpytP3Z555RgcOHNC0adM0depUSZKPj48++eQT/fWvfzWnYDhEWfvesGFDWSwWbdmyRaNHj7ZN3717t44fPy7p8u+COnXqmFB11UHohqmys7PthhDly79gQnZ29jW3MX/+fGVmZmr//v2aP3++srOzdenSJbm4MFCjqipP35OSkvTJJ58oOTnZtPpgnvL0/uGHH7Z7PnDgQEVFRWn48OGaPXu2nnjiCccWC4cpa9/zh5KfPHlSS5cu1ZAhQyRJ8fHxatasmaZOnUrorsIc8TtekqxWq5YuXaqWLVuqSZMmDq0Rjleevnt6eioyMlLx8fEaMGCALl26pLlz52rEiBHasGGD2rVrZ1rdKJ+y9j04OFiDBw/WwoUL1aRJE/Xv31+HDx+2fbF64cKFEv9fUZ2RWmAqb29v27CSgnJycmzzr6V9+/bq3r27xo0bpy+//FKLFy/Wk08+6fBa4Thl7fvFixf10EMP6Z577lGbNm1MrRHmcMRnvqC7775boaGh+uqrrxxSH8xR1r7nT3d3d1d8fLxtuouLi4YMGaI//vhDv/32mwkVwxEc9Xn/5ptvdPjwYY5yVxPl6fvEiRO1evVqLV26VEOHDtXw4cP11VdfKSwsrNAXr6haytP3d999V7169dKkSZPUoEEDdezYUc2aNVOfPn0kye6OJTUVoRumCgsLU1paWqHp+dPCw8NLtb2goCDdddddtmFoqJrK2vcPP/xQu3fv1v3336+DBw/aHpKUlZWlgwcP6vz586bVjfJz9GdekiIiInTq1Kly1wbzlLXvtWvXlpeXl+rUqVPo1nB169aVdHnYIaomR33elyxZIhcXFw0bNsyh9cEcZe17Xl6e3n//fcXFxdmNVnR3d1fPnj2VkpKivLw8c4pGuZXn8x4YGKjPPvtMhw4d0jfffKODBw9q0aJFSktLU0hIiGrVqmVW2VUGoRumatGihfbs2aPMzEy76flDh1u0aFHqbWZnZ+vMmTOOKA8mKWvff/vtN124cEEdOnTQTTfdZHtIlwP5TTfdpH/84x+m1o7ycfRn3jAMHTx4UCEhIY4qESYoa99dXFzUokULHT9+vNAf2/nnB9L7qssRn/fc3Fx98skniomJKdOXcqh4Ze37yZMndfHiRV26dKnQvAsXLshqtRY5D1WDIz7vN9xwgzp27Kgbb7xRGRkZ+vHHH9WlSxczyq1yCN0wVXx8vO18nXy5ubmaP3++2rZtq4iICEmXw9a//vUvu3WPHTtWaHsHDx7Uxo0b1bp1a3MLR7mUte9Dhw7VypUrCz0kqVevXlq5cqXatm1bsTuDUinPZz7/gioFzZkzR8ePH1ePHj3MLRzlUp6+DxkyRJcuXdLChQtt03JycrRkyRLdeuutBLEqrDx9z7du3TplZGQwtLwaKWvf69atq1q1amnlypV2X7KdPXtWq1evVuPGjbljSRXmiM97QU8++aQuXryoRx991LSaq5TKvU04nMGgQYMMNzc3Y/Lkyca7775rREdHG25ubsY333xjW6ZTp07GlT+OdevWNYYNG2a8/PLLxty5c43JkycbtWvXNry8vIwtW7ZU9G6glMra96JIMiZMmGBmuXCgsvbe29vbSEhIMF5//XVj1qxZxrBhwwyLxWK0aNHCOHfuXEXvBkqprH0/f/680bRpU8Pd3d2YNGmSMWPGDKNNmzaGq6ursW7duoreDZRSef+vHzhwoOHp6WlkZGRUVMlwgLL2ferUqYYko2XLlsabb75pvPbaa0aTJk0MScbixYsrejdQSmXt+4svvmgMHz7cmDFjhjF79myjW7duhiRj6tSpFb0LlYbQDdNlZ2cbkyZNMkJDQw1PT0+jTZs2xvr16+2WKeoDmpiYaLRu3doICgoy3NzcjPDwcGPo0KHGL7/8UpHlo4zK2veiELqrl7L2fuzYscatt95q+Pv7G+7u7sYtt9xiTJkyxcjMzKzI8lFG5fnMHz161Bg1apRRu3Ztw9PT02jbtm2hdVE1lafvZ86cMby8vIwBAwZUVLlwkPL0fcmSJUZUVJRRq1Ytw9vb22jbtq2xYsWKiiod5VDWvq9Zs8aIiooy/P39DR8fH6Ndu3bG8uXLK7L0SmcxDMOo4IPrAAAAAAA4Bc7pBgAAAADAJIRuAAAAAABMQugGAAAAAMAkhG4AAAAAAExC6AYAAAAAwCSEbgAAAAAATELoBgAAAADAJIRuAAAAAABMQugGAAAAAMAkhG4AAKo4i8VieyxYsMA2fcGCBXbzqpuDBw/a1b9p06bKLqmQhIQEW30xMTGVXQ4AoBoidAMAnMqmTZvsgl7Bh5+fn2699VY9+OCD2r9/f2WXWqFiYmJs70NCQkJll1Okw4cPy9XV1Vbn+PHjr7rsN998Y9fbt956qwIrBQDgPwjdAAD86dy5c0pNTdXMmTPVrFkzffXVV5VdUrHatGmjV1991fao6a677jp17drV9nzZsmXKy8srctlFixbZ/u3u7q7hw4ebXh8AAEVxq+wCAACoTEOGDFHr1q2Vl5enbdu2ac2aNZKk8+fP65577tHBgwfl6el5ze1kZmYqICDA7HLtNG3aVE2bNq3Q16xsCQkJ+vLLLyVJp06d0tq1a9W/f3+7ZXJycrRixQrb87i4OAUHB1donQAA5ONINwDAqfXo0UOTJk3S3//+d61evdruiGh6erq2bNkiqfCw9L179+q1115TkyZN5OnpqZEjR9rWs1qtWrRokbp166a6devKw8NDISEhiouL07p164qs4+LFi3rppZfUsGFDeXp6qkGDBpo6daouXLhw1dqvdU73xYsX9cEHH6hbt26qV6+erY527drpueeekyQ9++yzslgs+uabb2zrLVy40G67Bw8etM3Lzc3VzJkz1bFjR9WuXVseHh4KCwvToEGDtG3btiLrPH/+vJ544glFRETIy8tLTZs21axZs2QYxlX37Wr69eunWrVq2Z4XPKKd77PPPtOZM2dsz0ePHi3pcg/HjBmjO+64Q2FhYfL09JSPj49uueUWjR49Wjt27ChxHdc6H/1aw/X379+vhx56SE2aNJGvr6+8vb1166236oknntCJEycKLX/ixAlNmjRJTZs2la+vrzw8PBQaGqqoqChNnDhR33//fYlrBwBUMAMAACeSlJRkSLI95s+fbzd/5syZdvOXLFlS5Hp/+ctf7J7/9a9/NQzDMM6fP2906dLFbt6Vj8cee6xQXUOHDi1y2bi4uKvWO3/+fLt5BZ08edJo06bNVWsIDAw0DMMwEhMTi61VknHgwAHDMAzj2LFjRosWLa66nIuLizF9+nS7OvLy8gq9V1fbt6SkpBL18IEHHrCt4+HhYZw8edJufsHt1q1b17hw4YJhGIbxt7/9rdj99PDwMDZs2GC3rVGjRtnmd+rUyTb9wIEDxdbeqVMn27xRo0bZzVu1apXh4+Nz1Tquu+46Y9euXbbls7OzjUaNGhVb+5QpU0r03gEAKh7DywEAKODKo7WhoaFFLrd582Y1bdpUffr0kWEYcnV1lSQ9+uijtnPBPTw8NHToUDVs2FA7duzQxx9/LMMw9MYbb6hVq1a6++67JUkrVqzQ0qVLbdu+5ZZbNHjwYB0+fLjII7klcc899+j//u//bM+bNGmiXr16ydPTU9u3b1dycrIkqVu3bvLz89OcOXNsF49r3bq1hgwZYlu3du3atm3+9NNPkiR/f3/dfffduv7667VlyxatX79eVqtVjz76qFq3bq0OHTpIkt566y1t3rzZtq2WLVuqd+/e2rlzp1auXFmmfUtISNA777wjScrLy9OyZcs0btw4SdKxY8dsw88lafjw4XJzu/znjq+vrzp16qRmzZqpdu3a8vb21smTJ7V27VqlpqYqLy9PDz30kHbt2lWmukriwIEDGjZsmLKzsyVdPkWgf//+slqtWrJkiQ4dOqTDhw9r4MCB2rFjh1xdXZWUlKTdu3dLkry8vDRmzBhdd911Sk9P1969e+1GKQAAqh5CNwDAqa1fv14nTpxQXl6evv/+e61evdo2r169eoqOji5yvXbt2ikpKUleXl62aadOndL7779ve/7OO+/YhjZLUnBwsGbPni1Jeu2112yhe968ebZlAgMDlZycbAu6kZGReuqpp0q1Tzt27LAbxt6rVy+tWrVK7u7utmn5ATs6OlrR0dFas2aNbVrTpk01adIku23+8ssvdmH2s88+U2xsrO15/tB5wzD0+uuv20J3wX275ZZbtG3bNts58vfdd5/ee++9Uu2bJLVt21ZNmjRRamqqpMtDzPND9//+7//q4sWLtmULDu1+7rnnZLValZKSotTUVGVkZKhevXrq2bOnbVupqan6/fffFRERUeq6SuLtt9+2Be7IyEilpKTYfoYmTpyoiIgIXbp0SampqVq7dq369u2rnJwc2/qdOnXSzJkz7baZm5tb5JB0AEDVQOgGADi1ZcuWadmyZYWme3l5aeHChXahuqBJkyYVmpecnGwX+O69917de++9Ra7/008/6fz58/Lx8VFKSopteo8ePWyBW5JGjBhR6tD93Xff2T1PTEy0C9ySdPPNN5dqm/nntue76667rrrs1q1bJUlnz561HaGVpIEDB9pdlG7EiBFlCt3S5TA9ZcoUSZdHJ+zdu1e33HKL3ciAO+64Q82bN7c937Bhg8aOHavffvut2G3/8ccfpoXugu/jnj175O3tfdVlt27dqr59+6pNmzby9PRUbm6uvvzySzVt2lTNmzdXZGSkWrZsqc6dO+u6664zpV4AQPlxITUAAP7k7e2txo0ba/z48dqxY4e6d+9+1WUbN25caNqpU6dK/FqGYejkyZOSpIyMDNv0unXr2i1Xr169Em/zanXcdNNNpd7GtbZZnOPHj0uy3y/JMfuW75577rEN6ZcuH+3etWuXfvzxR9u0gke5jxw5on79+l0zcEuXjxyXlnHFReGuto2yvI/XX3+9FixYYLsC+65du7R06VI9//zz6t+/v8LDw+1OTwAAVC0c6QYAOLX58+cXeXXpa/H19S00reARauny+d3h4eFX3UZgYKAkqVatWrYAfuzYMbtljh49WurarqzjwIEDCgkJKfV2itvm888/X+xRWuk/+5fPEfuWLywsTN26ddMXX3whSVq8eLFd0PXw8LAN35ek1atX6/z587bnr7/+usaMGaPAwEDt2rWr1Ldec3GxP26RP2Rcunz1+n379hW5XsH3sWnTpsX+7N122222fw8dOlQDBw7UDz/8oB07dujf//63kpKStH37dp09e1ZjxoxR79695efnV6r9AACYj9ANAICDtG3bVq6urrp06ZIkyd3dvdC50dLl203t3r3bdl/v1q1b286XXr9+vU6dOmULZ4sXLy51HXfeeafd8xdeeEErV660XVBMkg4dOqQbb7zR9rzg8POC4TTflee2BwcH286jLujXX3/V6dOnJV2+2FqjRo1sQ8w/+eQTPffcc7Yh5mXZt4JGjx5tC9379+/X22+/bZvXp08f1alTx/Y8/0uNguvmfymwfPnyUr92wduWSdL333+vXr16SZLee+8921HqK0VHR+uHH36QJKWlpWnYsGGFhoZfvHhRq1evVtu2bSVdPjqelZWlG2+8UR06dLCdL3/69Gnbz8n58+e1e/dutWrVqtT7AgAwF6EbAAAHqV27tu69917becqvvPKKUlJSFB0dLS8vLx0+fFjff/+9tm/frlGjRtmGr48ZM8YWus+cOaO2bdtqyJAh+uOPP8p09fJmzZqpV69etouprVmzRrfffrt69eolLy8v/frrr/r222/tLr5VMPitXbtWTzzxhIKDgxUcHKyEhATdfvvt6tq1qzZs2CDp8kW/vvjiC7Vq1UouLi46dOiQtm7dqtTUVCUmJtqC/5gxY/T4449Lkvbu3av27durT58+2rlzpz799NNS71tBffv2Ve3atW1Dtgt+WXDlEeRGjRrZPY+Li1PPnj31yy+/aMWKFaV+7YCAAEVGRmrPnj2SpGnTpmn79u3Kzs7W119/fdX1HnzwQb3zzjvKycnRqVOn1KJFCw0aNEgRERE6e/asdu3apU2bNikjI0MHDhxQUFCQ9uzZo/bt26tNmza6/fbbFR4eLjc3N61fv95u21d+EQAAqCIq9YZlAABUsGvdp7uk6+Xfu/pK586du+Z9ulXEvZsHDRpU5HIxMTFluk/3iRMnSnSf7nyfffZZkcs1bdrUtszRo0eLvU93/iMxMdG2Tl5enhEdHV2ifSvpfboLGj9+fKHthoaG2u7NXbCOZs2aXbUXV6vjavfpNgzDmDdvXpHbu/nmm43GjRtftdcrV640fH19r/k+5v+Mbdu27ZrLDhgwoNTvHQCgYnAhNQAAHMjHx0dffvmlPvroI/Xq1Uv16tWTm5ubvL291aBBA8XHx2vu3Ll644037NZbsmSJpk2bpptvvlnu7u6qX7++nnrqKdvw6dKqU6eOtmzZonnz5qlLly4KCQmRm5ubgoKC1KpVKz3yyCN2y/ft21czZ85UkyZN5OHhUeQ269atq+TkZM2ZM0d33XWXgoOD5erqKl9fXzVu3FgjRozQkiVLNHnyZNs67u7u+sc//qHJkyfruuuuk4eHhxo1aqTXX3/d7nZiZVXwlmz5Ct6bu2AdX3/9tRISElSnTh15enrqtttu09y5c/Xss8+W6bXHjBmj9957z/aehYaGaty4cfrhhx+KvUhcv379tHPnTj322GNq1qyZ/Pz85Orqqjp16qh9+/aaPHmytmzZovr160uS7f0aMGCAIiMjFRgYKFdXVwUFBalDhw566623uJAaAFRhFsO44nKbAAAAAADAITjSDQAAAACASQjdAAAAAACYhNANAAAAAIBJCN0AAAAAAJiE0A0AAAAAgEkI3QAAAAAAmITQDQAAAACASQjdAAAAAACYhNANAAAAAIBJCN0AAAAAAJiE0A0AAAAAgEkI3QAAAAAAmOT/AUVYfmbp75HoAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 1000x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Residuals Histogram gespeichert: /home/bt708583/ml_in_ms_wt24/AdvancedModule/AutoGluon_Models/automl_20250930_130252/individual_plots/03_residuals_histogram.png\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Feature Importance gespeichert: /home/bt708583/ml_in_ms_wt24/AdvancedModule/AutoGluon_Models/automl_20250930_130252/individual_plots/04_feature_importance.png\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Model Comparison gespeichert: /home/bt708583/ml_in_ms_wt24/AdvancedModule/AutoGluon_Models/automl_20250930_130252/individual_plots/05_model_comparison.png\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Metrics Summary gespeichert: /home/bt708583/ml_in_ms_wt24/AdvancedModule/AutoGluon_Models/automl_20250930_130252/individual_plots/06_metrics_summary.png\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Alle 6 Plots einzeln gespeichert in: /home/bt708583/ml_in_ms_wt24/AdvancedModule/AutoGluon_Models/automl_20250930_130252/individual_plots\n",
      "📊 Plot-Dateien:\n",
      "  1. 01_parity_plot.png\n",
      "  2. 02_residuals_plot.png\n",
      "  3. 03_residuals_histogram.png\n",
      "  4. 04_feature_importance.png\n",
      "  5. 05_model_comparison.png\n",
      "  6. 06_metrics_summary.png\n",
      "✅ Alle Visualisierungen erstellt!\n"
     ]
    }
   ],
   "source": [
    "# ===== VISUALIZATIONS - EINZELNE PLOTS =====\n",
    "print(\"📊 Erstelle Visualisierungen für Vergleich mit Neural Network...\")\n",
    "print(\"💾 Jeder Plot wird einzeln als PNG gespeichert...\")\n",
    "\n",
    "# Setup für Plots\n",
    "plt.style.use('default')\n",
    "individual_fig_size = (10, 8)\n",
    "residuals = test_predictions - test_true\n",
    "\n",
    "# Create plots directory\n",
    "plots_dir = os.path.join(output_dir, 'individual_plots')\n",
    "os.makedirs(plots_dir, exist_ok=True)\n",
    "\n",
    "# ===== 1. PARITY PLOT (mit R² und RMSE) =====\n",
    "plt.figure(figsize=individual_fig_size)\n",
    "plt.scatter(test_true, test_predictions, alpha=0.7, s=60, color='blue', edgecolor='black', linewidth=0.5)\n",
    "# Perfect prediction line\n",
    "min_val = min(test_true.min(), test_predictions.min())\n",
    "max_val = max(test_true.max(), test_predictions.max())\n",
    "plt.plot([min_val, max_val], [min_val, max_val], 'r--', lw=3, label='Perfect Prediction', alpha=0.8)\n",
    "plt.xlabel('True Values', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('Predicted Values', fontsize=14, fontweight='bold')\n",
    "plt.title(f'AutoGluon Parity Plot\\nR² = {test_r2:.4f} | RMSE = {test_rmse:.4f}', \n",
    "         fontsize=16, fontweight='bold', pad=20)\n",
    "plt.legend(fontsize=12)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tick_params(axis='both', which='major', labelsize=12)\n",
    "# Tight layout\n",
    "plt.tight_layout()\n",
    "# Save\n",
    "parity_path = os.path.join(plots_dir, '01_parity_plot.png')\n",
    "plt.savefig(parity_path, dpi=300, bbox_inches='tight', facecolor='white')\n",
    "print(f\"📊 Parity Plot gespeichert: {parity_path}\")\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "# ===== 2. RESIDUALS PLOT =====\n",
    "plt.figure(figsize=individual_fig_size)\n",
    "plt.scatter(test_predictions, residuals, alpha=0.7, s=60, color='orange', edgecolor='black', linewidth=0.5)\n",
    "plt.axhline(y=0, color='r', linestyle='--', lw=3, alpha=0.8, label='Perfect Residuals')\n",
    "plt.xlabel('Predicted Values', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('Residuals (Predicted - True)', fontsize=14, fontweight='bold')\n",
    "plt.title('Residuals vs Predicted Values', fontsize=16, fontweight='bold', pad=20)\n",
    "plt.legend(fontsize=12)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tick_params(axis='both', which='major', labelsize=12)\n",
    "plt.tight_layout()\n",
    "# Save\n",
    "residuals_path = os.path.join(plots_dir, '02_residuals_plot.png')\n",
    "plt.savefig(residuals_path, dpi=300, bbox_inches='tight', facecolor='white')\n",
    "print(f\"📊 Residuals Plot gespeichert: {residuals_path}\")\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "# ===== 3. RESIDUALS HISTOGRAM =====\n",
    "plt.figure(figsize=individual_fig_size)\n",
    "plt.hist(residuals, bins=25, alpha=0.8, edgecolor='black', color='lightgreen', linewidth=1.2)\n",
    "plt.axvline(x=0, color='r', linestyle='--', lw=3, alpha=0.8, label='Perfect Residuals')\n",
    "plt.xlabel('Residuals', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('Frequency', fontsize=14, fontweight='bold')\n",
    "plt.title('Distribution of Residuals', fontsize=16, fontweight='bold', pad=20)\n",
    "plt.legend(fontsize=12)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tick_params(axis='both', which='major', labelsize=12)\n",
    "plt.tight_layout()\n",
    "# Save\n",
    "residuals_hist_path = os.path.join(plots_dir, '03_residuals_histogram.png')\n",
    "plt.savefig(residuals_hist_path, dpi=300, bbox_inches='tight', facecolor='white')\n",
    "print(f\"📊 Residuals Histogram gespeichert: {residuals_hist_path}\")\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "# ===== 4. FEATURE IMPORTANCE =====\n",
    "plt.figure(figsize=individual_fig_size)\n",
    "top_features = feature_importance.head(15)  # Mehr Features zeigen\n",
    "colors = plt.cm.viridis(np.linspace(0, 1, len(top_features)))\n",
    "bars = plt.barh(range(len(top_features)), top_features['importance'], color=colors, alpha=0.8, edgecolor='black', linewidth=0.5)\n",
    "plt.yticks(range(len(top_features)), top_features.index, fontsize=11)\n",
    "plt.xlabel('Feature Importance', fontsize=14, fontweight='bold')\n",
    "plt.title('Top 15 Feature Importance', fontsize=16, fontweight='bold', pad=20)\n",
    "plt.grid(True, alpha=0.3, axis='x')\n",
    "plt.tick_params(axis='both', which='major', labelsize=11)\n",
    "# Add value labels on bars\n",
    "for i, (bar, value) in enumerate(zip(bars, top_features['importance'])):\n",
    "    plt.text(value + max(top_features['importance'])*0.01, bar.get_y() + bar.get_height()/2, \n",
    "             f'{value:.3f}', ha='left', va='center', fontsize=10)\n",
    "plt.tight_layout()\n",
    "# Save\n",
    "feature_importance_path = os.path.join(plots_dir, '04_feature_importance.png')\n",
    "plt.savefig(feature_importance_path, dpi=300, bbox_inches='tight', facecolor='white')\n",
    "print(f\"📊 Feature Importance gespeichert: {feature_importance_path}\")\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "# ===== 5. MODEL PERFORMANCE COMPARISON =====\n",
    "plt.figure(figsize=individual_fig_size)\n",
    "top_models = leaderboard.head(12)  # Mehr Modelle zeigen\n",
    "colors = plt.cm.RdYlGn(np.linspace(0.3, 1, len(top_models)))\n",
    "bars = plt.barh(range(len(top_models)), top_models['score_test'], color=colors, alpha=0.8, edgecolor='black', linewidth=0.5)\n",
    "model_names_short = [name[:20] + '...' if len(name) > 20 else name for name in top_models['model']]\n",
    "plt.yticks(range(len(top_models)), model_names_short, fontsize=10)\n",
    "plt.xlabel('Test Score (R²)', fontsize=14, fontweight='bold')\n",
    "plt.title('Model Performance Comparison', fontsize=16, fontweight='bold', pad=20)\n",
    "plt.grid(True, alpha=0.3, axis='x')\n",
    "plt.tick_params(axis='both', which='major', labelsize=10)\n",
    "# Add value labels on bars\n",
    "for i, (bar, value) in enumerate(zip(bars, top_models['score_test'])):\n",
    "    plt.text(value + max(top_models['score_test'])*0.01, bar.get_y() + bar.get_height()/2, \n",
    "             f'{value:.4f}', ha='left', va='center', fontsize=9)\n",
    "plt.tight_layout()\n",
    "# Save\n",
    "model_comparison_path = os.path.join(plots_dir, '05_model_comparison.png')\n",
    "plt.savefig(model_comparison_path, dpi=300, bbox_inches='tight', facecolor='white')\n",
    "print(f\"📊 Model Comparison gespeichert: {model_comparison_path}\")\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "# ===== 6. ERROR METRICS SUMMARY =====\n",
    "plt.figure(figsize=individual_fig_size)\n",
    "metrics = ['R² Score', 'RMSE', 'MAE']\n",
    "values = [test_r2, test_rmse, test_mae]\n",
    "colors = ['#2E8B57' if test_r2 > 0.8 else '#FF8C00' if test_r2 > 0.6 else '#DC143C', '#4169E1', '#8A2BE2']\n",
    "bars = plt.bar(metrics, values, color=colors, alpha=0.8, edgecolor='black', linewidth=1.5, width=0.6)\n",
    "plt.title('Test Metrics Summary', fontsize=16, fontweight='bold', pad=20)\n",
    "plt.ylabel('Metric Value', fontsize=14, fontweight='bold')\n",
    "plt.tick_params(axis='both', which='major', labelsize=12)\n",
    "# Annotate bars with values\n",
    "for bar, value in zip(bars, values):\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2., height + height*0.02,\n",
    "            f'{value:.4f}', ha='center', va='bottom', fontsize=13, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "plt.tight_layout()\n",
    "# Save\n",
    "metrics_summary_path = os.path.join(plots_dir, '06_metrics_summary.png')\n",
    "plt.savefig(metrics_summary_path, dpi=300, bbox_inches='tight', facecolor='white')\n",
    "print(f\"📊 Metrics Summary gespeichert: {metrics_summary_path}\")\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "print(f\"\\n✅ Alle 6 Plots einzeln gespeichert in: {plots_dir}\")\n",
    "print(\"📊 Plot-Dateien:\")\n",
    "for i, filename in enumerate(['01_parity_plot.png', '02_residuals_plot.png', '03_residuals_histogram.png', \n",
    "                              '04_feature_importance.png', '05_model_comparison.png', '06_metrics_summary.png'], 1):\n",
    "    print(f\"  {i}. {filename}\")\n",
    "print(\"✅ Alle Visualisierungen erstellt!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
